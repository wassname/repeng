Title: SSVD: Structured SVD for Parameter-Efficient Fine-Tuning and Benchmarking under Domain Shift in ASR

URL Source: https://arxiv.org/html/2509.02830v1

Markdown Content:
Shinji Watanabe Language Technologies Institute

Carnegie Mellon University 

Pittsburgh, PA, USA 

shinjiw@ieee.org Hugo Van hamme Department of Electrical Engineering

KU Leuven 

Leuven, Belgium 

hugo.vanhamme@esat.kuleuven.be

###### Abstract

Parameter-efficient fine-tuning (PEFT) has emerged as a scalable solution for adapting large foundation models. While low-rank adaptation (LoRA) is widely used in speech applications, its state-of-the-art variants, e.g., VeRA, DoRA, PiSSA, and SVFT, are developed mainly for language and vision tasks, with limited validation in speech. This work presents the first comprehensive integration and benchmarking of these PEFT methods within ESPnet. We further introduce structured SVD-guided (SSVD) fine-tuning, which selectively rotates input-associated right singular vectors while keeping output-associated vectors fixed to preserve semantic mappings. This design enables robust domain adaptation with minimal trainable parameters and improved efficiency. We evaluate all methods on domain-shifted speech recognition tasks, including child speech and dialectal variation, across model scales from 0.1B to 2B. All implementations are released in ESPnet to support reproducibility and future work.

###### Index Terms:

speech recognition, parameter-efficient fine-tuning (PEFT), domain adaptation, low-rank adaptation (LoRA), singular value decomposition (SVD), child and dialectal speech

I Introduction
--------------

Large-scale foundation models pretrained on multilingual speech corpora, such as OpenAI‚Äôs Whisper[[1](https://arxiv.org/html/2509.02830v1#bib.bibx1)], the open-source Whisper-style OWSM[[2](https://arxiv.org/html/2509.02830v1#bib.bibx2), [3](https://arxiv.org/html/2509.02830v1#bib.bibx3)], NVIDIA‚Äôs Canary[[4](https://arxiv.org/html/2509.02830v1#bib.bibx4)], etc., have demonstrated strong generalization and adaptability across diverse speech recognition tasks. Fine-tuning these models for downstream or personalized applications has therefore become the dominant paradigm. However, these models are typically trained on broad yet generic datasets dominated by standard accents and adult speech, limiting their effectiveness under domain shifts involving regional accents, dialectal variation, child speech, and other low-resource speech domains. In such scenarios, mismatches in acoustic features, linguistic structures, and articulatory behaviors can significantly degrade performance.

Recent studies on scaling laws of large-scale multilingual automatic speech recognition (ASR) models have shown that increasing model size substantially improves recognition accuracy, particularly for low-resource languages[[5](https://arxiv.org/html/2509.02830v1#bib.bibx5), [6](https://arxiv.org/html/2509.02830v1#bib.bibx6), [7](https://arxiv.org/html/2509.02830v1#bib.bibx7), [8](https://arxiv.org/html/2509.02830v1#bib.bibx8)]. Nonetheless, rapid scaling introduces practical challenges, as adapting large models to specific downstream or out-of-domain scenarios can become prohibitively expensive in terms of computational resources and storage requirements.

To address the limitations of full-model fine-tuning, parameter-efficient fine-tuning (PEFT) methods have been actively explored, particularly with large language models (LLMs)[[9](https://arxiv.org/html/2509.02830v1#bib.bibx9), [10](https://arxiv.org/html/2509.02830v1#bib.bibx10), [11](https://arxiv.org/html/2509.02830v1#bib.bibx11), [12](https://arxiv.org/html/2509.02830v1#bib.bibx12)]. Among them, low-rank adaptation (LoRA) has received considerable attention for significantly reducing the number of trainable parameters without modifying the original model architecture or introducing additional decoding latency[[12](https://arxiv.org/html/2509.02830v1#bib.bibx12), [13](https://arxiv.org/html/2509.02830v1#bib.bibx13), [14](https://arxiv.org/html/2509.02830v1#bib.bibx14), [15](https://arxiv.org/html/2509.02830v1#bib.bibx15)]. Specifically, LoRA freezes the pre-trained weight matrices and injects two trainable low-rank matrices to approximate weight updates. While originally proposed for LLMs, LoRA has also attracted increasing interest in speech tasks and has been successfully applied in various speech applications[[15](https://arxiv.org/html/2509.02830v1#bib.bibx15), [16](https://arxiv.org/html/2509.02830v1#bib.bibx16), [17](https://arxiv.org/html/2509.02830v1#bib.bibx17), [18](https://arxiv.org/html/2509.02830v1#bib.bibx18), [19](https://arxiv.org/html/2509.02830v1#bib.bibx19), [20](https://arxiv.org/html/2509.02830v1#bib.bibx20)]. Wang et al.[[17](https://arxiv.org/html/2509.02830v1#bib.bibx17)] explore low-rankness in speech models and adapt it to spoken language understanding tasks. Liu et al.[[18](https://arxiv.org/html/2509.02830v1#bib.bibx18)] adapt Whisper with LoRA to child speech ASR. Xu et al.[[19](https://arxiv.org/html/2509.02830v1#bib.bibx19)] further investigate using LoRA to mitigate forgetting when fine-tuning Whisper to multiple languages. Song et al.[[20](https://arxiv.org/html/2509.02830v1#bib.bibx20)] propose LoRA with a mixture of experts (MoE) to build an extensible multilingual LoRA-Whisper ASR model.

Although LoRA effectively reduces parameter count, it still incurs noticeable storage overhead as foundation models scale and suffers from convergence inefficiencies[[21](https://arxiv.org/html/2509.02830v1#bib.bibx21), [22](https://arxiv.org/html/2509.02830v1#bib.bibx22)]. To mitigate these issues, vector-based random matrix adaptation (VeRA) has been recently proposed as a more lightweight alternative[[21](https://arxiv.org/html/2509.02830v1#bib.bibx21)]. VeRA randomly initializes and freezes the low-rank matrices of LoRA, sharing these matrices across layers while updating only two scaling vectors. Thus, further reducing the memory footprint with acceptable performance drops on natural language processing (NLP) and vision tasks. Additionally, recent studies in NLP have shown a persistent performance gap between LoRA and full fine-tuning. Liu et al.[[23](https://arxiv.org/html/2509.02830v1#bib.bibx23)] analyzed the dynamic differences in weights updated between LoRA-tuned and fully fine-tuned models and proposed weight-decomposed LoRA (DoRA). DoRA reparameterizes LoRA into separate directional and magnitude components, demonstrating that tuning primarily one component is sufficient to match full fine-tuning performance in downstream NLP and vision tasks.

To further improve the training efficiency of LoRA variants, Meng et al.[[22](https://arxiv.org/html/2509.02830v1#bib.bibx22)] and Lingam et al.[[24](https://arxiv.org/html/2509.02830v1#bib.bibx24)] introduce singular value decomposition (SVD) to guide parameter tuning along more meaningful directions. Instead of relying on random initialization within LoRA, Meng et al.[[22](https://arxiv.org/html/2509.02830v1#bib.bibx22)] proposed PiSSA, which initializes the low-rank matrices of LoRA using the principal singular vectors and values derived from the pre-trained weight matrix. It accelerates convergence and reduces the risk of becoming trapped in suboptimal local minima compared to standard LoRA in NLP tasks. Lingam et al.[[24](https://arxiv.org/html/2509.02830v1#bib.bibx24)] extended this idea by proposing singular vector guided fine-tuning (SVFT), which further minimizes the number of trainable parameters by freezing the singular vectors entirely and updating only the singular values, significantly reducing parameter counts compared to PiSSA in NLP and vision domains.

Despite standard LoRA showing promising results in ASR, its advanced variants, as mentioned earlier, are primarily developed and studied in natural language and vision tasks, with their application to speech recognition remaining relatively underexplored.

Speech fundamentally differs from text in its dynamic, and highly variable nature, with significant acoustic diversity, pronunciation variability, dialectal shifts, etc. For instance, Flemish (Belgian Dutch) differs from standard Dutch, with a prominent distinction being the use of the soft /g//g/, compared to the hard /g//g/ commonly spoken in the Netherlands. Even identical lexical content ‚ÄúGoed geregend, zeg!‚Äù, such phonetic differences introduce substantial acoustic mismatch. Similarly, child speech presents unique acoustic characteristics, such as phonetic variations arising from developmental articulation patterns (e.g., pronouncing ‚Äúwabbit‚Äù instead of ‚Äúrabbit‚Äù). PEFT methods are thus expected to efficiently adapt to domain shifts in acoustic variation while maintaining consistent semantic recognition.

To address this gap and facilitate deeper understanding and advancement in PEFT specifically for speech, we provide a thorough investigation of LoRA and its leading variants. We implement and comprehensively evaluate these methods on challenging speech recognition scenarios, including low-resource child speech, as well as dialectal variations between Dutch and Flemish, by fine-tuning on OWSM models at multiple scales (from 0.1B to 2B) using the widely adopted open-source ESPnet toolkit. All implemented methods are publicly released and fully integrated into ESPnet toolkit, facilitating reproducibility and enabling further research in speech-specific PEFT.

Meanwhile, this paper introduces a novel structured SVD-guided (SSVD) PEFT approach specifically designed to handle domain shifts in speech recognition tasks. The proposed SSVD method leverages SVD to first decompose pre-trained model weights into right singular vectors, representing the input (acoustic) feature space, and left singular vectors, capturing the output (semantic) feature space. During fine-tuning, our method selectively applies structured rotations to the input-associated right singular vectors, effectively adapting acoustic feature representations to align better with domain-shifted inputs. Meanwhile, the left singular vectors remain fixed, preserving stable semantic mappings. This innovative approach ensures efficient adaptation to acoustic domain shifts while maintaining robust linguistic outputs.

The main contributions of this paper are:

*   ‚Ä¢
Providing the first comprehensive benchmarking of SoTA PEFT methods, including LoRA, VeRA, DoRA, PiSSA, and SVFT, within the widely adopted ESPnet framework[[25](https://arxiv.org/html/2509.02830v1#bib.bibx25)]. These methods are comprehensively evaluated on challenging speech recognition tasks involving low-resource child speech and dialectal variations (Dutch vs. Flemish) at multiple model scales, making all implementations publicly available and fully integrated into ESPnet for reproducibility and future research.

*   ‚Ä¢
Introducing a novel structured SVD-guided (SSVD) PEFT method that explicitly designed to efficiently adapt large-scale speech foundation models to challenging domain shifts by selectively adapting input-related singular vectors, while preserving semantic mappings through fixed output-related singular vector components.

*   ‚Ä¢
Empirically demonstrating that SSVD achieves comparable performance with significantly fewer trainable parameters and higher efficiency than LoRA and SoTA LoRA variants, approaching fully fine-tuned model performance.

II Related Work for Benchmarking
--------------------------------

TABLE I: Summary of PEFT methods formulations and trainable parameter counts (underlined variables).

In this section, we introduce the state-of-the-art (SoTA) LoRA variants evaluated in this work. Table[I](https://arxiv.org/html/2509.02830v1#S2.T1 "TABLE I ‚Ä£ II Related Work for Benchmarking ‚Ä£ SSVD: Structured SVD for Parameter-Efficient Fine-Tuning and Benchmarking under Domain Shift in ASR") summarizes their corresponding formulations and trainable parameter counts. Trainable variables in this study are indicated by underlining.

LoRA. LoRA freezes the pre-trained weights of models and injects trainable low-rank matrices into each linear layer. For a pre-trained weight ùêñ 0‚àà‚Ñù m√ón\mathbf{W}_{0}\in\mathbb{R}^{m\times n}, LoRA parameterizes its fine-tuning update as:

ùêñ‚Ä≤=ùêñ 0+ùêÄùêÅ¬Ø‚ä§,\mathbf{W}^{\prime}=\mathbf{W}_{0}+\underline{\mathbf{A}\mathbf{B}}^{\top},(1)

where ùêÄ‚àà‚Ñù m√ór\mathbf{A}\in\mathbb{R}^{m\times r}, ùêÅ‚àà‚Ñù n√ór\mathbf{B}\in\mathbb{R}^{n\times r} are trainable low-rank matrices. LoRA significantly reduces the number of trainable parameters to r√ó(m+n)r\times(m+n) by choosing a rank r‚â™min‚Å°(m,n)r\ll\min(m,n), but as model size scales, it can still accumulate a non-trivial number of learnable parameters.

VERA. Unlike LoRA, which introduces trainable low-rank matrices for each adapted layer, VeRA employs a single pair of frozen, randomly initialized low-rank matrices shared across all layers. Adaptation is achieved through the learning of small, trainable scaling vectors specific to each layer. For a pre-trained weight ùêñ 0\mathbf{W}_{0}, VERA adapt it as:

ùêñ‚Ä≤=ùêñ 0+(ùö≤¬Ød‚ÄãùêÄ)‚Äã(ùö≤¬Øb‚ÄãùêÅ)‚ä§\mathbf{W}^{\prime}=\mathbf{W}_{0}+({\underline{\boldsymbol{\Lambda}}_{d}}\mathbf{A})({\underline{\boldsymbol{\Lambda}}_{b}}\mathbf{B})^{\top}(2)

where ùêÄ\mathbf{A} and ùêÅ\mathbf{B} are low-rank random matrices, generated once and kept frozen. ùö≤ d{\boldsymbol{\Lambda}}_{d} and ùö≤ b{\boldsymbol{\Lambda}}_{b} are diagonal matrices formed from small trainable scaling vectors b‚àà‚Ñù r b\in\mathbb{R}^{r} and d‚àà‚Ñù m d\in\mathbb{R}^{m}, respectively. Since ùêÄ\mathbf{A} and ùêÅ\mathbf{B} are shared and reproducible from a fixed random seed, VeRA significantly lowers the memory footprint to r+m r+m.

DoRA. While VeRA focuses on achieving decent performance with minimal trainable parameters, DoRA aims to bridge the performance gap between LoRA and full fine-tuning. For a pre-trained weight matrix ùêñ 0\mathbf{W}_{0}, DoRA decomposes it into a magnitude vector ùê¶=‚Äñùêñ 0‚Äñc\mathbf{m}=\|\mathbf{W}_{0}\|_{c}, where ‚à•.‚à•c\|.\|_{c} is the vector-wise (column-wise) norm, and a directional matrix ùêñ 0‚Äñùêñ 0‚Äñc\displaystyle\frac{\mathbf{W}_{0}}{\|\mathbf{W}_{0}\|_{c}}. By analyzing the change in the components during LoRA tuning versus full fine-tuning, Liu et al.[[23](https://arxiv.org/html/2509.02830v1#bib.bibx23)] observed that LoRA struggles to simultaneously learn both direction and scale, while full fine-tuning tends to adapt primarily one of the two. DoRA hereby simplifies optimization and focuses solely on directional adaptation, injecting low-rank updates only into the directional component:

ùêñ‚Ä≤=ùê¶¬Ø‚Äãùêñ ùüé+ùêÄùêÅ¬Ø‚ä§‚Äñùêñ ùüé+ùêÄùêÅ¬Ø‚ä§‚Äñc\mathbf{W}^{\prime}=\mathbf{\underline{m}}\frac{\mathbf{W_{0}}+\mathbf{\underline{AB}}^{\top}}{\|\mathbf{W_{0}}+\mathbf{\underline{AB}}^{\top}\|_{c}}(3)

Compared to LoRA, DoRA introduces an additional trainable magnitude vector ùê¶‚àà‚Ñù m\mathbf{m}\in\mathbb{R}^{m}, increasing the trainable parameter count to r√ó(m+n)+m r\times(m+n)+m. However, this increase remains negligible compared to the size of the full model.

PiSSA. Unlike LoRA, VeRA, and DoRA, which rely on randomly initialized low-rank matrices and suffer from inefficient early training or suboptimal local minima, PiSSA initializes the low-rank components ùêÄ\mathbf{A} and ùêÅ\mathbf{B} using the top-k k principal singular vectors and values from the SVD of the pre-trained weight matrix ùêñ 0‚àà‚Ñù m√ón\mathbf{W}_{0}\in\mathbb{R}^{m\times n}.

Given the SVD, with m‚â•n m\geq n (e.g., a feedforward layer):

ùêñ 0=ùêî‚Äãùö∫‚Äãùêï‚ä§=‚àëi=1 n œÉ i‚ÄãùêÆ i‚ÄãùêØ i‚ä§,\mathbf{W}_{0}=\mathbf{U}\boldsymbol{\Sigma}\mathbf{V}^{\top}=\sum_{i=1}^{n}\sigma_{i}\mathbf{u}_{i}\mathbf{v}_{i}^{\top},(4)

where, ùêî‚àà‚Ñù m√ón\mathbf{U}\in\mathbb{R}^{m\times n}, ùêï‚àà‚Ñù n√ón\mathbf{V}\in\mathbb{R}^{n\times n} contain the left and right singular vectors ùêÆ i\mathbf{u}_{i} and ùêØ i\mathbf{v}_{i} respectively, and ùö∫‚àà‚Ñù n√ón\boldsymbol{\Sigma}\in\mathbb{R}^{n\times n} is a diagonal matrix of singular values œÉ i\sigma_{i}. Fine-tuning is performed along the k k principal directions, while the residual components with the (n‚àík)(n-k) smallest singular values are frozen:

ùêñ‚Ä≤=ùêÄùêÅ¬Ø‚ä§+‚àëi=k+1 n‚àík œÉ i‚ÄãùêÆ i‚ÄãùêØ i‚ä§,\mathbf{W}^{\prime}=\underline{\mathbf{A}\mathbf{B}}^{\top}+\sum_{i=k+1}^{n-k}\sigma_{i}\mathbf{u}_{i}\mathbf{v}_{i}^{\top},(5)

Here, ùêÄ\mathbf{A} and ùêÅ\mathbf{B} are initialized by ‚àëi=1 k œÉ i 1 2‚ÄãùêÆ i\sum_{i=1}^{k}\sigma_{i}^{\frac{1}{2}}\mathbf{u}_{i} resp. ‚àëi=1 k œÉ i 1 2‚ÄãùêØ i T\sum_{i=1}^{k}\sigma_{i}^{\frac{1}{2}}\mathbf{v}_{i}^{\mathrm{T}}. This initialization enables more efficient optimization by aligning parameter updates with meaningful directions in the model‚Äôs parameter space, leading to faster convergence and improved performance compared to standard LoRA.

SVFT. SVFT extends PiSSA by further reducing the number of trainable parameters, modifying only a sparse subset of singular values and associated directions. Starting from the SVD of a pre-trained weight matrix([4](https://arxiv.org/html/2509.02830v1#S2.E4 "In II Related Work for Benchmarking ‚Ä£ SSVD: Structured SVD for Parameter-Efficient Fine-Tuning and Benchmarking under Domain Shift in ASR")), SVFT updates the weight as:

ùêñ‚Ä≤=ùêî‚Äã(ùö∫+ùêå¬Ø)‚Äãùêï‚ä§\mathbf{W}^{\prime}=\mathbf{U}(\mathbf{\Sigma}+\underline{\mathbf{M}})\mathbf{V}^{\top}(6)

where ùêå‚àà‚Ñù n√ón\mathbf{M}\in\mathbb{R}^{n\times n} is a small, learnable matrix. Different structures of ùêå\mathbf{M} lead to different SVFT variants: 1) S‚ÄãV‚ÄãF‚ÄãT p SVFT^{p} uses a diagonal matrix ùêå\mathbf{M}, adapting only singular values (akin to reweighting frozen singular directions); 2) S‚ÄãV‚ÄãF‚ÄãT d B SVFT_{d}^{B} uses a banded matrix to introduce learnable off-diagonal interactions; 3) S‚ÄãV‚ÄãF‚ÄãT d R SVFT_{d}^{R} samples ùêå\mathbf{M} as a fixed random matrix; 4) S‚ÄãV‚ÄãF‚ÄãT d T SVFT_{d}^{T} makes the top-k k strong interactions between singular vector directions learnable. These variants enable SVFT to explicitly adjust the most critical directions in the parameter space. Among them, S‚ÄãV‚ÄãF‚ÄãT d B SVFT_{d}^{B} demonstrated the best performance in[[24](https://arxiv.org/html/2509.02830v1#bib.bibx24)], and has a trainable parameter count of n√óq+(n‚àíq)‚Äã(q+1)n\times q+(n-q)(q+1), where q q denotes the bandwidth.

Although these methods have been extensively studied in text and vision domains, their effectiveness in speech remains largely unexplored. Moreover, none of them is fundamentally tailored to the unique characteristics of speech signals. This limitation is particularly evident in SVD-guided methods, which explicitly decompose the input and output feature spaces‚Äîan operation linked to the acoustic and semantic properties of speech.

In the following section, we highlight this intrinsic nature of SVD-based methods and introduce structured SVD-Guided fine-tuning (SSVD), which explicitly adapts the input feature space to enable efficient and robust domain adaptation in speech.

III SSVD Method
---------------

As SVD is mathematically demonstrated in Section[II](https://arxiv.org/html/2509.02830v1#S2 "II Related Work for Benchmarking ‚Ä£ SSVD: Structured SVD for Parameter-Efficient Fine-Tuning and Benchmarking under Domain Shift in ASR") - PiSSA([4](https://arxiv.org/html/2509.02830v1#S2.E4 "In II Related Work for Benchmarking ‚Ä£ SSVD: Structured SVD for Parameter-Efficient Fine-Tuning and Benchmarking under Domain Shift in ASR")), each right singular vector ùêØ i\mathbf{v}_{i} is mapped to the corresponding left singular vector ùêÆ i\mathbf{u}_{i}, scaled by the singular value ùö∫\boldsymbol{\Sigma}.

An input ùê±‚àà‚Ñù n\mathbf{x}\in\mathbb{R}^{n}, represented in the coordinate system spanned by the right singular vector ùêØ i\mathbf{v}_{i}, is mapped to an output ùê≤‚ààCol‚Å°ùêé‚äÇ‚Ñù m\mathbf{y}\in\operatorname{Col}\mathbf{O}\subset\mathbb{R}^{m} in the coordinate system spanned by the left singular vector ùêÆ i\mathbf{u}_{i}.

ùê≤=ùêî‚Äãùö∫‚Äãùêï‚ä§‚Äãùê±\mathbf{y}=\mathbf{U}\boldsymbol{\Sigma}\mathbf{V}^{\top}\mathbf{x}(7)

Under domain shift in the speech input space, the input ùê±‚àà‚Ñù n\mathbf{x}\in\mathbb{R}^{n} is no longer aligned with the original right singular basis ùêØ i\mathbf{v}_{i}, but instead aligned with a shifted basis ùêØ i‚Ä≤\mathbf{v}_{i}^{\prime}. This constitutes an ‚Äúinner‚Äù transform, whereas the output semantic space, governed by ùêÆ i\mathbf{u}_{i}, remains unchanged (the ‚Äúouter‚Äù transform). The shift in coordinate basis can be modeled through a rotation and scaling of the original input space. Accordingly, the transformation becomes:

ùê≤=ùêî‚Äã(Œ£+ùö´‚Äãùö∫¬Ø)‚ÄãùêÜ¬Ø‚Äãùêï‚ä§‚Äãùê±=ùêñ‚Ä≤‚Äãùê±\mathbf{y}=\mathbf{U}\boldsymbol{(}{\Sigma+\underline{\mathbf{\Delta\Sigma}}})\underline{\mathbf{G}}\mathbf{V}^{\top}\mathbf{x}\\ =\mathbf{W}^{\prime}\mathbf{x}(8)

where, the diagonal matrix ùö´‚Äãùö∫\mathbf{\Delta\Sigma} models axis-wise scaling (i.e., singular value shifts), and ùêÜ\mathbf{G} is an orthogonal matrix representing a series of rotations in the right singular vector space.

Since the principal singular values and vectors for the best matrix approximation (Eckart-Young theorem)[[26](https://arxiv.org/html/2509.02830v1#bib.bibx26)], we use

ùö´‚Äãùö∫=[ùö´‚Äãùö∫ k 0 0 0]‚Äãand‚ÄãùêÜ=[ùêÜ k 0 0 I]\mathbf{\Delta\Sigma}=\begin{bmatrix}\mathbf{\Delta\Sigma}_{k}&0\\ 0&0\end{bmatrix}~~\text{and}~~\mathbf{G}=\begin{bmatrix}\mathbf{G}_{k}&0\\ 0&I\end{bmatrix}(9)

to apply adaptations to the top-k k components. In Section[V-A](https://arxiv.org/html/2509.02830v1#S5.SS1 "V-A ASR performance ‚Ä£ V Results ‚Ä£ SSVD: Structured SVD for Parameter-Efficient Fine-Tuning and Benchmarking under Domain Shift in ASR"), we discuss different choices for the proportion of adapted components.

To implement rotation within the inner transformation, we explore three parameterizations of the transformation matrix ùêÜ k\mathbf{G}_{k}, which trade off orthogonality constraints for computational efficiency.

### III-A Strict orthogonal constraint

We enforce strict orthogonality by defining ùêÜ k\mathbf{G}_{k} using the Cayley transform[[27](https://arxiv.org/html/2509.02830v1#bib.bibx27)]:

ùêÜ k=(ùêà‚àíùêä)‚Äã(ùêà+ùêä)‚àí1,\mathbf{G}_{k}=(\mathbf{I}-\mathbf{K})(\mathbf{I}+\mathbf{K})^{-1},(10)

where ùêä‚àà‚Ñù k√ók\mathbf{K}\in\mathbb{R}^{k\times k} is a skew-symmetric matrix (i.e, ùêä T=‚àíùêä\mathbf{K}^{T}=-\mathbf{K}). By learning a skew-symmetric matrix ùêä\mathbf{K}, the number of trainable parameters in SSVD is reduced to (k‚Äã(k‚àí1)2+k)(\frac{k(k-1)}{2}+k), which is significantly fewer than the (k 2+k)(k^{2}+k) parameters required to directly learn a full matrix ùêÜ k\mathbf{G}_{k} (‚Äò+k+k‚Äô accounts for singular values scaling update).

However, the matrix inversion in the Cayley transform incurs a computational cost of approximately ùí™‚Äã(k 3)\mathcal{O}(k^{3}) cost, which becomes prohibitive for large dimensions k k. To address this, we introduce an approximate orthogonality constraint using a first‚Äìorder Cayley approximation in[III-B](https://arxiv.org/html/2509.02830v1#S3.SS2 "III-B Approximate orthogonal constraint ‚Ä£ III SSVD Method ‚Ä£ SSVD: Structured SVD for Parameter-Efficient Fine-Tuning and Benchmarking under Domain Shift in ASR").

### III-B Approximate orthogonal constraint

When ‚à•ùêä‚à•‚â™1\lVert\mathbf{K}\rVert\!\ll\!1, we can approximate the inverse in the Cayley transform via a truncated Neumann series:

(ùêà+ùêä)‚àí1=ùêà‚àíùêä+ùí™‚Äã(ùêä 2)(\mathbf{I}+\mathbf{K})^{-1}=\mathbf{I}-\mathbf{K}+\mathcal{O}(\mathbf{K}^{2})(11)

leading to a first-order approximation:

ùêÜ k‚âà(ùêà‚àíùêä)‚Äã(ùêà‚àíùêä)=ùêà‚àí2‚Äãùêä+ùí™‚Äã(ùêä 2).\mathbf{G}_{k}\;\approx\;(\mathbf{I}-\mathbf{K})(\mathbf{I}-\mathbf{K})=\mathbf{I}-2\mathbf{K}+\mathcal{O}(\mathbf{K}^{2}).(12)

By keeping only the linear term, we obtain the simplified form:

ùêÜ k‚âàùêà‚àí2‚Äãùêä\mathbf{G}_{k}\approx\mathbf{I}-2\mathbf{K}\,(13)

which avoids matrix inversion and reduces the cost to ùí™‚Äã(k 2)\mathcal{O}(k^{2}). This approximation introduces an orthogonality error of order ‚à•ùêä‚à•2\lVert\mathbf{K}\rVert^{2}, which is often acceptable for small k k, a common setting in PEFT. We use approximate orthogonality constraint by default in this study. In Section[V-C](https://arxiv.org/html/2509.02830v1#S5.SS3 "V-C Ablation study ‚Ä£ V Results ‚Ä£ SSVD: Structured SVD for Parameter-Efficient Fine-Tuning and Benchmarking under Domain Shift in ASR"), we compare the strict and approximate orthogonality settings and demonstrate that the resulting error is trivial.

### III-C Unconstrained rotations

As a further relaxation, we drop the orthogonality constraint altogether, allowing ùêÜ k‚àà‚Ñù k√ók\mathbf{G}_{k}\in\mathbb{R}^{k\times k} to be a freely parameterized matrix. This maximally reduces computational overhead but sacrifices the beneficial geometric properties of orthogonal transformations, such as preserving norms and angles. It also increases the number of trainable parameters to (k 2+k)(k^{2}+k). The performance of this unconstrained rotation is also evaluated in Section[V-C](https://arxiv.org/html/2509.02830v1#S5.SS3 "V-C Ablation study ‚Ä£ V Results ‚Ä£ SSVD: Structured SVD for Parameter-Efficient Fine-Tuning and Benchmarking under Domain Shift in ASR").

IV Experiments
--------------

In this study, we choose the open Whisper-style speech models OWSM[[2](https://arxiv.org/html/2509.02830v1#bib.bibx2), [3](https://arxiv.org/html/2509.02830v1#bib.bibx3), [5](https://arxiv.org/html/2509.02830v1#bib.bibx5)] as our initial models, as they support a wide range of model scales, from 0.1B to 18B parameters. Moreover, the OWSM series is fully open-sourced, providing researchers with detailed information about the corpora used in pre-trainig. This transparency ensures that there is no domain overlap in our evaluation setup, allowing for a reliable study of domain-shift issues in ASR.

All PEFT methods are implemented within the ESPnet framework, we evaluate them by fine-tuning OWSM models at 0.1B, 1B, and 2B (OWLS) scales on two domain-shifted speech datasets: MyST[[28](https://arxiv.org/html/2509.02830v1#bib.bibx28)] (child speech) and CGN[[29](https://arxiv.org/html/2509.02830v1#bib.bibx29)] (dialectal Flemish and Dutch).

### IV-A Dataset

TABLE II: Summary of corpora.

The MyST corpus[[28](https://arxiv.org/html/2509.02830v1#bib.bibx28)] consists of English dialogues between elementary school students and virtual science tutors, spanning eight topics. The transcriptions provide verbatim orthographic annotations that capture hesitations, repetitions, and disfluencies. Following the protocol of[[30](https://arxiv.org/html/2509.02830v1#bib.bibx30)], we filter out utterances with a word error rate (WER) higher than 50% on Whisper-large-v2 to ensure transcription quality.

The CGN (Spoken Dutch Corpus)[[29](https://arxiv.org/html/2509.02830v1#bib.bibx29)] is a manually annotated speech database containing approximately 900 hours of Dutch speech, including 270 hours of Flemish Dutch. It consists of 15 components covering various speaking styles, such as read speech, interviews, spontaneous conversations, and telephone dialogues. In this study, we use a subset containing both Flemish and Dutch dialects, excluding component ùöå\mathtt{c} (spontaneous interview), component ùöç\mathtt{d} (discussions), and component ùöè\mathtt{f} (spontaneous telephone dialogues), resulting in a total of approximately 341 hours.

The dataset statistics are summarized in Table[II](https://arxiv.org/html/2509.02830v1#S4.T2 "TABLE II ‚Ä£ IV-A Dataset ‚Ä£ IV Experiments ‚Ä£ SSVD: Structured SVD for Parameter-Efficient Fine-Tuning and Benchmarking under Domain Shift in ASR").

### IV-B Implementation details

TABLE III: Architectural configurations of models.

OWSM uses an E-Branchformer[[31](https://arxiv.org/html/2509.02830v1#bib.bibx31)] encoder with a Transformer[[32](https://arxiv.org/html/2509.02830v1#bib.bibx32)] decoder, while OWLS is a standard Transformer encoder‚Äìdecoder. Table [III](https://arxiv.org/html/2509.02830v1#S4.T3 "TABLE III ‚Ä£ IV-B Implementation details ‚Ä£ IV Experiments ‚Ä£ SSVD: Structured SVD for Parameter-Efficient Fine-Tuning and Benchmarking under Domain Shift in ASR") lists their full configurations. OWSM and OWLS models are trained on 180k hours of public speech data, as documented in[[5](https://arxiv.org/html/2509.02830v1#bib.bibx5), [3](https://arxiv.org/html/2509.02830v1#bib.bibx3)], which do not include child speech or Flemish Dutch, although standard Dutch data are included. Therefore, domain-shifted scenarios are expected.

In our study, PEFT methods are applied to all linear layers in each model, including query, key, and value projections. For different low-rank configurations, we denote them as L‚Äão‚ÄãR‚ÄãA r=r‚Äãa‚Äãn‚Äãk LoRA_{r=rank}, V‚Äãe‚ÄãR‚ÄãA r=r‚Äãa‚Äãn‚Äãk VeRA_{r=rank}, D‚Äão‚ÄãR‚ÄãA r=r‚Äãa‚Äãn‚Äãk DoRA_{r=rank}, and P‚Äãi‚ÄãS‚ÄãS‚ÄãA r=r‚Äãa‚Äãn‚Äãk PiSSA_{r=rank}. For SVFT, we follow[[24](https://arxiv.org/html/2509.02830v1#bib.bibx24)] and use the best-performing banded matrix ùêå\mathbf{M}, denoted as S‚ÄãV‚ÄãF‚ÄãT d=b‚Äãa‚Äãn‚Äãd‚Äãs‚Äãi‚Äãz‚Äãe B SVFT_{d=band~size}^{B}. For SSVD, with different choices of k k in([9](https://arxiv.org/html/2509.02830v1#S3.E9 "In III SSVD Method ‚Ä£ SSVD: Structured SVD for Parameter-Efficient Fine-Tuning and Benchmarking under Domain Shift in ASR")), rotation adaptations are applied to p%p\% of the right singular vectors and values, denoted as S‚ÄãS‚ÄãV‚ÄãD p=p‚Äão‚Äãr‚Äãt‚Äãi‚Äão‚Äãn SSVD_{p=portion}. All experiments are conducted on a single NVIDIA H100 80GB GPU.

TABLE IV: PEFT methods across OWSM models on MyST.

Model PEFT Method# Params WER (%) ‚Üì\downarrow
OWSM-0.1B Zero-shot‚àí-25.0 25.0
Full fine-tuning 101 101 M 14.9 14.9
L‚Äão‚ÄãR‚ÄãA r=8 LoRA_{r=8}2.07 2.07 M 19.2 19.2
L‚Äão‚ÄãR‚ÄãA r=16 LoRA_{r=16}4.13 4.13 M 17.3 17.3
L‚Äão‚ÄãR‚ÄãA r=32 LoRA_{r=32}8.26 8.26 M 16.4 16.4
V‚Äãe‚ÄãR‚ÄãA r=32 VeRA_{r=32}1.74 1.74 M 22.4 22.4
V‚Äãe‚ÄãR‚ÄãA r=64 VeRA_{r=64}3.36 3.36 M 21.1 21.1
V‚Äãe‚ÄãR‚ÄãA r=128 VeRA_{r=128}6.59 6.59 M 20.2 20.2
D‚Äão‚ÄãR‚ÄãA r=8 DoRA_{r=8}2.19 2.19 M 21.2 21.2
D‚Äão‚ÄãR‚ÄãA r=16 DoRA_{r=16}4.26 4.26 M 19.1 19.1
D‚Äão‚ÄãR‚ÄãA r=32 DoRA_{r=32}8.39 8.39 M 17.8 17.8
P‚Äãi‚ÄãS‚ÄãS‚ÄãA r=8 PiSSA_{r=8}2.07 2.07 M 19.1 19.1
P‚Äãi‚ÄãS‚ÄãS‚ÄãA r=16 PiSSA_{r=16}4.13 4.13 M 16.8 16.8
P‚Äãi‚ÄãS‚ÄãS‚ÄãA r=32 PiSSA_{r=32}8.26\mathbf{8.26}M 16.0\mathbf{16.0}
S‚ÄãV‚ÄãF‚ÄãT d=8 B SVFT_{d=8}^{B}1.23 1.23 M 23.9 23.9
S‚ÄãV‚ÄãF‚ÄãT d=16 B SVFT_{d=16}^{B}2.39 2.39 M 20.4 20.4
S‚ÄãV‚ÄãF‚ÄãT d=32 B SVFT_{d=32}^{B}4.67 4.67 M 18.2 18.2
S‚ÄãV‚ÄãF‚ÄãT d=64 B SVFT_{d=64}^{B}9.03 9.03 M 16.8 16.8
S‚ÄãS‚ÄãV‚ÄãD p=40%SSVD_{p=40\%}1.51 1.51 M 18.4 18.4
S‚ÄãS‚ÄãV‚ÄãD p=60%SSVD_{p=60\%}3.67 3.67 M 17.0 17.0
S‚ÄãS‚ÄãV‚ÄãD p=80%SSVD_{p=80\%}6.57 6.57 M‚Ä†16.2 16.2‚Ä†
OWSM-1B Zero-shot‚àí-19.3 19.3
Full fine-tuning 1.01 1.01 B 12.4 12.4
L‚Äão‚ÄãR‚ÄãA r=8 LoRA_{r=8}10.56 10.56 M 17.6 17.6
L‚Äão‚ÄãR‚ÄãA r=16 LoRA_{r=16}21.13 21.13 M 16.1 16.1
L‚Äão‚ÄãR‚ÄãA r=32 LoRA_{r=32}42.26 42.26 M 15.1 15.1
V‚Äãe‚ÄãR‚ÄãA r=256 VeRA_{r=256}13.82 13.82 M 18.7 18.7
V‚Äãe‚ÄãR‚ÄãA r=384 VeRA_{r=384}20.40 20.40 M 17.7 17.7
D‚Äão‚ÄãR‚ÄãA r=8 DoRA_{r=8}11.22 11.22 M 16.7 16.7
D‚Äão‚ÄãR‚ÄãA r=16 DoRA_{r=16}21.76 21.76 M 15.3 15.3
D‚Äão‚ÄãR‚ÄãA r=32 DoRA_{r=32}42.92 42.92 M‚Ä†14.2 14.2‚Ä†
P‚Äãi‚ÄãS‚ÄãS‚ÄãA r=8 PiSSA_{r=8}10.56 10.56 M 16.8 16.8
P‚Äãi‚ÄãS‚ÄãS‚ÄãA r=16 PiSSA_{r=16}21.13 21.13 M 15.4 15.4
P‚Äãi‚ÄãS‚ÄãS‚ÄãA r=32 PiSSA_{r=32}42.26 42.26 M 14.3 14.3
S‚ÄãV‚ÄãF‚ÄãT d=8 B SVFT_{d=8}^{B}7.00 7.00 M 19.0 19.0
S‚ÄãV‚ÄãF‚ÄãT d=16 B SVFT_{d=16}^{B}13.55 13.55 M 16.3 16.3
S‚ÄãV‚ÄãF‚ÄãT d=32 B SVFT_{d=32}^{B}26.52 26.52 M 15.1 15.1
S‚ÄãV‚ÄãF‚ÄãT d=64 B SVFT_{d=64}^{B}51.88 51.88 M‚Ä†14.2 14.2‚Ä†
S‚ÄãS‚ÄãV‚ÄãD p=22%SSVD_{p=22\%}9.83 9.83 M 15.3 15.3
S‚ÄãS‚ÄãV‚ÄãD p=25%SSVD_{p=25\%}12.50 12.50 M 14.5 14.5
S‚ÄãS‚ÄãV‚ÄãD p=29%SSVD_{p=29\%}16.26 16.26 M 14.1 14.1
S‚ÄãS‚ÄãV‚ÄãD p=33%SSVD_{p=33\%}22.16 22.16 M 14.1 14.1
S‚ÄãS‚ÄãV‚ÄãD p=40%SSVD_{p=40\%}31.86 M 13.8
OWLS-2B Zero-shot‚àí-20.3 20.3
Full fine-tuning 2.30 2.30 B 13.1 13.1
L‚Äão‚ÄãR‚ÄãA r=8 LoRA_{r=8}12.69 12.69 M 18.3 18.3
L‚Äão‚ÄãR‚ÄãA r=16 LoRA_{r=16}25.39 25.39 M 16.9 16.9
L‚Äão‚ÄãR‚ÄãA r=32 LoRA_{r=32}50.78 50.78 M 15.6 15.6
V‚Äãe‚ÄãR‚ÄãA r=256 VeRA_{r=256}14.16 14.16 M 19.1 19.1
V‚Äãe‚ÄãR‚ÄãA r=384 VeRA_{r=384}20.86 20.86 M 18.0 18.0
D‚Äão‚ÄãR‚ÄãA r=8 DoRA_{r=8}13.47 13.47 M 17.1 17.1
D‚Äão‚ÄãR‚ÄãA r=16 DoRA_{r=16}26.16 26.16 M 16.1 16.1
D‚Äão‚ÄãR‚ÄãA r=32 DoRA_{r=32}51.55 51.55 M 15.2 15.2
P‚Äãi‚ÄãS‚ÄãS‚ÄãA r=8 PiSSA_{r=8}12.69 12.69 M 17.4 17.4
P‚Äãi‚ÄãS‚ÄãS‚ÄãA r=16 PiSSA_{r=16}25.39 25.39 M 16.1 16.1
P‚Äãi‚ÄãS‚ÄãS‚ÄãA r=32 PiSSA_{r=32}50.78 50.78 M‚Ä†15.0 15.0‚Ä†
S‚ÄãV‚ÄãF‚ÄãT d=8 B SVFT_{d=8}^{B}9.38 9.38 M 19.9 19.9
S‚ÄãV‚ÄãF‚ÄãT d=16 B SVFT_{d=16}^{B}18.20 18.20 M 17.3 17.3
S‚ÄãV‚ÄãF‚ÄãT d=32 B SVFT_{d=32}^{B}35.74 35.74 M 15.3 15.3
S‚ÄãS‚ÄãV‚ÄãD p=17%SSVD_{p=17\%}15.04 15.04 M 15.1 15.1
S‚ÄãS‚ÄãV‚ÄãD p=20%SSVD_{p=20\%}21.63 21.63 M‚Ä†14.7 14.7‚Ä†
S‚ÄãS‚ÄãV‚ÄãD p=25%SSVD_{p=25\%}33.88\mathbf{33.88}M 14.6\mathbf{14.6}

V Results
---------

### V-A ASR performance

![Image 1: Refer to caption](https://arxiv.org/html/2509.02830v1/trade-off.png)

Figure 1: WER (%) versus trainable parameters for PEFT methods fine-tuning OWSM-0.1B, OWSM-1B, OWLS-2B on MyST.

In Table[IV](https://arxiv.org/html/2509.02830v1#S4.T4 "TABLE IV ‚Ä£ IV-B Implementation details ‚Ä£ IV Experiments ‚Ä£ SSVD: Structured SVD for Parameter-Efficient Fine-Tuning and Benchmarking under Domain Shift in ASR"), we summarize the WERs on the MyST test data after fine-tuning the OWSM-0.1B, OWSM-1B and OWLS-2B models with different PEFT methods using various configurations. For each model the best-performing method is highlighted in bold, and the second-best is marked with a superscript ‚Ä†.

Across all three models, the zero-shot WERs remain around 20% or higher. After fine-tuning, for the smaller model OWSM-0.1B, all PEFT methods with similar parameter scales yield comparable performance, with P‚Äãi‚ÄãS‚ÄãS‚ÄãA r=32 PiSSA_{r=32} slightly outperforming others. Notably, S‚ÄãS‚ÄãV‚ÄãD p=80%SSVD_{p=80\%} achieves the second-best performance, with only a 0.2% higher WER than P‚Äãi‚ÄãS‚ÄãS‚ÄãA r=32 PiSSA_{r=32} while requiring nearly 2M fewer trainable parameters (6.57M vs. 8.26M).

![Image 2: Refer to caption](https://arxiv.org/html/2509.02830v1/trade-off-cgn.png)

Figure 2: WER (%) versus trainable parameters for PEFT methods fine-tuning OWSM-1B on CGN.

For the larger-scale OWSM-1B and OWLS-2B models, clearer performance trends show among PEFT methods. SSVD consistently outperforms other approaches while using significantly fewer trainable parameters. For example, when fine-tuning OWSM-1B, S‚ÄãS‚ÄãV‚ÄãD p=40%SSVD_{p=40\%} achieves a WER of 13.8%, approaching the 12.4% obtained by full fine-tuning, using only around 32M parameters, compared to 14.2% WER from the second best (D‚Äão‚ÄãR‚ÄãA r=32 DoRA_{r=32} and S‚ÄãV‚ÄãF‚ÄãT d=64 B SVFT_{d=64}^{B}) with approximately 52M and 43M parameters. Similarly, for the OWLS-2B model, SSVD achieves 14.7% WER with only 21.6M trainable parameters, outperforming LoRA, DoRA, and PiSSA, each of which requires around 51M parameters, demonstrating SSVD‚Äôs high efficiency in a domain-shift scenario.

![Image 3: Refer to caption](https://arxiv.org/html/2509.02830v1/conv_curve_myst.png)

Figure 3: WER (%) versus training epoch for PEFT methods fine-tuning OWSM-0.1B, OWSM-1B, OWLS-2B.

To better illustrate the trade-off between ASR performance and parameter efficiency, Figure[1](https://arxiv.org/html/2509.02830v1#S5.F1 "Figure 1 ‚Ä£ V-A ASR performance ‚Ä£ V Results ‚Ä£ SSVD: Structured SVD for Parameter-Efficient Fine-Tuning and Benchmarking under Domain Shift in ASR") presents WERs as a function of the number of trainable parameters for each PEFT method. Across all model scales, SSVD (depicted by black diamonds and dashed lines) consistently achieves lower WERs compared to other methods under similar trainable parameters (i.e., vertically). The performance gap becomes increasingly pronounced for larger models: in OWSM-1B and OWLS-2B, the SSVD curve lies significantly below those of other methods, indicating its strong ability to balance performance and efficiency, particularly in larger-scale ASR settings.

TABLE V: PEFT methods across OWSM models on CGN.

Model PEFT Method# Params WER (%) ‚Üì\downarrow
OWSM-0.1B Zero-shot‚àí-65.7 65.7
Full fine-tuning 101 101 M 17.8 17.8
L‚Äão‚ÄãR‚ÄãA r=8 LoRA_{r=8}2.07 2.07 M 25.2 25.2
L‚Äão‚ÄãR‚ÄãA r=16 LoRA_{r=16}4.13 4.13 M 21.9 21.9
L‚Äão‚ÄãR‚ÄãA r=32 LoRA_{r=32}8.26 M 19.7
D‚Äão‚ÄãR‚ÄãA r=8 DoRA_{r=8}2.19 2.19 M 24.2 24.2
D‚Äão‚ÄãR‚ÄãA r=16 DoRA_{r=16}4.26 4.26 M 21.9 21.9
D‚Äão‚ÄãR‚ÄãA r=32 DoRA_{r=32}8.39 8.39 M 20.7 20.7
P‚Äãi‚ÄãS‚ÄãS‚ÄãA r=8 PiSSA_{r=8}2.07 2.07 M 25.5 25.5
P‚Äãi‚ÄãS‚ÄãS‚ÄãA r=16 PiSSA_{r=16}4.13 4.13 M 22.1 22.1
P‚Äãi‚ÄãS‚ÄãS‚ÄãA r=32 PiSSA_{r=32}8.26 8.26 M‚Ä†20.2 20.2‚Ä†
S‚ÄãV‚ÄãF‚ÄãT d=8 B SVFT_{d=8}^{B}1.23 1.23 M 36.7 36.7
S‚ÄãV‚ÄãF‚ÄãT d=16 B SVFT_{d=16}^{B}2.39 2.39 M 32.6 32.6
S‚ÄãV‚ÄãF‚ÄãT d=32 B SVFT_{d=32}^{B}4.67 4.67 M 28.6 28.6
S‚ÄãV‚ÄãF‚ÄãT d=64 B SVFT_{d=64}^{B}9.03 9.03 M 25.0 25.0
S‚ÄãS‚ÄãV‚ÄãD p=40%SSVD_{p=40\%}1.51 1.51 M 25.8 25.8
S‚ÄãS‚ÄãV‚ÄãD p=60%SSVD_{p=60\%}3.67 3.67 M 22.9 22.9
S‚ÄãS‚ÄãV‚ÄãD p=80%SSVD_{p=80\%}6.57 6.57 M 20.9 20.9
OWSM-1B Zero-shot‚àí-46.3 46.3
Full fine-tuning 1.01 1.01 B 17.7 17.7
L‚Äão‚ÄãR‚ÄãA r=8 LoRA_{r=8}10.56 10.56 M 18.0 18.0
L‚Äão‚ÄãR‚ÄãA r=16 LoRA_{r=16}21.13 21.13 M 16.3 16.3
L‚Äão‚ÄãR‚ÄãA r=32 LoRA_{r=32}42.26\mathbf{42.26}M 15.1\mathbf{15.1}
D‚Äão‚ÄãR‚ÄãA r=8 DoRA_{r=8}11.22 11.22 M 19.6 19.6
D‚Äão‚ÄãR‚ÄãA r=16 DoRA_{r=16}21.76 21.76 M 19.1 19.1
D‚Äão‚ÄãR‚ÄãA r=32 DoRA_{r=32}42.92 42.92 M 18.4 18.4
P‚Äãi‚ÄãS‚ÄãS‚ÄãA r=8 PiSSA_{r=8}10.56 10.56 M 18.6 18.6
P‚Äãi‚ÄãS‚ÄãS‚ÄãA r=16 PiSSA_{r=16}21.13 21.13 M 16.7 16.7
P‚Äãi‚ÄãS‚ÄãS‚ÄãA r=32 PiSSA_{r=32}42.26 42.26 M‚Ä†15.3 15.3‚Ä†
S‚ÄãV‚ÄãF‚ÄãT d=8 B SVFT_{d=8}^{B}7.00 7.00 M 27.2 27.2
S‚ÄãV‚ÄãF‚ÄãT d=16 B SVFT_{d=16}^{B}13.55 13.55 M 25.1 25.1
S‚ÄãV‚ÄãF‚ÄãT d=32 B SVFT_{d=32}^{B}26.52 26.52 M 21.7 21.7
S‚ÄãV‚ÄãF‚ÄãT d=64 B SVFT_{d=64}^{B}51.88 51.88 M 19.5 19.5
S‚ÄãS‚ÄãV‚ÄãD p=22%SSVD_{p=22\%}9.83 9.83 M 18.5 18.5
S‚ÄãS‚ÄãV‚ÄãD p=29%SSVD_{p=29\%}16.26 16.26 M 16.8 16.8
S‚ÄãS‚ÄãV‚ÄãD p=40%SSVD_{p=40\%}31.86 31.86 M‚Ä†15.4 15.4‚Ä†

Moreover, as observed vertically in Figure[1](https://arxiv.org/html/2509.02830v1#S5.F1 "Figure 1 ‚Ä£ V-A ASR performance ‚Ä£ V Results ‚Ä£ SSVD: Structured SVD for Parameter-Efficient Fine-Tuning and Benchmarking under Domain Shift in ASR"), SVD-guided methods and the directionally tuned DoRA, consistently outperform LoRA when using a similar number of trainable parameters. This trend suggests that structured initialization (as in SVD-guided methods) and directional constraints (as in DoRA) are more effective than LoRA‚Äôs random initialization with unconstrained updates, a finding that aligns with prior observations in NLP and vision tasks[[23](https://arxiv.org/html/2509.02830v1#bib.bibx23), [22](https://arxiv.org/html/2509.02830v1#bib.bibx22), [24](https://arxiv.org/html/2509.02830v1#bib.bibx24)].

Table[V](https://arxiv.org/html/2509.02830v1#S5.T5 "TABLE V ‚Ä£ V-A ASR performance ‚Ä£ V Results ‚Ä£ SSVD: Structured SVD for Parameter-Efficient Fine-Tuning and Benchmarking under Domain Shift in ASR") reports the WERs on the CGN test set after fine-tuning. Since VeRA performs poorly in this scenario, we exclude it from the table. The results for OWSM-0.1B follow a similar trend as on MyST, except SVFT, which will be discussed further in Section[V-B](https://arxiv.org/html/2509.02830v1#S5.SS2 "V-B Efficiency analysis ‚Ä£ V Results ‚Ä£ SSVD: Structured SVD for Parameter-Efficient Fine-Tuning and Benchmarking under Domain Shift in ASR"). For the OWSM-1B model, full fine-tuning becomes less effective, and methods such as DoRA, which mimic full fine-tuning behavior, also yield suboptimal results.

SSVD achieves comparable WERs (15.4% vs. 15.1% from LoRA and 15.3% from PiSSA) with 10M fewer parameters (32M vs. 42M). Figure[2](https://arxiv.org/html/2509.02830v1#S5.F2 "Figure 2 ‚Ä£ V-A ASR performance ‚Ä£ V Results ‚Ä£ SSVD: Structured SVD for Parameter-Efficient Fine-Tuning and Benchmarking under Domain Shift in ASR") visualizes the WER versus trainable parameter trade-off for OWSM-1B. On CGN, LoRA outperforms DoRA and SVFT at similar scales. One explanation is that CGN data require larger update deviations, and LoRA, unlike structured methods, offers more flexibility due to its unconstrained parameter updates. Nevertheless, SSVD shows a similar trend to LoRA, particularly when a larger portion of components (e.g., at 40%) are made trainable.

### V-B Efficiency analysis

TABLE VI: Comparing SSVD constraint methods on the MyST data.

To further evaluate the training efficiency, Figure[3](https://arxiv.org/html/2509.02830v1#S5.F3 "Figure 3 ‚Ä£ V-A ASR performance ‚Ä£ V Results ‚Ä£ SSVD: Structured SVD for Parameter-Efficient Fine-Tuning and Benchmarking under Domain Shift in ASR") presents accuracy versus epoch. Since methods with close model size have similar computing time, epoch here serves as a proxy for training time. Solid lines indicate training with a uniform learning rate, e.g., 1‚Äãe‚àí4 1\mathrm{e}{-4}, while dashed lines correspond to a tenfold higher learning rate, e.g., 1‚Äãe‚àí3 1\mathrm{e}{-3}. For larger-scale models, a clear convergence hierarchy shows: SSVD >> PiSSA >> DoRA >> LoRA. The convergence speed of SVFT, however, is highly sensitive to the chosen band size. Since SVFT updates only a fixed band of singular values, it requires a larger band size to achieve convergence comparable to PiSSA, resulting in increased trainable parameters. Additionally, SVFT benefits more from higher learning rates, as it only updates singular values while keeping singular vectors fixed, limiting the flexibility of optimization under smaller learning rates.

### V-C Ablation study

In Tables[IV](https://arxiv.org/html/2509.02830v1#S4.T4 "TABLE IV ‚Ä£ IV-B Implementation details ‚Ä£ IV Experiments ‚Ä£ SSVD: Structured SVD for Parameter-Efficient Fine-Tuning and Benchmarking under Domain Shift in ASR") and[V](https://arxiv.org/html/2509.02830v1#S5.T5 "TABLE V ‚Ä£ V-A ASR performance ‚Ä£ V Results ‚Ä£ SSVD: Structured SVD for Parameter-Efficient Fine-Tuning and Benchmarking under Domain Shift in ASR"), SSVD is implemented in the ‚ÄúApprox.‚Äù variant as described in Section[III-B](https://arxiv.org/html/2509.02830v1#S3.SS2 "III-B Approximate orthogonal constraint ‚Ä£ III SSVD Method ‚Ä£ SSVD: Structured SVD for Parameter-Efficient Fine-Tuning and Benchmarking under Domain Shift in ASR"), which introduces minor orthogonality errors. As further evaluated in Table[VI](https://arxiv.org/html/2509.02830v1#S5.T6 "TABLE VI ‚Ä£ V-B Efficiency analysis ‚Ä£ V Results ‚Ä£ SSVD: Structured SVD for Parameter-Efficient Fine-Tuning and Benchmarking under Domain Shift in ASR"), we compare this variant against the ‚ÄúStrict‚Äù implementation (Section[III-A](https://arxiv.org/html/2509.02830v1#S3.SS1 "III-A Strict orthogonal constraint ‚Ä£ III SSVD Method ‚Ä£ SSVD: Structured SVD for Parameter-Efficient Fine-Tuning and Benchmarking under Domain Shift in ASR")) and a ‚ÄúNone‚Äù constraint baseline (Section[III-C](https://arxiv.org/html/2509.02830v1#S3.SS3 "III-C Unconstrained rotations ‚Ä£ III SSVD Method ‚Ä£ SSVD: Structured SVD for Parameter-Efficient Fine-Tuning and Benchmarking under Domain Shift in ASR")). The results show that ‚ÄúStrict‚Äù and ‚ÄúApprox.‚Äù yield nearly identical WERs, suggesting that the orthogonality deviations in the approximate implementation are relatively minor and do not significantly affect performance in the PEFT setting. The ‚ÄúNone‚Äù constraint implementation, as explained in Section[III-C](https://arxiv.org/html/2509.02830v1#S3.SS3 "III-C Unconstrained rotations ‚Ä£ III SSVD Method ‚Ä£ SSVD: Structured SVD for Parameter-Efficient Fine-Tuning and Benchmarking under Domain Shift in ASR"), doubles the number of trainable parameters. While increasing parameters can improve performance, the ‚ÄúNone‚Äù constraint performs worse than ‚ÄúStrict‚Äù and ‚ÄúApprox.‚Äù when the number of trainable parameters is close. For instance, with OWLS-2B, it achieves a WER of 14.9% using 30.09M parameters, higher than the 14.7% WER achieved with only 21.63M parameters.

VI Conclusion
-------------

In this work, we presented a comprehensive evaluation of PEFT methods for adapting large-scale speech foundation models under domain-shifted conditions, such as child speech and dialectal variation. We integrated and benchmarked SoTA PEFT techniques, including LoRA, DoRA, VeRA, PiSSA, SVFT, and our proposed SSVD, within ESPnet across model sizes from 0.1B to 2B. Our results highlight that while several PEFT methods perform comparably on small models, performance gaps widen at larger scales, especially under constrained compute budgets. Among all methods, SSVD consistently achieves the best trade-off between performance and parameter efficiency, closely approaching full fine-tuning performance with significantly fewer trainable parameters.

Acknowledgment
--------------

Experiments of this work used the Bridges2 system at PSC and Delta system at NCSA through allocations CIS210014 and IRI120008P from the Advanced Cyberinfrastructure Coordination Ecosystem: Services & Support (ACCESS) program, supported by National Science Foundation grants #2138259,#:2138286, #:2138307, #:2137603, and #:2138296. This research was supported by the Flemish Government under ‚ÄúOnderzoeksprogramma AI Vlaanderen‚Äù, the FWO-SBO grant S004923N: NELF, and the FWO grant V401325N.

References
----------

*   [1]Alec Radford et al. ‚ÄúRobust speech recognition via large-scale weak supervision‚Äù In _Proceedings of the International Conference on Machine Learning (ICML)_, 2023, pp. 28492‚Äì28518 
*   [2]Yifan Peng et al. ‚ÄúReproducing whisper-style training using an open-source toolkit and publicly available data‚Äù In _2023 IEEE Automatic Speech Recognition and Understanding Workshop (ASRU)_, 2023, pp. 1‚Äì8 
*   [3]Yifan Peng et al. ‚ÄúOWSM v3.1: Better and faster open Whisper-style speech models based on E-Branchformer‚Äù In _Proceedings of Interspeech_, 2024, pp. 352‚Äì356 
*   [4]Krishna C Puvvada et al. ‚ÄúLess is more: Accurate speech recognition & translation without web-scale data‚Äù In _Proceedings of Interspeech_, 2024, pp. 3964‚Äì3968 
*   [5]William Chen et al. ‚ÄúOWLS: Scaling laws for multilingual speech recognition and translation models‚Äù In _Proceedings of the International Conference on Machine Learning (ICML)_, 2025 
*   [6]Ruchao Fan, Natarajan Balaji Shankar and Abeer Alwan ‚ÄúBenchmarking children‚Äôs ASR with supervised and self-supervised speech foundation models‚Äù In _Proceedings of Interspeech_, 2024, pp. 5173‚Äì5177 
*   [7]Pu Wang and Hugo Van hamme ‚ÄúBenefits of pre-trained mono-and cross-lingual speech representations for spoken language understanding of Dutch dysarthric speech‚Äù In _EURASIP Journal on Audio, Speech, and Music Processing_ 2023.1 Springer, 2023, pp. 15 
*   [8]Yaroslav Getman, Tam√°s Gr√≥sz, Katri Hiovain-Asikainen and Mikko Kurimo ‚ÄúExploring adaptation techniques of large speech foundation models for low-resource ASR: a case study on Northern S√°mi‚Äù In _Proceedings of Interspeech_, 2024, pp. 2539‚Äì2543 
*   [9]Neil Houlsby et al. ‚ÄúParameter-efficient transfer learning for NLP‚Äù In _Proceedings of the International Conference on Machine Learning (ICML)_, 2019, pp. 2790‚Äì2799 
*   [10]Brian Lester, Rami Al-Rfou and Noah Constant ‚ÄúThe power of scale for parameter-efficient prompt tuning‚Äù In _Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing (EMNLP)_, 2021, pp. 3045‚Äì3059 
*   [11]Elad Ben Zaken, Yoav Goldberg and Shauli Ravfogel ‚ÄúBitFit: Simple parameter-efficient fine-tuning for Transformer-based masked language-models‚Äù In _Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (ACL) (Volume 2: Short Papers)_, 2022, pp. 1‚Äì9 
*   [12]Edward J. Hu et al. ‚ÄúLoRA: Low-rank adaptation of large language models‚Äù In _Proceedings of the International Conference on Learning Representations (ICLR)_, 2022 
*   [13]Yuhui Gu et al. ‚ÄúQA-LoRA: Quantization-aware low-rank adaptation of large language models‚Äù In _Proceedings of the International Conference on Learning Representations (ICLR)_, 2023 
*   [14]Qingru Zhang et al. ‚ÄúAdaptive budget allocation for parameter-efficient fine-tuning‚Äù In _Proceedings of the International Conference on Learning Representations (ICLR)_, 2023 
*   [15]Pranay Dighe et al. ‚ÄúLeveraging large language models for exploiting asr uncertainty‚Äù In _ICASSP 2024-2024 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)_, 2024, pp. 12231‚Äì12235 IEEE 
*   [16]Arun Baby, George Joseph and Shatrughan Singh ‚ÄúRobust speaker personalisation using generalized low-rank adaptation for automatic speech recognition‚Äù In _ICASSP 2024-2024 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)_, 2024, pp. 11381‚Äì11385 IEEE 
*   [17]Pu Wang and Hugo Van hamme ‚ÄúBottleneck low-rank Transformers for low-resource spoken language understanding‚Äù In _Proceedings of Interspeech_, 2022, pp. 1248‚Äì1252 
*   [18]Wei Liu, Ying Qin, Zhiyuan Peng and Tan Lee ‚ÄúSparsely shared lora on whisper for child speech recognition‚Äù In _ICASSP 2024-2024 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)_, 2024, pp. 11751‚Äì11755 IEEE 
*   [19]Tianyi Xu et al. ‚ÄúTowards rehearsal-free multilingual ASR: A LoRA-based case study on Whisper‚Äù In _Proceedings of Interspeech_, 2024, pp. 2534‚Äì2538 
*   [20]Zheshu Song et al. ‚ÄúLoRA-Whisper: Parameter-efficient and extensible multilingual ASR‚Äù In _Proceedings of Interspeech_, 2024, pp. 3934‚Äì3938 
*   [21]Dawid Jan Kopiczko, Tijmen Blankevoort and Yuki M. Asano ‚ÄúVeRA: Vector-based random matrix adaptation‚Äù In _Proceedings of the International Conference on Learning Representations (ICLR)_, 2024 
*   [22]Fanxu Meng, Zhaohui Wang and Muhan Zhang ‚ÄúPiSSA: Principal singular values and singular vectors adaptation of large language models‚Äù In _Advances in Neural Information Processing Systems (NeurIPS)_ 37, 2024, pp. 121038‚Äì121072 
*   [23]Shih-Yang Liu et al. ‚ÄúDoRA: Weight-decomposed low-rank adaptation‚Äù In _Proceedings of the International Conference on Machine Learning (ICML)_, 2024 
*   [24]Vijay Chandra Lingam et al. ‚ÄúSVFT: Parameter-efficient fine-tuning with singular vectors‚Äù In _Advances in Neural Information Processing Systems (NeurIPS)_ 37, 2024, pp. 41425‚Äì41446 
*   [25]Shinji Watanabe et al. ‚ÄúESPnet: End-to-end speech processing toolkit‚Äù In _Proceedings of Interspeech_, 2018 
*   [26]Carl Eckart and Gale Young ‚ÄúThe approximation of one matrix by another of lower rank‚Äù In _Psychometrika_ 1.3 Springer, 1936, pp. 211‚Äì218 
*   [27]Asher Trockman and J Zico Kolter ‚ÄúOrthogonalizing convolutional layers with the Cayley Transform‚Äù In _Proceedings of the International Conference on Learning Representations (ICLR)_, 2021 
*   [28]Sameer Pradhan and et al. ‚ÄúMy Science Tutor (MyST): A large corpus of children‚Äôs conversational speech‚Äù In _Proceedings of the Joint International Conference on Computational Linguistics, Language Resources and Evaluation (LREC-COLING)_, 2024, pp. 12040‚Äì12045 
*   [29]Nelleke Oostdijk and et al. ‚ÄúThe Spoken Dutch Corpus: Overview and first evaluation‚Äù In _Proceedings of the 2nd International Conference on Language Resources and Evaluation (LREC)_, 2000, pp. 887‚Äì894 
*   [30]Ahmed Adel Attia and et al. ‚ÄúKid-Whisper: Bridging the performance gap in automatic speech recognition for children‚Äù In _Proceedings of the AAAI/ACM Conference on AI, Ethics, and Society (AIES)_ 7, 2024, pp. 74‚Äì80 
*   [31]Kwangyoun Kim et al. ‚ÄúE-branchformer: Branchformer with enhanced merging for speech recognition‚Äù In _2022 IEEE Spoken Language Technology Workshop (SLT)_, 2023, pp. 84‚Äì91 IEEE 
*   [32]Ashish Vaswani et al. ‚ÄúAttention is all you need‚Äù In _Advances in Neural Information Processing Systems (NeurIPS)_ 30, 2017
