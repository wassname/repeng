Title: ETHER: Efficient Finetuning of Large-Scale Models with Hyperplane Reflections

URL Source: https://arxiv.org/html/2405.20271v1

Published Time: Fri, 31 May 2024 01:05:24 GMT

Markdown Content:
###### Abstract

Parameter-efficient finetuning (PEFT) has become ubiquitous to adapt foundation models to downstream task requirements while retaining their generalization ability. However, the amount of additionally introduced parameters and compute for successful adaptation and hyperparameter searches can explode quickly, especially when deployed at scale to serve numerous individual requests. To ensure effective, parameter-efficient, and hyperparameter-robust adaptation, we propose the ETHER transformation family, which performs E fficient fine T uning via H yp E rplane R eflections. By design, ETHER transformations require a minimal number of parameters, are less likely to deteriorate model performance, and exhibit robustness to hyperparameter and learning rate choices. In particular, we introduce ETHER and its relaxation ETHER+, which match or outperform existing PEFT methods with significantly fewer parameters (‚àºsimilar-to\sim‚àº10 10 10 10-100 100 100 100 times lower than LoRA or OFT) across multiple image synthesis and natural language tasks without exhaustive hyperparameter tuning. Finally, we investigate the recent emphasis on Hyperspherical Energy retention for adaptation and raise questions on its practical utility. The code is available at [https://github.com/mwbini/ether](https://github.com/mwbini/ether).

Machine Learning, ICML

1 Introduction
--------------

Recently, large-scale foundation models (Bommasani et al., [2021](https://arxiv.org/html/2405.20271v1#bib.bib2)) have demonstrated impressive general-purpose capabilities across both generative and discriminative tasks(Rombach et al., [2022](https://arxiv.org/html/2405.20271v1#bib.bib53); Touvron et al., [2023a](https://arxiv.org/html/2405.20271v1#bib.bib61); OpenAI, [2023](https://arxiv.org/html/2405.20271v1#bib.bib46); Kirillov et al., [2023](https://arxiv.org/html/2405.20271v1#bib.bib26)), showing extensive flexibility and strong performance when further adapted to different, more specialized tasks such as instruction following or controlled image synthesis(Zhang & Agrawala, [2023](https://arxiv.org/html/2405.20271v1#bib.bib69); Ruiz et al., [2022](https://arxiv.org/html/2405.20271v1#bib.bib55); Taori et al., [2023](https://arxiv.org/html/2405.20271v1#bib.bib60); Chiang et al., [2023](https://arxiv.org/html/2405.20271v1#bib.bib6)).

While impressive, these capabilities come with parameter counts increasing into the billions (OpenAI, [2023](https://arxiv.org/html/2405.20271v1#bib.bib46); Podell et al., [2023a](https://arxiv.org/html/2405.20271v1#bib.bib48); Touvron et al., [2023b](https://arxiv.org/html/2405.20271v1#bib.bib62)). To allow for affordable and scalable model adaptation that can serve large and diverse client bases, various techniques have been introduced in the literature. They range from full finetuning(Zhao et al., [2024](https://arxiv.org/html/2405.20271v1#bib.bib75); Zhang et al., [2023a](https://arxiv.org/html/2405.20271v1#bib.bib68); Stojanovski et al., [2022](https://arxiv.org/html/2405.20271v1#bib.bib59)) to just a few layers of the pretrained model (Kornblith et al., [2019](https://arxiv.org/html/2405.20271v1#bib.bib29)), concatenating additional learning modules (Houlsby et al., [2019](https://arxiv.org/html/2405.20271v1#bib.bib18); Pfeiffer et al., [2020](https://arxiv.org/html/2405.20271v1#bib.bib47); Mou et al., [2023](https://arxiv.org/html/2405.20271v1#bib.bib44)), and more recently to adapters on the network weights with lightweight learnable transformations (Qiu et al., [2023](https://arxiv.org/html/2405.20271v1#bib.bib50); Hu et al., [2022](https://arxiv.org/html/2405.20271v1#bib.bib20); Kopiczko et al., [2023](https://arxiv.org/html/2405.20271v1#bib.bib28); Valipour et al., [2023](https://arxiv.org/html/2405.20271v1#bib.bib63)). The latter have proven particularly effective, introducing no inference latency, fewer adaptation parameters, and strong performance.

Conceptually, these methods finetune on smaller datasets to adapt to downstream task and data requirements, without (1) compromising too much on the costly pretraining and (2) incurring concept and semantic drifts by catastrophically overwriting pretrained weights (Kirkpatrick et al., [2017](https://arxiv.org/html/2405.20271v1#bib.bib27); Lee et al., [2019](https://arxiv.org/html/2405.20271v1#bib.bib31); Lu et al., [2020](https://arxiv.org/html/2405.20271v1#bib.bib41); Mehta et al., [2022](https://arxiv.org/html/2405.20271v1#bib.bib43); Ruiz et al., [2023](https://arxiv.org/html/2405.20271v1#bib.bib56); Ke et al., [2023](https://arxiv.org/html/2405.20271v1#bib.bib25); Roth et al., [2024](https://arxiv.org/html/2405.20271v1#bib.bib54); Garg et al., [2024](https://arxiv.org/html/2405.20271v1#bib.bib12); Ibrahim et al., [2024](https://arxiv.org/html/2405.20271v1#bib.bib21)). Treading the line for a suitable trade-off between adaptation and retention of the foundational model capabilities thus presents itself as a difficult task to tackle, often requiring costly tuning of hyperparameters such as learning rates. This problem is acknowledged explicitly in Li et al. ([2018](https://arxiv.org/html/2405.20271v1#bib.bib33)); Chen et al. ([2023a](https://arxiv.org/html/2405.20271v1#bib.bib4)); Gouk et al. ([2021](https://arxiv.org/html/2405.20271v1#bib.bib13)) aiming to preserve Euclidean weight distances between pretrained and finetuned models, and implicitly with approaches opting for both lower learning rates (at the cost of more tuning iterations) and inclusion of tuning parameters via summation (Qiu et al., [2023](https://arxiv.org/html/2405.20271v1#bib.bib50)).

In particular, Qiu et al. ([2023](https://arxiv.org/html/2405.20271v1#bib.bib50)) hints that a Euclidean distance measure likely fails to fully capture the preservation of the network‚Äôs ability, suggesting instead Hyperspherical Energy (HE) as an alternative measure. The resulting objective uses orthogonal transformations (OFT) for multiplicative weight changes that control HE. Still, even OFT requires specific and restricted hyperparameter choices such as small learning rates and initialization from identity matrices to ensure sufficient knowledge preservation. In addition, while more robust and stable for finetuning in controllable generation settings compared to LoRA (Qiu et al., [2023](https://arxiv.org/html/2405.20271v1#bib.bib50)), OFT comes with a high computational overhead due to matrix multiplication and a large number of tuning parameters.

In this work, we propose E fficient fine T uning via H yp E rplane R eflections (ETHER) - a new family of weight transformations, efficient in parameter count while preserving model abilities and being robust in convergence and learning rate choices. By default, ETHER transformations frame the tuning process as a search for suitable hyperplanes, along which weight vectors can be reflected based on the orthogonal Householder transformation (Householder, [1958](https://arxiv.org/html/2405.20271v1#bib.bib19)). This keeps the distance to the transformation neutral element - the identity matrix - constant by construction and improves training stability while reducing the chance of deteriorating model performance. In addition, being built from single vectors, Householder transformations allow for efficient block-parallel matrix multiplication with minimal performance trade-offs.

However, situations may arise where the hard distance restriction of ETHER can prove suboptimal (such as for subject-driven image generation, where finegrained subject-specific semantics need to be retained). As such, we augment the ETHER family with ETHER+ - a relaxation on the default ETHER method. More precisely, ETHER+ derives from the Householder transformation, but breaks the orthogonality and constant distance constraints, introducing multiple hyperplanes that can interact with a weight vector. As a result, ETHER+ allows for more controlled and finegrained adaptation, while still having a bounded distance to the transformation neutral element, and retaining the ETHER benefits of high parameter-efficiency, training stability, and hyperparameter robustness.

Indeed, across subject-driven image generation, controlled image synthesis, natural language understanding and instruction tuning tasks, we find that ETHER and especially ETHER+ match and outperform existing methods using only a few additional tuning parameters (e.g. 100√ó100\times 100 √ó less than OFT when finetuning Stable Diffusion for controlled image synthesis) - all while presenting stronger learning rate robustness compared to other methods and consequently requiring minimal hyperparameter tuning to achieve strong performance (c.f. Sec.[4](https://arxiv.org/html/2405.20271v1#S4 "4 Intriguing Properties of ETHER ‚Ä£ ETHER: Efficient Finetuning of Large-Scale Models with Hyperplane Reflections")). Finally, we also utilize our experimental benchmark findings to further investigate and question the recent emphasis on transformation orthogonality and hyperspherical energy (HE) retention (e.g. Qiu et al. ([2023](https://arxiv.org/html/2405.20271v1#bib.bib50))), showing how non-orthogonal ETHER+ can achieve strong performance while displaying increased HE.

2 Related Work
--------------

Parameter-Efficient Finetuning (PEFT). PEFT of pretrained models has seen different strategies evolve in the past years - starting from finetuning protocols and concatenation of learnable modules(Houlsby et al., [2019](https://arxiv.org/html/2405.20271v1#bib.bib18); Lester et al., [2021](https://arxiv.org/html/2405.20271v1#bib.bib32); Li & Liang, [2021](https://arxiv.org/html/2405.20271v1#bib.bib34); Pfeiffer et al., [2020](https://arxiv.org/html/2405.20271v1#bib.bib47); Guo et al., [2021](https://arxiv.org/html/2405.20271v1#bib.bib14)) to more recently reparametrization of network weights with efficient transformations(Qiu et al., [2023](https://arxiv.org/html/2405.20271v1#bib.bib50); Hu et al., [2022](https://arxiv.org/html/2405.20271v1#bib.bib20); Kopiczko et al., [2023](https://arxiv.org/html/2405.20271v1#bib.bib28); Valipour et al., [2023](https://arxiv.org/html/2405.20271v1#bib.bib63); Zhang et al., [2023c](https://arxiv.org/html/2405.20271v1#bib.bib71)). The latter have shown convincing trade-offs between adaptation quality, additional parameters, and inference latency. LoRA (Hu et al., [2022](https://arxiv.org/html/2405.20271v1#bib.bib20)) transforms network weights by adding the result of a learnable, low-rank matrix product. On top of LoRA, multiple variations have been proposed, s.a. QLora(Dettmers et al., [2023](https://arxiv.org/html/2405.20271v1#bib.bib8)) with quantized weights, AdaLoRA(Zhang et al., [2023c](https://arxiv.org/html/2405.20271v1#bib.bib71)) with dynamic rank adjustment, and VeRA (Kopiczko et al., [2023](https://arxiv.org/html/2405.20271v1#bib.bib28)) with low-rank frozen random projections and trainable vectors to reduce parameter counts. OFT (Qiu et al., [2023](https://arxiv.org/html/2405.20271v1#bib.bib50)) instead learns matrix multiplier with orthogonality constraints to retain hyperspherical energy. In our work, we use the same paradigm but introduce hyperplane reflections for better parameter efficiency and learning rate robustness.

Controlling Diffusion Generative Models. Diffusion-based generative models show strong compositional generation (Rombach et al., [2022](https://arxiv.org/html/2405.20271v1#bib.bib53); Mukhopadhyay et al., [2023](https://arxiv.org/html/2405.20271v1#bib.bib45); Podell et al., [2023b](https://arxiv.org/html/2405.20271v1#bib.bib49); Karthik et al., [2023](https://arxiv.org/html/2405.20271v1#bib.bib24); Saharia et al., [2022](https://arxiv.org/html/2405.20271v1#bib.bib57)). Among these, Gal et al. ([2022](https://arxiv.org/html/2405.20271v1#bib.bib10)); Ruiz et al. ([2023](https://arxiv.org/html/2405.20271v1#bib.bib56)) popularized personalized generation - teaching models to generate variations of user-provided samples. Based on DreamBooth (Ruiz et al., [2023](https://arxiv.org/html/2405.20271v1#bib.bib56)), other works (Liu et al., [2023b](https://arxiv.org/html/2405.20271v1#bib.bib39); Richardson et al., [2023](https://arxiv.org/html/2405.20271v1#bib.bib52); Zhang et al., [2023e](https://arxiv.org/html/2405.20271v1#bib.bib74)) followed. ControlNet (Zhang et al., [2023b](https://arxiv.org/html/2405.20271v1#bib.bib70)) shows model controllability through external signals s.a. semantic and depth maps or face landmarks via extra layers at the cost of higher inference latency. Qiu et al. ([2023](https://arxiv.org/html/2405.20271v1#bib.bib50)) show controllability through direct finetuning with learnable matrix-multiplication transformations. Our work suggests an alternative, more robust and parameter-efficient approach through hyperplane reflections.

Instruction Tuning Language Models. Large Language Models (LLMs) have shown striking generalization across a wide range of tasks (Zhao et al., [2023](https://arxiv.org/html/2405.20271v1#bib.bib76); Zhang et al., [2023d](https://arxiv.org/html/2405.20271v1#bib.bib73); OpenAI, [2023](https://arxiv.org/html/2405.20271v1#bib.bib46); Touvron et al., [2023a](https://arxiv.org/html/2405.20271v1#bib.bib61)). However, the default training objective often does not exactly match downstream task requirements and intentions. To address this mismatch, Instruction Tuning (Wang et al., [2023](https://arxiv.org/html/2405.20271v1#bib.bib65); Zhang et al., [2023d](https://arxiv.org/html/2405.20271v1#bib.bib73); [Longpre et al.,](https://arxiv.org/html/2405.20271v1#bib.bib40); Taori et al., [2023](https://arxiv.org/html/2405.20271v1#bib.bib60)) finetunes LLMs using additional (Instruction, Output) pairs to explicitly align the model with human preferences. This enhances capabilities and controllability while avoiding costly retraining (K√∂pf et al., [2023](https://arxiv.org/html/2405.20271v1#bib.bib30)). Recently, methods based on LoRA (Hu et al., [2022](https://arxiv.org/html/2405.20271v1#bib.bib20)) have been proposed to efficiently achieve this control (Dettmers et al., [2023](https://arxiv.org/html/2405.20271v1#bib.bib8); Xu et al., [2023](https://arxiv.org/html/2405.20271v1#bib.bib67); Chen et al., [2023b](https://arxiv.org/html/2405.20271v1#bib.bib5); Valipour et al., [2023](https://arxiv.org/html/2405.20271v1#bib.bib63); Kopiczko et al., [2023](https://arxiv.org/html/2405.20271v1#bib.bib28)). This work proposes a strong alternative with further parameter-efficiency and high learning rate robustness.

3 Method
--------

We first discuss adapter-based PEFT in ¬ß[3.1](https://arxiv.org/html/2405.20271v1#S3.SS1 "3.1 Preliminaries ‚Ä£ 3 Method ‚Ä£ ETHER: Efficient Finetuning of Large-Scale Models with Hyperplane Reflections"), before describing and motivating the use of hyperplane reflections in ETHER (¬ß[3.2](https://arxiv.org/html/2405.20271v1#S3.SS2 "3.2 ETHER: Finetuning with Hyperplane Reflections ‚Ä£ 3 Method ‚Ä£ ETHER: Efficient Finetuning of Large-Scale Models with Hyperplane Reflections")). To encourage flexibility in trainable control and adaptation, we propose a simple, yet effective relaxation ETHER+ in ¬ß[3.3](https://arxiv.org/html/2405.20271v1#S3.SS3 "3.3 Relaxing Orthogonality in ETHER ‚Ä£ 3 Method ‚Ä£ ETHER: Efficient Finetuning of Large-Scale Models with Hyperplane Reflections"). Finally, ¬ß[3.4](https://arxiv.org/html/2405.20271v1#S3.SS4 "3.4 Efficient ETHER through Block-Parallelism ‚Ä£ 3 Method ‚Ä£ ETHER: Efficient Finetuning of Large-Scale Models with Hyperplane Reflections") describes block-diagonal ETHER for improved computational efficiency.

### 3.1 Preliminaries

##### Parameter-Efficient Finetuning with Adapters.

The most commonly deployed form of PEFT with an adapter is Low-rank Adaptation (LoRA, Hu et al. ([2022](https://arxiv.org/html/2405.20271v1#bib.bib20))). LoRA parametrizes a change of pretrained weights W ùëä W italic_W as

(W+B‚Å¢A)‚ä∫‚Å¢x+b superscript ùëä ùêµ ùê¥‚ä∫ùë• ùëè(W+BA)^{\intercal}x+b\\ ( italic_W + italic_B italic_A ) start_POSTSUPERSCRIPT ‚ä∫ end_POSTSUPERSCRIPT italic_x + italic_b

where B‚Å¢A ùêµ ùê¥ BA italic_B italic_A is the matrix product of two low-rank matrices, i.e. for W‚àà‚Ñù d√óf ùëä superscript ‚Ñù ùëë ùëì W\in\mathbb{R}^{d\times f}italic_W ‚àà blackboard_R start_POSTSUPERSCRIPT italic_d √ó italic_f end_POSTSUPERSCRIPT, A‚àà‚Ñù d√ór ùê¥ superscript ‚Ñù ùëë ùëü A\in\mathbb{R}^{d\times r}italic_A ‚àà blackboard_R start_POSTSUPERSCRIPT italic_d √ó italic_r end_POSTSUPERSCRIPT and B‚àà‚Ñù r√óf ùêµ superscript ‚Ñù ùëü ùëì B\in\mathbb{R}^{r\times f}italic_B ‚àà blackboard_R start_POSTSUPERSCRIPT italic_r √ó italic_f end_POSTSUPERSCRIPT. When rank r<<min‚Å¢(d,f)much-less-than ùëü min ùëë ùëì r<<\text{min}(d,f)italic_r << min ( italic_d , italic_f ), this can bring down required tuning parameters significantly compared to full finetuning. In addition, B‚Å¢A ùêµ ùê¥ BA italic_B italic_A can be absorbed into W ùëä W italic_W during inference to avoid additional latency.

##### Orthogonal Finetuning (OFT).

However, finetuning with LoRA can incur significant, potentially catastrophic weight changes. To ensure better preservation of pretrained model weights, Qiu et al. ([2023](https://arxiv.org/html/2405.20271v1#bib.bib50)) propose Orthogonal Finetuning (OFT). Based on the hypothesis that Hyperspherical Energy (HE) needs to be kept unaltered to preserve the original model abilities, OFT proposes the usage of multiplicative orthogonal transformations on the model weights. By retaining pairwise weight angles, HE can remain unaffected. However, to work in practice, Qiu et al. ([2023](https://arxiv.org/html/2405.20271v1#bib.bib50)) require the construction of the orthogonal matrix Q ùëÑ Q italic_Q via a Cayley parametrization Q=(I+S)‚Å¢(I‚àíS)‚àí1 ùëÑ ùêº ùëÜ superscript ùêº ùëÜ 1 Q=(I+S)(I-S)^{-1}italic_Q = ( italic_I + italic_S ) ( italic_I - italic_S ) start_POSTSUPERSCRIPT - 1 end_POSTSUPERSCRIPT, where S ùëÜ S italic_S is skew-symmetric. Notice that by using this parametrization, they limit the range of possible orthogonal matrices to those with determinant 1, missing orthogonal matrices with determinant equal to ‚àí1 1-1- 1. As we show, this is relevant, as it excludes reflections, which motivate ETHER. To make OFT more parameter efficient, the orthogonal matrix Q‚àà‚Ñù d√ód ùëÑ superscript ‚Ñù ùëë ùëë Q\in\mathbb{R}^{d\times d}italic_Q ‚àà blackboard_R start_POSTSUPERSCRIPT italic_d √ó italic_d end_POSTSUPERSCRIPT is built in a block-diagonal fashion, made up of n ùëõ n italic_n smaller blocks Q b superscript ùëÑ ùëè Q^{b}italic_Q start_POSTSUPERSCRIPT italic_b end_POSTSUPERSCRIPT of size d n√ód n ùëë ùëõ ùëë ùëõ\frac{d}{n}\times\frac{d}{n}divide start_ARG italic_d end_ARG start_ARG italic_n end_ARG √ó divide start_ARG italic_d end_ARG start_ARG italic_n end_ARG. The final OFT transformation on the forward pass can then be described as

(Q B‚Å¢W)‚ä∫‚Å¢x+b superscript superscript ùëÑ ùêµ ùëä‚ä∫ùë• ùëè(Q^{B}W)^{\intercal}x+b( italic_Q start_POSTSUPERSCRIPT italic_B end_POSTSUPERSCRIPT italic_W ) start_POSTSUPERSCRIPT ‚ä∫ end_POSTSUPERSCRIPT italic_x + italic_b

with block-diagonal Q B superscript ùëÑ ùêµ Q^{B}italic_Q start_POSTSUPERSCRIPT italic_B end_POSTSUPERSCRIPT. The trainable parameters are the n ùëõ n italic_n matrices Q b‚àà‚Ñù d n√ód n superscript ùëÑ ùëè superscript ‚Ñù ùëë ùëõ ùëë ùëõ Q^{b}\in\mathbb{R}^{\frac{d}{n}\times\frac{d}{n}}italic_Q start_POSTSUPERSCRIPT italic_b end_POSTSUPERSCRIPT ‚àà blackboard_R start_POSTSUPERSCRIPT divide start_ARG italic_d end_ARG start_ARG italic_n end_ARG √ó divide start_ARG italic_d end_ARG start_ARG italic_n end_ARG end_POSTSUPERSCRIPT that compose Q B superscript ùëÑ ùêµ Q^{B}italic_Q start_POSTSUPERSCRIPT italic_B end_POSTSUPERSCRIPT - more specifically the matrices R b superscript ùëÖ ùëè R^{b}italic_R start_POSTSUPERSCRIPT italic_b end_POSTSUPERSCRIPT that build the skew-symmetric matrices S b=1 2‚Å¢(R b‚àí(R b)‚ä∫)superscript ùëÜ ùëè 1 2 superscript ùëÖ ùëè superscript superscript ùëÖ ùëè‚ä∫S^{b}=\frac{1}{2}(R^{b}-(R^{b})^{\intercal})italic_S start_POSTSUPERSCRIPT italic_b end_POSTSUPERSCRIPT = divide start_ARG 1 end_ARG start_ARG 2 end_ARG ( italic_R start_POSTSUPERSCRIPT italic_b end_POSTSUPERSCRIPT - ( italic_R start_POSTSUPERSCRIPT italic_b end_POSTSUPERSCRIPT ) start_POSTSUPERSCRIPT ‚ä∫ end_POSTSUPERSCRIPT ) for Q b superscript ùëÑ ùëè Q^{b}italic_Q start_POSTSUPERSCRIPT italic_b end_POSTSUPERSCRIPT. For finetuning, the R b superscript ùëÖ ùëè R^{b}italic_R start_POSTSUPERSCRIPT italic_b end_POSTSUPERSCRIPT are initialized as zero, such that Q B|0=I evaluated-at superscript ùëÑ ùêµ 0 ùêº Q^{B}|_{0}=I italic_Q start_POSTSUPERSCRIPT italic_B end_POSTSUPERSCRIPT | start_POSTSUBSCRIPT 0 end_POSTSUBSCRIPT = italic_I and consequently Q B|0‚Å¢W=W evaluated-at superscript ùëÑ ùêµ 0 ùëä ùëä Q^{B}|_{0}W=W italic_Q start_POSTSUPERSCRIPT italic_B end_POSTSUPERSCRIPT | start_POSTSUBSCRIPT 0 end_POSTSUBSCRIPT italic_W = italic_W at the beginning of finetuning.

### 3.2 ETHER: Finetuning with Hyperplane Reflections

Fundamentally, ETHER (E fficient fine T uning via H yp E rplane R eflections) sets up weight transformations as hyperplane reflections. These reflections can be obtained via the Householder transformation matrix H‚àà‚Ñù d√ód ùêª superscript ‚Ñù ùëë ùëë H\in\mathbb{R}^{d\times d}italic_H ‚àà blackboard_R start_POSTSUPERSCRIPT italic_d √ó italic_d end_POSTSUPERSCRIPT with

H=I‚àí2‚Å¢u‚Å¢u‚ä∫ùêª ùêº 2 ùë¢ superscript ùë¢‚ä∫H=I-2uu^{\intercal}italic_H = italic_I - 2 italic_u italic_u start_POSTSUPERSCRIPT ‚ä∫ end_POSTSUPERSCRIPT(1)

with u‚àà‚Ñù d ùë¢ superscript ‚Ñù ùëë u\in\mathbb{R}^{d}italic_u ‚àà blackboard_R start_POSTSUPERSCRIPT italic_d end_POSTSUPERSCRIPT the hyperplane unit normal vector and the corresponding outer product u‚Å¢u‚ä∫ùë¢ superscript ùë¢‚ä∫uu^{\intercal}italic_u italic_u start_POSTSUPERSCRIPT ‚ä∫ end_POSTSUPERSCRIPT. The reflection can be easily intuited when applied to a weight vector w‚àà‚Ñù d ùë§ superscript ‚Ñù ùëë w\in\mathbb{R}^{d}italic_w ‚àà blackboard_R start_POSTSUPERSCRIPT italic_d end_POSTSUPERSCRIPT:

H‚Å¢w=(I‚àí2‚Å¢u‚Å¢u‚ä∫)‚Å¢w=w‚àí2‚Å¢u‚Å¢(u‚ä∫‚Å¢w).ùêª ùë§ ùêº 2 ùë¢ superscript ùë¢‚ä∫ùë§ ùë§ 2 ùë¢ superscript ùë¢‚ä∫ùë§ Hw=(I-2uu^{\intercal})w=w-2u(u^{\intercal}w).italic_H italic_w = ( italic_I - 2 italic_u italic_u start_POSTSUPERSCRIPT ‚ä∫ end_POSTSUPERSCRIPT ) italic_w = italic_w - 2 italic_u ( italic_u start_POSTSUPERSCRIPT ‚ä∫ end_POSTSUPERSCRIPT italic_w ) .

Transformation H ùêª H italic_H effectively subtracts twice the component of w ùë§ w italic_w projected on u ùë¢ u italic_u, thereby reflecting it with respect to the hyperplane defined by u ùë¢ u italic_u (see Fig.[1](https://arxiv.org/html/2405.20271v1#S3.F1 "Figure 1 ‚Ä£ 3.2 ETHER: Finetuning with Hyperplane Reflections ‚Ä£ 3 Method ‚Ä£ ETHER: Efficient Finetuning of Large-Scale Models with Hyperplane Reflections")). By construction, hyperplane reflections are well-suited for the efficient finetuning of pretrained models, as they keep the distance to the transformation neutral element - the identity matrix - constant, which minimizes the risk of divergence from the pretrained model and deterioration of model performance (c.f. Fig.[4](https://arxiv.org/html/2405.20271v1#S3.F4 "Figure 4 ‚Ä£ 3.4 Efficient ETHER through Block-Parallelism ‚Ä£ 3 Method ‚Ä£ ETHER: Efficient Finetuning of Large-Scale Models with Hyperplane Reflections")). This can be easily shown by computing the Frobenius norm of the difference between the Householder matrix H and the identity matrix I:

‚ÄñH‚àíI‚ÄñF=‚ÄñI‚àí2‚Å¢u‚Å¢u‚ä∫‚àíI‚ÄñF=2‚ãÖ‚Äñu‚Å¢u‚ä∫‚ÄñF=2 subscript norm ùêª ùêº ùêπ subscript norm ùêº 2 ùë¢ superscript ùë¢‚ä∫ùêº ùêπ‚ãÖ2 subscript norm ùë¢ superscript ùë¢‚ä∫ùêπ 2\left\|H-I\right\|_{F}=\left\|I-2uu^{\intercal}-I\right\|_{F}=2\cdot\left\|uu^% {\intercal}\right\|_{F}=2‚à• italic_H - italic_I ‚à• start_POSTSUBSCRIPT italic_F end_POSTSUBSCRIPT = ‚à• italic_I - 2 italic_u italic_u start_POSTSUPERSCRIPT ‚ä∫ end_POSTSUPERSCRIPT - italic_I ‚à• start_POSTSUBSCRIPT italic_F end_POSTSUBSCRIPT = 2 ‚ãÖ ‚à• italic_u italic_u start_POSTSUPERSCRIPT ‚ä∫ end_POSTSUPERSCRIPT ‚à• start_POSTSUBSCRIPT italic_F end_POSTSUBSCRIPT = 2(2)

The above equation leverages the fact that for any matrix M ùëÄ M italic_M

‚ÄñM‚ÄñF=Tr‚Å¢(M‚Å¢M‚ä∫)subscript norm ùëÄ ùêπ Tr ùëÄ superscript ùëÄ‚ä∫\left\|M\right\|_{F}=\sqrt{\text{Tr}(MM^{\intercal})}‚à• italic_M ‚à• start_POSTSUBSCRIPT italic_F end_POSTSUBSCRIPT = square-root start_ARG Tr ( italic_M italic_M start_POSTSUPERSCRIPT ‚ä∫ end_POSTSUPERSCRIPT ) end_ARG

and that with M=u‚Å¢u‚ä∫ùëÄ ùë¢ superscript ùë¢‚ä∫M=uu^{\intercal}italic_M = italic_u italic_u start_POSTSUPERSCRIPT ‚ä∫ end_POSTSUPERSCRIPT and u ùë¢ u italic_u having unit length u 1 2+u 2 2+‚Ä¶+u d 2=1 superscript subscript ùë¢ 1 2 superscript subscript ùë¢ 2 2‚Ä¶superscript subscript ùë¢ ùëë 2 1 u_{1}^{2}+u_{2}^{2}+...+u_{d}^{2}=1 italic_u start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT start_POSTSUPERSCRIPT 2 end_POSTSUPERSCRIPT + italic_u start_POSTSUBSCRIPT 2 end_POSTSUBSCRIPT start_POSTSUPERSCRIPT 2 end_POSTSUPERSCRIPT + ‚Ä¶ + italic_u start_POSTSUBSCRIPT italic_d end_POSTSUBSCRIPT start_POSTSUPERSCRIPT 2 end_POSTSUPERSCRIPT = 1, one can simply write (with (u‚Å¢u‚ä∫)‚ä∫=u‚Å¢u‚ä∫superscript ùë¢ superscript ùë¢‚ä∫‚ä∫ùë¢ superscript ùë¢‚ä∫(uu^{\intercal})^{\intercal}=uu^{\intercal}( italic_u italic_u start_POSTSUPERSCRIPT ‚ä∫ end_POSTSUPERSCRIPT ) start_POSTSUPERSCRIPT ‚ä∫ end_POSTSUPERSCRIPT = italic_u italic_u start_POSTSUPERSCRIPT ‚ä∫ end_POSTSUPERSCRIPT)

‚Äñu‚Å¢u‚ä∫‚ÄñF=‚àëi=1 d u i 2=1.subscript norm ùë¢ superscript ùë¢‚ä∫ùêπ superscript subscript ùëñ 1 ùëë subscript superscript ùë¢ 2 ùëñ 1\textstyle\left\|uu^{\intercal}\right\|_{F}=\sqrt{\sum_{i=1}^{d}u^{2}_{i}}=1.‚à• italic_u italic_u start_POSTSUPERSCRIPT ‚ä∫ end_POSTSUPERSCRIPT ‚à• start_POSTSUBSCRIPT italic_F end_POSTSUBSCRIPT = square-root start_ARG ‚àë start_POSTSUBSCRIPT italic_i = 1 end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_d end_POSTSUPERSCRIPT italic_u start_POSTSUPERSCRIPT 2 end_POSTSUPERSCRIPT start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT end_ARG = 1 .

Since the finetuning process simply consists of finding the optimal directions of the reflection hyperplanes with bounded deviations from the transformation neutral element, it allows for (i) a very low number of extra parameters corresponding to the unit vectors u ùë¢ u italic_u, and (ii) the usage of high learning rates, as the risk of divergence is minimized. This allows for general learning rate robustness and encourages fast convergence by default, as consistently high learning rates can be selected; reducing computational resources required to achieve good performance (e.g. Fig.[6](https://arxiv.org/html/2405.20271v1#S4.F6 "Figure 6 ‚Ä£ 4 Intriguing Properties of ETHER ‚Ä£ ETHER: Efficient Finetuning of Large-Scale Models with Hyperplane Reflections")).

Interestingly, as this transformation is orthogonal (H‚Å¢H‚ä∫=I ùêª superscript ùêª‚ä∫ùêº HH^{\intercal}=I italic_H italic_H start_POSTSUPERSCRIPT ‚ä∫ end_POSTSUPERSCRIPT = italic_I), it falls under the umbrella of orthogonal transformations motivated in OFT(Qiu et al., [2023](https://arxiv.org/html/2405.20271v1#bib.bib50)) from the perspective of Hyperspherical Energy control to better preserve model pretraining. However, OFT leverages the Cayley parametrization of orthogonal matrices, which only produces determinant 1 matrices. By construction, this excludes Householder matrices from OFT, which have determinant ‚àí1 1-1- 1! However, as noted above, it is indeed in this particular setting and through the use of Householder transformations that high parameter efficiency, strong pretraining retention, and learning rate robustness arise.

On top of that, we further investigate the importance of Hyperspherical Energy retention by conducting a control study comparing OFT against its non-orthogonal variant (Naive)1 1 1 Naive employs an unconstrained block-diagonal transformation matrix N B superscript ùëÅ ùêµ N^{B}italic_N start_POSTSUPERSCRIPT italic_B end_POSTSUPERSCRIPT made up of n ùëõ n italic_n blocks and initialized as an identity matrix, i.e. having the same number of trainable parameters and initialization as OFT‚Äôs transformation matrix Q B superscript ùëÑ ùêµ Q^{B}italic_Q start_POSTSUPERSCRIPT italic_B end_POSTSUPERSCRIPT. Our experiments do not show significant differences in terms of control and training stability, suggesting that such properties stem from the multiplicative finetuning approach rather than the underlying HE retention, contrasting insights in Qiu et al. ([2023](https://arxiv.org/html/2405.20271v1#bib.bib50)) (c.f. Sec.[5.3](https://arxiv.org/html/2405.20271v1#S5.SS3 "5.3 Hyperspherical Energy for Effective PEFT ‚Ä£ 5 Benchmark Experiments ‚Ä£ ETHER: Efficient Finetuning of Large-Scale Models with Hyperplane Reflections")). These findings partly motivate the exploration of a relaxed variant of the Householder reflection in the next section[3.3](https://arxiv.org/html/2405.20271v1#S3.SS3 "3.3 Relaxing Orthogonality in ETHER ‚Ä£ 3 Method ‚Ä£ ETHER: Efficient Finetuning of Large-Scale Models with Hyperplane Reflections"), which demonstrates that loosening the orthogonality constraint not only maintains good performance but can even lead to enhanced results.

![Image 1: Refer to caption](https://arxiv.org/html/2405.20271v1/x1.png)

Figure 1: ETHER and ETHER+ sketches. We visualize either a single hyperplane reflection for ETHER or two interacting hyperplanes for ETHER+, parametrized unit normals u ùë¢ u italic_u (and v ùë£ v italic_v). Unlike ETHER, the final result of ETHER+ does not have to retain the original length L ùêø L italic_L, as the need for hard reflections is softened, and orthogonality is no longer guaranteed.

### 3.3 Relaxing Orthogonality in ETHER

While finetuning via hyperplane reflections has several promising qualities as highlighted above, there is no free lunch. In particular, situations may arise where the strength of the transformation and inherent deviation from the identity may be too large by default, such as for potentially more nuanced tasks like subject-driven generation (Ruiz et al., [2023](https://arxiv.org/html/2405.20271v1#bib.bib56)). To allow for more nuanced transformations while retaining beneficial properties of ETHER - parameter efficiency and learning rate robustness through bounded deviations from the transformation neutral element - we propose the ETHER+ relaxation

H+=I‚àíu‚Å¢u‚ä∫+v‚Å¢v‚ä∫superscript ùêª ùêº ùë¢ superscript ùë¢‚ä∫ùë£ superscript ùë£‚ä∫H^{+}=I-uu^{\intercal}+vv^{\intercal}italic_H start_POSTSUPERSCRIPT + end_POSTSUPERSCRIPT = italic_I - italic_u italic_u start_POSTSUPERSCRIPT ‚ä∫ end_POSTSUPERSCRIPT + italic_v italic_v start_POSTSUPERSCRIPT ‚ä∫ end_POSTSUPERSCRIPT

with unit vectors u,v‚àà‚Ñù d ùë¢ ùë£ superscript ‚Ñù ùëë u,v\in\mathbb{R}^{d}italic_u , italic_v ‚àà blackboard_R start_POSTSUPERSCRIPT italic_d end_POSTSUPERSCRIPT. This is a simple variation of the Householder transformation that now allows for interaction between two distinct hyperplanes (see Fig.[1](https://arxiv.org/html/2405.20271v1#S3.F1 "Figure 1 ‚Ä£ 3.2 ETHER: Finetuning with Hyperplane Reflections ‚Ä£ 3 Method ‚Ä£ ETHER: Efficient Finetuning of Large-Scale Models with Hyperplane Reflections")). This helps to control the transformation strength as u‚Å¢u‚ä∫ùë¢ superscript ùë¢‚ä∫uu^{\intercal}italic_u italic_u start_POSTSUPERSCRIPT ‚ä∫ end_POSTSUPERSCRIPT and v‚Å¢v‚ä∫ùë£ superscript ùë£‚ä∫vv^{\intercal}italic_v italic_v start_POSTSUPERSCRIPT ‚ä∫ end_POSTSUPERSCRIPT can weaken or even cancel each other out to return the identity transformation in the limit where u=v ùë¢ ùë£ u=v italic_u = italic_v. In addition, the transformation distance remains bounded, as the relaxed variant H+superscript ùêª H^{+}italic_H start_POSTSUPERSCRIPT + end_POSTSUPERSCRIPT always has ‚ÄñH+‚àíI‚ÄñF‚â§2 subscript norm superscript ùêª ùêº ùêπ 2\left\|H^{+}-I\right\|_{F}\leq 2‚à• italic_H start_POSTSUPERSCRIPT + end_POSTSUPERSCRIPT - italic_I ‚à• start_POSTSUBSCRIPT italic_F end_POSTSUBSCRIPT ‚â§ 2, i.e.

max‚Å¢‚ÄñH+‚àíI‚ÄñF‚â§max‚Å¢‚ÄñH‚àíI‚ÄñF.max subscript norm superscript ùêª ùêº ùêπ max subscript norm ùêª ùêº ùêπ\text{max}\left\|H^{+}-I\right\|_{F}\leq\text{max}\left\|H-I\right\|_{F}.max ‚à• italic_H start_POSTSUPERSCRIPT + end_POSTSUPERSCRIPT - italic_I ‚à• start_POSTSUBSCRIPT italic_F end_POSTSUBSCRIPT ‚â§ max ‚à• italic_H - italic_I ‚à• start_POSTSUBSCRIPT italic_F end_POSTSUBSCRIPT .

This follows immediately from the triangle inequality of norms, i.e. ‚Äñv‚Å¢v‚ä∫‚àíu‚Å¢u‚ä∫‚ÄñF‚â§‚Äñv‚Å¢v‚ä∫‚ÄñF+‚Äñu‚Å¢u‚ä∫‚ÄñF=2 subscript norm ùë£ superscript ùë£‚ä∫ùë¢ superscript ùë¢‚ä∫ùêπ subscript norm ùë£ superscript ùë£‚ä∫ùêπ subscript norm ùë¢ superscript ùë¢‚ä∫ùêπ 2\left\|vv^{\intercal}-uu^{\intercal}\right\|_{F}\leq\left\|vv^{\intercal}% \right\|_{F}+\left\|uu^{\intercal}\right\|_{F}=2‚à• italic_v italic_v start_POSTSUPERSCRIPT ‚ä∫ end_POSTSUPERSCRIPT - italic_u italic_u start_POSTSUPERSCRIPT ‚ä∫ end_POSTSUPERSCRIPT ‚à• start_POSTSUBSCRIPT italic_F end_POSTSUBSCRIPT ‚â§ ‚à• italic_v italic_v start_POSTSUPERSCRIPT ‚ä∫ end_POSTSUPERSCRIPT ‚à• start_POSTSUBSCRIPT italic_F end_POSTSUBSCRIPT + ‚à• italic_u italic_u start_POSTSUPERSCRIPT ‚ä∫ end_POSTSUPERSCRIPT ‚à• start_POSTSUBSCRIPT italic_F end_POSTSUBSCRIPT = 2. Due to the weaker strength of this new transformation, we apply it both on the left (H+superscript ùêª H^{+}italic_H start_POSTSUPERSCRIPT + end_POSTSUPERSCRIPT) and right (H~+superscript~ùêª\tilde{H}^{+}over~ start_ARG italic_H end_ARG start_POSTSUPERSCRIPT + end_POSTSUPERSCRIPT) of the weight matrix W ùëä W italic_W, such that the forward pass becomes

(H+‚Å¢W‚Å¢H~+)‚ä∫‚Å¢x+b.superscript superscript ùêª ùëä superscript~ùêª‚ä∫ùë• ùëè\left(H^{+}W\tilde{H}^{+}\right)^{\intercal}x+b.( italic_H start_POSTSUPERSCRIPT + end_POSTSUPERSCRIPT italic_W over~ start_ARG italic_H end_ARG start_POSTSUPERSCRIPT + end_POSTSUPERSCRIPT ) start_POSTSUPERSCRIPT ‚ä∫ end_POSTSUPERSCRIPT italic_x + italic_b .

Consequently, ETHER+ effectively leverages a sequence of hyperplane interactions that no longer have to retain length to allow for more nuanced weight adjustment while still minimizing the risk of diverging from the pretrained model (as also shown e.g. in Figs.[3](https://arxiv.org/html/2405.20271v1#S3.F3 "Figure 3 ‚Ä£ 3.4 Efficient ETHER through Block-Parallelism ‚Ä£ 3 Method ‚Ä£ ETHER: Efficient Finetuning of Large-Scale Models with Hyperplane Reflections"), [4](https://arxiv.org/html/2405.20271v1#S3.F4 "Figure 4 ‚Ä£ 3.4 Efficient ETHER through Block-Parallelism ‚Ä£ 3 Method ‚Ä£ ETHER: Efficient Finetuning of Large-Scale Models with Hyperplane Reflections"),[5](https://arxiv.org/html/2405.20271v1#S4.F5 "Figure 5 ‚Ä£ 4 Intriguing Properties of ETHER ‚Ä£ ETHER: Efficient Finetuning of Large-Scale Models with Hyperplane Reflections") and[6](https://arxiv.org/html/2405.20271v1#S4.F6 "Figure 6 ‚Ä£ 4 Intriguing Properties of ETHER ‚Ä£ ETHER: Efficient Finetuning of Large-Scale Models with Hyperplane Reflections")).

![Image 2: Refer to caption](https://arxiv.org/html/2405.20271v1/x2.png)

Figure 2: Block-Parallel Computation scheme between d ùëë d italic_d-dimensional block-diagonal transformation with n ùëõ n italic_n blocks and a d√óf ùëë ùëì d\times f italic_d √ó italic_f -dimensional weight matrix W ùëä W italic_W.

Table 1: Better computational efficiency through block-diagonality on Phi1.5-1.3B and Llama-2-7B, with internal dimensions of 2048 and 4096 respectively. As the number of blocks n ùëõ n italic_n increases, so does the computational efficiency, quantified by the decrease in TFLOPs required for a single backward pass (using a sample with longest sequence length). The larger the model‚Äôs internal dimension, the larger the efficiency gain.

Phi1.5-1.3B Llama-2-7B
TFLOPs rel. drop TFLOPs rel. drop
LoRA r=8 6.04-6.85-
OFT n=256 9.13-25.26-
ETHER n=1 9.13-25.26-
ETHER n=4 7.07-23%12.07-52%
ETHER n=32 6.71-27%8.22-68%
ETHER+n=1 10.78-51.65-
ETHER+n=4 7.69-29%18.66-64%
ETHER+n=32 6.79-37%9.04-83%

### 3.4 Efficient ETHER through Block-Parallelism

In multiplicative finetuning like OFT or ETHER, further computational load is introduced through additional matrix multiplications. To mitigate this issue, we introduce a block-diagonal formulation of ETHER similar to block-diagonal OFT described in ¬ß[3.1](https://arxiv.org/html/2405.20271v1#S3.SS1 "3.1 Preliminaries ‚Ä£ 3 Method ‚Ä£ ETHER: Efficient Finetuning of Large-Scale Models with Hyperplane Reflections"). For this, we break down the Householder transformation H ùêª H italic_H (eq.[1](https://arxiv.org/html/2405.20271v1#S3.E1 "Equation 1 ‚Ä£ 3.2 ETHER: Finetuning with Hyperplane Reflections ‚Ä£ 3 Method ‚Ä£ ETHER: Efficient Finetuning of Large-Scale Models with Hyperplane Reflections")) into its corresponding block-diagonal variant H B superscript ùêª ùêµ H^{B}italic_H start_POSTSUPERSCRIPT italic_B end_POSTSUPERSCRIPT:

diag‚Å¢(H 1‚Å¢‚ãØ‚Å¢H n)=I‚àí2‚Å¢(u^1‚Å¢u^1‚ä∫‚ã±u^n‚Å¢u^n‚ä∫)diag superscript ùêª 1‚ãØsuperscript ùêª ùëõ ùêº 2 matrix subscript^ùë¢ 1 superscript subscript^ùë¢ 1‚ä∫missing-subexpression missing-subexpression missing-subexpression‚ã±missing-subexpression missing-subexpression missing-subexpression subscript^ùë¢ ùëõ superscript subscript^ùë¢ ùëõ‚ä∫\text{diag}(H^{1}\cdots H^{n})=I-2\begin{pmatrix}\hat{u}_{1}\hat{u}_{1}^{% \intercal}&&\\ &\ddots&\\ &&\hat{u}_{n}\hat{u}_{n}^{\intercal}\\ \end{pmatrix}diag ( italic_H start_POSTSUPERSCRIPT 1 end_POSTSUPERSCRIPT ‚ãØ italic_H start_POSTSUPERSCRIPT italic_n end_POSTSUPERSCRIPT ) = italic_I - 2 ( start_ARG start_ROW start_CELL over^ start_ARG italic_u end_ARG start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT over^ start_ARG italic_u end_ARG start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT start_POSTSUPERSCRIPT ‚ä∫ end_POSTSUPERSCRIPT end_CELL start_CELL end_CELL start_CELL end_CELL end_ROW start_ROW start_CELL end_CELL start_CELL ‚ã± end_CELL start_CELL end_CELL end_ROW start_ROW start_CELL end_CELL start_CELL end_CELL start_CELL over^ start_ARG italic_u end_ARG start_POSTSUBSCRIPT italic_n end_POSTSUBSCRIPT over^ start_ARG italic_u end_ARG start_POSTSUBSCRIPT italic_n end_POSTSUBSCRIPT start_POSTSUPERSCRIPT ‚ä∫ end_POSTSUPERSCRIPT end_CELL end_ROW end_ARG )

with each i ùëñ i italic_i-th block-plane parameterized by u^i‚àà‚Ñù d n subscript^ùë¢ ùëñ superscript ‚Ñù ùëë ùëõ\hat{u}_{i}\in\mathbb{R}^{\frac{d}{n}}over^ start_ARG italic_u end_ARG start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT ‚àà blackboard_R start_POSTSUPERSCRIPT divide start_ARG italic_d end_ARG start_ARG italic_n end_ARG end_POSTSUPERSCRIPT. Of course, one can do the same for H+superscript ùêª H^{+}italic_H start_POSTSUPERSCRIPT + end_POSTSUPERSCRIPT. In both cases, such a block-diagonal formulation reduces the cost of computing H ùêª H italic_H. More importantly, each i ùëñ i italic_i-th block now only affects the corresponding i ùëñ i italic_i-th block-row in the weight matrix W ùëä W italic_W. This means we can split W ùëä W italic_W into n ùëõ n italic_n sub-blocks W i‚àà‚Ñù d n√óf superscript ùëä ùëñ superscript ‚Ñù ùëë ùëõ ùëì W^{i}\in\mathbb{R}^{\frac{d}{n}\times f}italic_W start_POSTSUPERSCRIPT italic_i end_POSTSUPERSCRIPT ‚àà blackboard_R start_POSTSUPERSCRIPT divide start_ARG italic_d end_ARG start_ARG italic_n end_ARG √ó italic_f end_POSTSUPERSCRIPT, each of which is uniquely altered by its corresponding H i superscript ùêª ùëñ H^{i}italic_H start_POSTSUPERSCRIPT italic_i end_POSTSUPERSCRIPT counterpart. As a result, the full weight transformation can now be separated into smaller block-specific operations, reducing the overall number of computations. Furthermore, these operations can now be fully block-parallelized, significantly increasing training speed! In terms of computations, for each full-matrix-multiplication between H ùêª H italic_H and W ùëä W italic_W of sizes d√ód ùëë ùëë d\times d italic_d √ó italic_d and d√óf ùëë ùëì d\times f italic_d √ó italic_f respectively, d‚Å¢(d‚Å¢f)ùëë ùëë ùëì d(df)italic_d ( italic_d italic_f ) multiplications and (d‚àí1)‚Å¢d‚Å¢f ùëë 1 ùëë ùëì(d-1)df( italic_d - 1 ) italic_d italic_f additions are necessary, accounting for ùí™‚Å¢(d 2‚Å¢f)ùí™ superscript ùëë 2 ùëì\mathcal{O}(d^{2}f)caligraphic_O ( italic_d start_POSTSUPERSCRIPT 2 end_POSTSUPERSCRIPT italic_f ) operations. With our block-parallel scheme, we reduce these to n ùëõ n italic_n block-specific d n‚Å¢(d n‚Å¢f)ùëë ùëõ ùëë ùëõ ùëì\frac{d}{n}(\frac{d}{n}f)divide start_ARG italic_d end_ARG start_ARG italic_n end_ARG ( divide start_ARG italic_d end_ARG start_ARG italic_n end_ARG italic_f ) multiplications and d‚àí1 n‚Å¢(d n‚Å¢f)ùëë 1 ùëõ ùëë ùëõ ùëì\frac{d-1}{n}(\frac{d}{n}f)divide start_ARG italic_d - 1 end_ARG start_ARG italic_n end_ARG ( divide start_ARG italic_d end_ARG start_ARG italic_n end_ARG italic_f ) additions, resulting in ùí™‚Å¢(d 2‚Å¢f n)ùí™ superscript ùëë 2 ùëì ùëõ\mathcal{O}(\frac{d^{2}f}{n})caligraphic_O ( divide start_ARG italic_d start_POSTSUPERSCRIPT 2 end_POSTSUPERSCRIPT italic_f end_ARG start_ARG italic_n end_ARG ) operations (see Tab. [1](https://arxiv.org/html/2405.20271v1#S3.T1 "Table 1 ‚Ä£ 3.3 Relaxing Orthogonality in ETHER ‚Ä£ 3 Method ‚Ä£ ETHER: Efficient Finetuning of Large-Scale Models with Hyperplane Reflections")).

Furthermore, with each block being built from a single vector of dimension d n ùëë ùëõ\frac{d}{n}divide start_ARG italic_d end_ARG start_ARG italic_n end_ARG, ETHER transformations‚Äô construction ensures that the total number of trainable parameters remains constant for any n ùëõ n italic_n number of blocks. This stands in contrast to block-diagonal OFT, where the use of higher block counts was introduced to minimize the number of parameters while introducing noticeable decreases in adaptation performance! Instead, for block-diagonal ETHER, we find performance to be consistent over increasing block counts (see App.[D](https://arxiv.org/html/2405.20271v1#A4 "Appendix D ETHER Ablations ‚Ä£ ETHER: Efficient Finetuning of Large-Scale Models with Hyperplane Reflections")), allowing for an improved computational footprint with negligible performance decrease.

![Image 3: Refer to caption](https://arxiv.org/html/2405.20271v1/x3.png)

Figure 3: Change in model behavior as a function of perturbation strength, i.e. distance between weight transformation and identity matrix. As ETHER and ETHER+ are upper-bounded in perturbation by construction, catastrophic deterioration of model performances is rarely encountered, and weight transformations remain controllable even for maximal deviations. For standard approaches, s.a. OFT, larger deviations from the identity matrix may occur during training and result in substantial divergence from the pretrained model. Notice also that by breaking orthogonality constraints in ETHER+, both smaller and stronger semantic variants can be learned.

![Image 4: Refer to caption](https://arxiv.org/html/2405.20271v1/x4.png)

Figure 4: Distances as a function of learning rates between transformation and identity matrix (Transformation Distance), and finetuned and pretrained weights (Weights Distance). Distances obtained for subject-driven generation finetuning at convergence (1200 iterations). Results show distances magnitudes higher and unbounded for non-ETHER methods in both cases as learning rates increase.

4 Intriguing Properties of ETHER
--------------------------------

This section investigates and highlights the bounded distance and non-deteriorating nature of ETHER/ETHER+ in more detail while providing insights into its favorable learning rate robustness and the reliable use of high learning rates for fast convergence. For completeness, we also report here comparisons with the unconstrained Naive method, to better show the impact of orthogonality as proposed by Qiu et al. ([2023](https://arxiv.org/html/2405.20271v1#bib.bib50)), and how our method provides much stronger robustness. Finally, we include a discussion on the parameter efficiency. For all experiments in this section, please see ¬ß[5.1](https://arxiv.org/html/2405.20271v1#S5.SS1 "5.1 ETHER for Image-generative Model Adaptation ‚Ä£ 5 Benchmark Experiments ‚Ä£ ETHER: Efficient Finetuning of Large-Scale Models with Hyperplane Reflections") for relevant implementation details.

Non-Deteriorating Nature. Because both ETHER and ETHER+ are upper-bounded in their possible perturbation over the pretrained weight matrices (as measured for example by the distance to the transformation neutral element, the identity matrix), finetuning with both methods will guarantee suitable results for most hyperparameter choices. This is easily visualized in Fig.[3](https://arxiv.org/html/2405.20271v1#S3.F3 "Figure 3 ‚Ä£ 3.4 Efficient ETHER through Block-Parallelism ‚Ä£ 3 Method ‚Ä£ ETHER: Efficient Finetuning of Large-Scale Models with Hyperplane Reflections") by looking at generation samples after perturbing Stable Diffusion with randomly sampled transformations for each approach - OFT, ETHER and ETHER+ - respectively. While ETHER uses a fixed-distance transformation (c.f. Eq.[2](https://arxiv.org/html/2405.20271v1#S3.E2 "Equation 2 ‚Ä£ 3.2 ETHER: Finetuning with Hyperplane Reflections ‚Ä£ 3 Method ‚Ä£ ETHER: Efficient Finetuning of Large-Scale Models with Hyperplane Reflections")) that introduces a noticeable change (but still retaining semantics), ETHER+ can obtain both finegrained visual control as well as stronger semantic changes. Conversely, unbounded methods like OFT catastrophically deteriorate a model‚Äôs generative abilities as the perturbation strength increases.

This results in a much more controlled generation setting for ETHER and ETHER+ finetuning. This is also depicted quantitatively in Fig.[4](https://arxiv.org/html/2405.20271v1#S3.F4 "Figure 4 ‚Ä£ 3.4 Efficient ETHER through Block-Parallelism ‚Ä£ 3 Method ‚Ä£ ETHER: Efficient Finetuning of Large-Scale Models with Hyperplane Reflections"), which shows distances between the learned transformation and the transformed weights (at convergence) to the identity matrix and the pretrained weights, respectively, as a function of the learning rate. As can be seen, larger learning rate values for OFT and Naive finetuning (OFT without orthogonality constraints) result in distances that are orders of magnitude higher than those of ETHER and ETHER+, leading to catastrophic deterioration and model collapse (see Fig.[8](https://arxiv.org/html/2405.20271v1#A1.F8 "Figure 8 ‚Ä£ Appendix A Qualitative Evidence of Learning Rate Robustness ‚Ä£ ETHER: Efficient Finetuning of Large-Scale Models with Hyperplane Reflections") in App.).

Learning Rate and Hyperparameter Robustness. Practically, the non-deteriorating nature of ETHER and ETHER+ manifests in learning rate robustness during finetuning. As the risks of divergence and collapse are minimized, training stability becomes much less dependent on the choice of learning rate. This is seen when evaluating performance (e.g. mIoU for controllable image synthesis in Fig.[5](https://arxiv.org/html/2405.20271v1#S4.F5 "Figure 5 ‚Ä£ 4 Intriguing Properties of ETHER ‚Ä£ ETHER: Efficient Finetuning of Large-Scale Models with Hyperplane Reflections")) and model convergence (Fig.[6](https://arxiv.org/html/2405.20271v1#S4.F6 "Figure 6 ‚Ä£ 4 Intriguing Properties of ETHER ‚Ä£ ETHER: Efficient Finetuning of Large-Scale Models with Hyperplane Reflections")) against learning rates. For non-ETHER methods, Fig.[5](https://arxiv.org/html/2405.20271v1#S4.F5 "Figure 5 ‚Ä£ 4 Intriguing Properties of ETHER ‚Ä£ ETHER: Efficient Finetuning of Large-Scale Models with Hyperplane Reflections") shows significant performance drops for high learning rates, while Fig.[6](https://arxiv.org/html/2405.20271v1#S4.F6 "Figure 6 ‚Ä£ 4 Intriguing Properties of ETHER ‚Ä£ ETHER: Efficient Finetuning of Large-Scale Models with Hyperplane Reflections") reveals fast convergence speeds for ETHER+ with learning rates covering multiple magnitudes, much more general than e.g. OFT.

This means that not only can good performance be guaranteed for most learning rate choices, but fast convergence as well, with competitive results already after the first epoch. Since ETHER also only introduces a single hyperparameter, the number of diagonal blocks, which marginally impacts performance (c.f. ¬ß[3.4](https://arxiv.org/html/2405.20271v1#S3.SS4 "3.4 Efficient ETHER through Block-Parallelism ‚Ä£ 3 Method ‚Ä£ ETHER: Efficient Finetuning of Large-Scale Models with Hyperplane Reflections")), ETHER methods become very attractive for practical usage, as the need for grid-search and cautious low learning rate training for good performance (c.f. ¬ß[1](https://arxiv.org/html/2405.20271v1#S1 "1 Introduction ‚Ä£ ETHER: Efficient Finetuning of Large-Scale Models with Hyperplane Reflections")) is reduced.

Parameter Efficiency. Finally, we provide a more detailed exploration on the parameter efficiency of ETHER-based methods. Let L ùêø L italic_L be the number of finetuned layers, d ùëë d italic_d and f ùëì f italic_f the respective weight dimensions for W‚àà‚Ñù d√óf ùëä superscript ‚Ñù ùëë ùëì W\in\mathbb{R}^{d\times f}italic_W ‚àà blackboard_R start_POSTSUPERSCRIPT italic_d √ó italic_f end_POSTSUPERSCRIPT. Then the parameter complexity for OFT can be written as ùí™‚Å¢(L‚Å¢d 2 n)ùí™ ùêø superscript ùëë 2 ùëõ\mathcal{O}(\frac{Ld^{2}}{n})caligraphic_O ( divide start_ARG italic_L italic_d start_POSTSUPERSCRIPT 2 end_POSTSUPERSCRIPT end_ARG start_ARG italic_n end_ARG )(Qiu et al., [2023](https://arxiv.org/html/2405.20271v1#bib.bib50)) with n ùëõ n italic_n number of diagonal blocks 2 2 2 Qiu et al. ([2023](https://arxiv.org/html/2405.20271v1#bib.bib50)) note a possible ùí™‚Å¢(L‚Å¢d)ùí™ ùêø ùëë\mathcal{O}(Ld)caligraphic_O ( italic_L italic_d ) if n=Œ±‚Å¢d ùëõ ùõº ùëë n=\alpha d italic_n = italic_Œ± italic_d. However, in practice, equally scaling n ùëõ n italic_n with d ùëë d italic_d disproportionally reduces adaptation parameters for large weight matrices. As OFT is fairly dependent on the parameter count, we omit this estimate.. Similarly, for LoRA we get ùí™‚Å¢(L‚Å¢r‚Å¢(d+f))ùí™ ùêø ùëü ùëë ùëì\mathcal{O}(Lr(d+f))caligraphic_O ( italic_L italic_r ( italic_d + italic_f ) ), while for ETHER and ETHER+ we only have ùí™‚Å¢(L‚Å¢d)ùí™ ùêø ùëë\mathcal{O}(Ld)caligraphic_O ( italic_L italic_d ) and ùí™‚Å¢(L‚Å¢(d+f))ùí™ ùêø ùëë ùëì\mathcal{O}(L(d+f))caligraphic_O ( italic_L ( italic_d + italic_f ) ) respectively. With respect to both LoRA and OFT, this omits at the very least the rank multiplier r ùëü r italic_r, or a potentially quadratic scaling. As already motivated in Sec.[3](https://arxiv.org/html/2405.20271v1#S3 "3 Method ‚Ä£ ETHER: Efficient Finetuning of Large-Scale Models with Hyperplane Reflections"), this results in incredibly efficient finetuning while achieving comparable or stronger performances. For example, when finetuning Stable Diffusion as done above, ETHER and ETHER+ use 120 times and 30 times fewer parameters than OFT respectively.

![Image 5: Refer to caption](https://arxiv.org/html/2405.20271v1/x5.png)

Figure 5: mIoU and FID performances as a function of learning rates. Results are obtained for controllable generation S2I finetuning on Stable Diffusion, and reveal a much stronger learning rate robustness of ETHER-based methods; retaining strong performance across entire learning rate magnitudes.

![Image 6: Refer to caption](https://arxiv.org/html/2405.20271v1/x6.png)

Figure 6: Achieved controllability (mIoU) per epoch for different finetuning methods. This figure extends Fig.[5](https://arxiv.org/html/2405.20271v1#S4.F5 "Figure 5 ‚Ä£ 4 Intriguing Properties of ETHER ‚Ä£ ETHER: Efficient Finetuning of Large-Scale Models with Hyperplane Reflections") and highlights in detail how only a learning rate of 10‚àí4 superscript 10 4 10^{-4}10 start_POSTSUPERSCRIPT - 4 end_POSTSUPERSCRIPT allows for optimal convergence in OFT and Naive, while for ETHER+ fastest convergence speeds are stably achieved across magnitudes.

5 Benchmark Experiments
-----------------------

We first investigate generative model adaptation in Sec.[5.1](https://arxiv.org/html/2405.20271v1#S5.SS1 "5.1 ETHER for Image-generative Model Adaptation ‚Ä£ 5 Benchmark Experiments ‚Ä£ ETHER: Efficient Finetuning of Large-Scale Models with Hyperplane Reflections"), with a focus on subject-driven image synthesis (¬ß[5.1.1](https://arxiv.org/html/2405.20271v1#S5.SS1.SSS1 "5.1.1 Subject-driven Generation ‚Ä£ 5.1 ETHER for Image-generative Model Adaptation ‚Ä£ 5 Benchmark Experiments ‚Ä£ ETHER: Efficient Finetuning of Large-Scale Models with Hyperplane Reflections")) and controllable image synthesis (¬ß[5.1.2](https://arxiv.org/html/2405.20271v1#S5.SS1.SSS2 "5.1.2 Controllable Image Generation ‚Ä£ 5.1 ETHER for Image-generative Model Adaptation ‚Ä£ 5 Benchmark Experiments ‚Ä£ ETHER: Efficient Finetuning of Large-Scale Models with Hyperplane Reflections")) following recent works (Qiu et al., [2023](https://arxiv.org/html/2405.20271v1#bib.bib50); Liu et al., [2023a](https://arxiv.org/html/2405.20271v1#bib.bib37)). Sec.[5.2](https://arxiv.org/html/2405.20271v1#S5.SS2 "5.2 ETHER for Language Models Adaptation ‚Ä£ 5 Benchmark Experiments ‚Ä£ ETHER: Efficient Finetuning of Large-Scale Models with Hyperplane Reflections") then correspondingly investigates language model adaptation, looking at both natural language understanding (¬ß[5.2.1](https://arxiv.org/html/2405.20271v1#S5.SS2.SSS1 "5.2.1 Natural Language Understanding ‚Ä£ 5.2 ETHER for Language Models Adaptation ‚Ä£ 5 Benchmark Experiments ‚Ä£ ETHER: Efficient Finetuning of Large-Scale Models with Hyperplane Reflections")) and instruction tuning (¬ß[5.2.2](https://arxiv.org/html/2405.20271v1#S5.SS2.SSS2 "5.2.2 Instruction Tuning ‚Ä£ 5.2 ETHER for Language Models Adaptation ‚Ä£ 5 Benchmark Experiments ‚Ä£ ETHER: Efficient Finetuning of Large-Scale Models with Hyperplane Reflections")). Finally, we study the importance of orthogonality and hyperspherical energy on finetuning performance in Sec.[5.3](https://arxiv.org/html/2405.20271v1#S5.SS3 "5.3 Hyperspherical Energy for Effective PEFT ‚Ä£ 5 Benchmark Experiments ‚Ä£ ETHER: Efficient Finetuning of Large-Scale Models with Hyperplane Reflections").

### 5.1 ETHER for Image-generative Model Adaptation

For our experiments on diffusion-based generative models, we apply the finetuning methods on the pretrained Stable Diffusion-v1.5(Rombach et al., [2022](https://arxiv.org/html/2405.20271v1#bib.bib53)), following the setting from OFT (Qiu et al., [2023](https://arxiv.org/html/2405.20271v1#bib.bib50)). Our experiments follow best practices and hyperparameter choices for each method. For implementation details, please refer to App.[C](https://arxiv.org/html/2405.20271v1#A3 "Appendix C Experimental Details ‚Ä£ ETHER: Efficient Finetuning of Large-Scale Models with Hyperplane Reflections").

#### 5.1.1 Subject-driven Generation

We first deploy ETHER and ETHER+ on subject-driven generation following Ruiz et al. ([2023](https://arxiv.org/html/2405.20271v1#bib.bib56)); Qiu et al. ([2023](https://arxiv.org/html/2405.20271v1#bib.bib50)); finetuning the generative model for each of the 30 subjects and 25 prompts. For each combination, we generate four images, and measure image quality via a DINO (Caron et al., [2021](https://arxiv.org/html/2405.20271v1#bib.bib3)) and a CLIP image encoder (Radford et al., [2021](https://arxiv.org/html/2405.20271v1#bib.bib51)), text-prompt fidelity via a CLIP text encoder, and image diversity using LPIPS (Zhang et al., [2018](https://arxiv.org/html/2405.20271v1#bib.bib72)).

Quantitative Results. Results are shown in Tab.[2](https://arxiv.org/html/2405.20271v1#S5.T2 "Table 2 ‚Ä£ 5.1.1 Subject-driven Generation ‚Ä£ 5.1 ETHER for Image-generative Model Adaptation ‚Ä£ 5 Benchmark Experiments ‚Ä£ ETHER: Efficient Finetuning of Large-Scale Models with Hyperplane Reflections"). On subject-driven generation, we find competitive performance for both image quality, text-prompt fidelity and image diversity, particularly for ETHER+ (e.g. DINO and CLIP-I scores of 0.666 0.666 0.666 0.666 vs 0.652 0.652 0.652 0.652 and 0.8 0.8 0.8 0.8 vs 0.794 0.794 0.794 0.794 for OFT, respectively). Most importantly, we achieve this performance while only utilizing a fraction of tuning parameters; with ETHER+ only introducing 0.4‚Å¢M 0.4 ùëÄ 0.4M 0.4 italic_M as compared to 11.6‚Å¢M 11.6 ùëÄ 11.6M 11.6 italic_M by OFT. As hypothesized in Sec.[3](https://arxiv.org/html/2405.20271v1#S3 "3 Method ‚Ä£ ETHER: Efficient Finetuning of Large-Scale Models with Hyperplane Reflections"), for nuanced finetuning, ETHER‚Äôs transformation strength seems to be too high to retain key semantic concepts in subject-driven generation, falling short in image quality with respect to other methods (e.g. also qualitatively depicted in Fig[3](https://arxiv.org/html/2405.20271v1#S3.F3 "Figure 3 ‚Ä£ 3.4 Efficient ETHER through Block-Parallelism ‚Ä£ 3 Method ‚Ä£ ETHER: Efficient Finetuning of Large-Scale Models with Hyperplane Reflections")), despite achieving strong image diversity and text-prompt fidelity.

Table 2: Subject-driven Generation Results. We use r ùëü r italic_r to denote rank, and n ùëõ n italic_n the number of diagonal blocks. We measure image quality (DINO, CLIP-I), text-prompt fidelity (CLIP-T) and image diversity (LPIPS). ETHER+ addresses finegrained adaptation shortcomings of ETHER (c.f. Sec.[3.3](https://arxiv.org/html/2405.20271v1#S3.SS3 "3.3 Relaxing Orthogonality in ETHER ‚Ä£ 3 Method ‚Ä£ ETHER: Efficient Finetuning of Large-Scale Models with Hyperplane Reflections")) and achieves strong performance with only few adaptation parameters.

#params DINO ‚Üë‚Üë\uparrow‚ÜëCLIP-I ‚Üë‚Üë\uparrow‚ÜëCLIP-T ‚Üë‚Üë\uparrow‚ÜëLPIPS ‚Üë‚Üë\uparrow‚Üë
Real Images-0.703 0.864-0.695
DreamBooth 859.5M 0.644 0.793 0.236 0.709
LoRA r=4 0.8M 0.660 0.796 0.231 0.714
OFT n=4 11.6M 0.652 0.794 0.241 0.725
ETHER 0.1M 0.567 0.746 0.256 0.766
ETHER+0.4M 0.666 0.800 0.240 0.729

Table 3: Semantic Map to Image Results. We use n ùëõ n italic_n to denote the number of diagonal blocks. ETHER and particularly ETHER+ achieve strong synthesis control (mIoU, Acc) with few parameters while retaining good image alignment (FID). We indicate with (+ magn. r.f.) the OFT version with magnitude re-fitting.

#params mIoU ‚Üë‚Üë\uparrow‚ÜëAcc ‚Üë‚Üë\uparrow‚ÜëFID ‚Üì‚Üì\downarrow‚Üì
Encoder-only 0 8.2 38.0 41.2
OFT n=4 13.2M 24.5 62.8 31.1
OFT n=4 (+ magn. r.f.)13.4M 24.6 63.3 30.8
ETHER 0.1M 24.6 63.3 32.0
ETHER+0.4M 27.3 68.1 31.0

#### 5.1.2 Controllable Image Generation

This section applies ETHER for controllability of Stable Diffusion following Qiu et al. ([2023](https://arxiv.org/html/2405.20271v1#bib.bib50)) for the Semantic Map to Image (S2I) task on ADE20K (Zhou et al., [2018](https://arxiv.org/html/2405.20271v1#bib.bib77)). We use the trainable encoder from ControlNet (Zhang et al., [2023b](https://arxiv.org/html/2405.20271v1#bib.bib70)) for the control signal and perform finetuning on the Stable Diffusion weights only. We report a baseline with just the control signal encoder to highlight relative gains through finetuning. Evaluations are performed on 2000 images generated from the validation set using mean Intersection-over-Union (mIoU) and accuracy of semantic maps over generated images using UperNet-101(Xiao et al., [2018](https://arxiv.org/html/2405.20271v1#bib.bib66)) pretrained on ADE20K. Finally, we measure the similarity between generated and original images via FID(Heusel et al., [2018](https://arxiv.org/html/2405.20271v1#bib.bib17)). For OFT, we also test magnitude re-fitting (Qiu et al., [2023](https://arxiv.org/html/2405.20271v1#bib.bib50)) for an additional epoch. We note that LoRA did not provide good results even for larger hyperparameter grids and was therefore omitted from Tab.[3](https://arxiv.org/html/2405.20271v1#S5.T3 "Table 3 ‚Ä£ 5.1.1 Subject-driven Generation ‚Ä£ 5.1 ETHER for Image-generative Model Adaptation ‚Ä£ 5 Benchmark Experiments ‚Ä£ ETHER: Efficient Finetuning of Large-Scale Models with Hyperplane Reflections") (more details in App. [F](https://arxiv.org/html/2405.20271v1#A6 "Appendix F LoRA failures for S2I task ‚Ä£ ETHER: Efficient Finetuning of Large-Scale Models with Hyperplane Reflections")).

Quantitative Results. Results are depicted in Tab.[3](https://arxiv.org/html/2405.20271v1#S5.T3 "Table 3 ‚Ä£ 5.1.1 Subject-driven Generation ‚Ä£ 5.1 ETHER for Image-generative Model Adaptation ‚Ä£ 5 Benchmark Experiments ‚Ä£ ETHER: Efficient Finetuning of Large-Scale Models with Hyperplane Reflections"), and clearly demonstrate competitive control with both ETHER and ETHER+. Unlike subject-driven image generation, we find that ETHER performs on the same level as OFT multiplicative finetuning while using over 100√ó100\times 100 √ó fewer parameters (e.g. 24.6 24.6 24.6 24.6 versus 24.5 24.5 24.5 24.5 mIoU of OFT with 0.1‚Å¢M 0.1 ùëÄ 0.1M 0.1 italic_M versus 13.2‚Å¢M 13.2 ùëÄ 13.2M 13.2 italic_M parameters). Introducing magnitude re-fitting to OFT yields only limited gains while adding 0.2‚Å¢M 0.2 ùëÄ 0.2M 0.2 italic_M parameters. Similar to Tab.[2](https://arxiv.org/html/2405.20271v1#S5.T2 "Table 2 ‚Ä£ 5.1.1 Subject-driven Generation ‚Ä£ 5.1 ETHER for Image-generative Model Adaptation ‚Ä£ 5 Benchmark Experiments ‚Ä£ ETHER: Efficient Finetuning of Large-Scale Models with Hyperplane Reflections") for subject-driven image generation, we find that for controllable image synthesis, the ETHER+ relaxation provides additional performance gains (e.g. 27.3 27.3 27.3 27.3 vs 24.5 24.5 24.5 24.5 mIoU and 68.1 68.1 68.1 68.1 vs 62.8 62.8 62.8 62.8 Acc against OFT). Taking into account the more robust (Fig.[5](https://arxiv.org/html/2405.20271v1#S4.F5 "Figure 5 ‚Ä£ 4 Intriguing Properties of ETHER ‚Ä£ ETHER: Efficient Finetuning of Large-Scale Models with Hyperplane Reflections")) and faster convergence (Fig.[6](https://arxiv.org/html/2405.20271v1#S4.F6 "Figure 6 ‚Ä£ 4 Intriguing Properties of ETHER ‚Ä£ ETHER: Efficient Finetuning of Large-Scale Models with Hyperplane Reflections")), this presents ETHER+ as a practically attractive finetuning alternative.

### 5.2 ETHER for Language Models Adaptation

To understand the applicability of the ETHER transformation family in the language domain, we follow Liu et al. ([2023a](https://arxiv.org/html/2405.20271v1#bib.bib37))‚Äôs and Hu et al. ([2022](https://arxiv.org/html/2405.20271v1#bib.bib20))‚Äôs experimental setup. For fair comparisons, we run grid searches over the most relevant hyperparameters in common value ranges. For additional implementation details, please refer to App.[C](https://arxiv.org/html/2405.20271v1#A3 "Appendix C Experimental Details ‚Ä£ ETHER: Efficient Finetuning of Large-Scale Models with Hyperplane Reflections").

#### 5.2.1 Natural Language Understanding

We begin by deploying ETHER and ETHER+ on the widely utilized (Devlin et al., [2019](https://arxiv.org/html/2405.20271v1#bib.bib9); Liu et al., [2019](https://arxiv.org/html/2405.20271v1#bib.bib38); He et al., [2023](https://arxiv.org/html/2405.20271v1#bib.bib15); Kopiczko et al., [2023](https://arxiv.org/html/2405.20271v1#bib.bib28)) GLUE benchmark (Wang et al., [2018](https://arxiv.org/html/2405.20271v1#bib.bib64)), finetuning a pretrained DeBERTaV3-base model (He et al., [2023](https://arxiv.org/html/2405.20271v1#bib.bib15)) following Liu et al. ([2023a](https://arxiv.org/html/2405.20271v1#bib.bib37)), from which we report the baselines‚Äô results. GLUE comprises various English sentence understanding tasks, such as inference tasks (MNLI, QNLI, RTE), classification of sentiment (SST-2) or correct English grammatical structures (CoLA), and semantic similarity and equivalence prediction (MRPC, QQP, STS-B). CoLA scores report the Matthews correlation coefficient, MNLI matched accuracy, and STS-B average correlation. All other tasks are evaluated on accuracy.

Quantitative Results. Results in Tab.[4](https://arxiv.org/html/2405.20271v1#S5.T4 "Table 4 ‚Ä£ 5.2.1 Natural Language Understanding ‚Ä£ 5.2 ETHER for Language Models Adaptation ‚Ä£ 5 Benchmark Experiments ‚Ä£ ETHER: Efficient Finetuning of Large-Scale Models with Hyperplane Reflections") show that ETHER and ETHER+ match and even outperform previous methods with significantly fewer parameters. For example, ETHER outperforms the second-best BOFT on the RTE inference task (89.53 89.53 89.53 89.53 vs 88.81 88.81 88.81 88.81) or equivalence prediction on MRPC (93.68 93.68 93.68 93.68 vs 92.40 92.40 92.40 92.40) while using just one-ninth of the parameters (0.085‚Å¢M 0.085 ùëÄ 0.085M 0.085 italic_M compared to 0.75‚Å¢M 0.75 ùëÄ 0.75M 0.75 italic_M). ETHER+ sets both the best performance on STS-B and particularly the highest overall score (90.10 90.10 90.10 90.10) using less than half of the parameters of BOFT. These results provide additional support for the practical viability of ETHER transformations, now for natural language adaptation - being a strong, but much more parameter-efficient competitor.

Table 4: GLUE benchmark. Comparisons of different methods finetuning DeBERTaV3-base. Results of all baselines are taken from (Liu et al., [2023a](https://arxiv.org/html/2405.20271v1#bib.bib37)). We use r ùëü r italic_r to denote rank, and n ùëõ n italic_n the number of diagonal blocks. As can be seen, ETHER and ETHER+ achieve competitive performances across metrics while utilizing fewer parameters (up to a magnitude in the case of ETHER) while also retaining all practical benefits such as learning rate robustness depicted e.g. in Sec.[4](https://arxiv.org/html/2405.20271v1#S4 "4 Intriguing Properties of ETHER ‚Ä£ ETHER: Efficient Finetuning of Large-Scale Models with Hyperplane Reflections").

#params MNLI‚Üë‚Üë\uparrow‚ÜëSST-2‚Üë‚Üë\uparrow‚ÜëCoLA‚Üë‚Üë\uparrow‚ÜëQQP‚Üë‚Üë\uparrow‚ÜëQNLI‚Üë‚Üë\uparrow‚ÜëRTE‚Üë‚Üë\uparrow‚ÜëMRPC‚Üë‚Üë\uparrow‚ÜëSTS-B‚Üë‚Üë\uparrow‚ÜëAvg‚Üë‚Üë\uparrow‚Üë
Full Finet.184M 89.90 95.63 69.19 92.40 94.03 83.75 89.46 91.60 88.25
BitFit 0.10M 89.37 94.84 66.96 88.41 92.24 78.70 87.75 91.35 86.20
H-Adapter 1.22M 90.13 95.53 68.64 91.91 94.11 84.48 89.95 91.48 88.28
P-Adapter 1.18M 90.33 95.61 68.77 92.04 94.29 85.20 89.46 91.54 88.41
LoRA r=8 1.33M 90.65 94.95 69.82 91.99 93.87 85.20 89.95 91.60 88.50
AdaLoRA 1.27M 90.76 96.10 71.45 92.23 94.55 88.09 90.69 91.84 89.46
OFT n=16 0.79M 90.33 96.33 73.91 92.10 94.07 87.36 92.16 91.91 89.77
BOFT m=2 n=8 superscript subscript absent ùëõ 8 ùëö 2{}_{n=8}^{m=2}start_FLOATSUBSCRIPT italic_n = 8 end_FLOATSUBSCRIPT start_POSTSUPERSCRIPT italic_m = 2 end_POSTSUPERSCRIPT 0.75M 90.25 96.44 72.95 92.10 94.23 88.81 92.40 91.92 89.89
ETHER 0.09M 90.23 96.10 71.31 91.42 94.31 89.53 93.68 92.30 89.86
ETHER+0.33M 90.52 96.33 72.64 92.22 94.33 89.53 92.89 92.35 90.10

#### 5.2.2 Instruction Tuning

Our instruction tuning experiments make use of Llama-2-7B (Touvron et al., [2023b](https://arxiv.org/html/2405.20271v1#bib.bib62)) as pretrained model, finetuning it on the Alpaca dataset (Taori et al., [2023](https://arxiv.org/html/2405.20271v1#bib.bib60)) for one epoch. To operate on a consumer GPU, we truncate the maximum sequence length to 256 and use bfloat16 precision (Kalamkar et al., [2019](https://arxiv.org/html/2405.20271v1#bib.bib22)). We evaluate 0-shot performance of our instruction-tuned model on (i) Massive Multitask Language Understanding (MMLU) (Hendrycks et al., [2021](https://arxiv.org/html/2405.20271v1#bib.bib16)) with 57 different tasks in four different subjects (STEM, Humanities, Social Sciences, Others); (ii) the AI2 Reasoning Challenge (ARC) (Clark et al., [2018](https://arxiv.org/html/2405.20271v1#bib.bib7)), a common-sense reasoning dataset of questions from science grade exams; (iii) TruthfulQA (Lin et al., [2022](https://arxiv.org/html/2405.20271v1#bib.bib35)) comprising 817 questions spanning 38 categories testing how much the model (wrongly) relies on imitation of human text to answer.

Quantitative Results. Results in Tab.[5](https://arxiv.org/html/2405.20271v1#S5.T5 "Table 5 ‚Ä£ 5.2.2 Instruction Tuning ‚Ä£ 5.2 ETHER for Language Models Adaptation ‚Ä£ 5 Benchmark Experiments ‚Ä£ ETHER: Efficient Finetuning of Large-Scale Models with Hyperplane Reflections") show that both ETHER and ETHER+ outperform comparable finetuning approaches while utilizing fewer parameters. Across all metrics, the Llama-2-7B baseline is consistently surpassed by significant margins (e.g. 44.87 44.87 44.87 44.87 MMLU for ETHER+ vs the 41.81 41.81 41.81 41.81 baseline, or 46.50 46.50 46.50 46.50 vs 42.92 42.92 42.92 42.92 ARC score). Despite being the most parameter-efficient method, ETHER outperforms all baselines with comparable number of parameters, such as the recently introduced VeRA (Kopiczko et al., [2023](https://arxiv.org/html/2405.20271v1#bib.bib28)) with rank r=64 ùëü 64 r=64 italic_r = 64, and LoRA rank 1 1 1 1. Surprisingly, increasing the rank of VeRA to 256 256 256 256 leads to a decrease in performance, while LoRA rank 8 8 8 8 shows better results but is still outperformed on MMLU despite having 16√ó16\times 16 √ó more parameters. On the other hand, ETHER+ surpasses all other methods across all benchmarks, while having 4√ó4\times 4 √ó fewer parameters than LoRA rank 8 8 8 8.

Table 5: Instruction Tuning. We use r ùëü r italic_r to denote rank, and n ùëõ n italic_n the number of diagonal blocks. Both ETHER and ETHER+ outperform LoRA/OFT which use up to a magnitude more parameters, and beat VeRA with similar parameter counts.

#params MMLU‚Üë‚Üë\uparrow‚ÜëARC‚Üë‚Üë\uparrow‚ÜëTru-1‚Üë‚Üë\uparrow‚ÜëTru-2‚Üë‚Üë\uparrow‚Üë
Llama-2-7B-41.81 42.92 25.21 38.95
VeRA r=64 0.27M 42.30 45.13 27.41 41.04
VeRA r=256 1.05M 42.21 43.85 25.33 39.02
LoRA r=1 0.52M 42.40 44.62 27.05 41.94
LoRA r=8 4.19M 43.61 46.16 28.76 42.21
OFT n=256 2.09M 42.92 44.88 27.42 41.11
ETHER n=32 0.26M 44.57 45.14 27.91 41.83
ETHER+n=32 1.04M 44.87 46.50 29.38 43.51

### 5.3 Hyperspherical Energy for Effective PEFT

Qiu et al. ([2023](https://arxiv.org/html/2405.20271v1#bib.bib50)) link finetuning stability and performance obtained by transforming the weights via matrix-multiplication to the orthogonality of the transformations, and a consequently unaltered hyperspherical energy (HE). To test this assumption, we have included an OFT control baseline (Naive), which does not utilize orthogonality constraints, on the same finetuning settings in which OFT was proposed. Results at convergence, as reported in Tab.[6](https://arxiv.org/html/2405.20271v1#S5.T6 "Table 6 ‚Ä£ 5.3 Hyperspherical Energy for Effective PEFT ‚Ä£ 5 Benchmark Experiments ‚Ä£ ETHER: Efficient Finetuning of Large-Scale Models with Hyperplane Reflections"), do not show significant differences, while actually introducing the overhead of computing the Cayley parametrizations (which also involve computing the inverse of a matrix). We also included the Naive baseline in the learning rate robustness studies in Fig. [4](https://arxiv.org/html/2405.20271v1#S3.F4 "Figure 4 ‚Ä£ 3.4 Efficient ETHER through Block-Parallelism ‚Ä£ 3 Method ‚Ä£ ETHER: Efficient Finetuning of Large-Scale Models with Hyperplane Reflections") and Fig. [5](https://arxiv.org/html/2405.20271v1#S4.F5 "Figure 5 ‚Ä£ 4 Intriguing Properties of ETHER ‚Ä£ ETHER: Efficient Finetuning of Large-Scale Models with Hyperplane Reflections"), showcasing that while differences are present for high learning rates, the optimal working range remains unaltered. Finally, we validate that the HE indeed varies during training, as reported in Fig. [7](https://arxiv.org/html/2405.20271v1#S5.F7 "Figure 7 ‚Ä£ 5.3 Hyperspherical Energy for Effective PEFT ‚Ä£ 5 Benchmark Experiments ‚Ä£ ETHER: Efficient Finetuning of Large-Scale Models with Hyperplane Reflections").

In contrast, on these same evaluations, our newly proposed ETHER transformation family, by introducing a boundary on the Euclidean distance on the transformation side, achieves stronger performance and greater robustness. This is especially true for the non-orthogonal ETHER+, which alters the overall HE even more than Naive (Fig. [7](https://arxiv.org/html/2405.20271v1#S5.F7 "Figure 7 ‚Ä£ 5.3 Hyperspherical Energy for Effective PEFT ‚Ä£ 5 Benchmark Experiments ‚Ä£ ETHER: Efficient Finetuning of Large-Scale Models with Hyperplane Reflections")). This evidence diminishes the role of the HE and instead emphasizes the greater importance of the Euclidean distance, establishing the ETHER family as a favorable option in multiplicative finetuning settings.

![Image 7: Refer to caption](https://arxiv.org/html/2405.20271v1/x7.png)

Figure 7: Difference in HE between finetuned/pretrained models for Subject-driven Generation and S2I. Notice that by removing the orthogonality constraint, both ETHER+ and Naive alter the HE of the pretrained model, while OFT and ETHER do not.

Table 6: OFT vs Naive. OFT performance-test against its non-orthogonal counterpart Naive. We show that results don‚Äôt differ significantly, questioning the relevance of HE retaining for finetuning performance. 

Subject-driven Generation S2I
DINO CLIP-I CLIP-T LPIPS mIoU Acc FID
OFT n=4 0.652 0.794 0.241 0.725 24.5 62.8 31.1
Naive n=4 0.648 0.793 0.245 0.730 24.3 62.9 29.9

6 Conclusions
-------------

Our paper introduces the ETHER family of transformations for parameter-efficient finetuning. Based on the Householder formulation of hyperplane reflections, ETHER methods frame finetuning as a search for unit normal vectors that define hyperplanes along which weight vectors are reflected. In doing so, ETHER (and its relaxation ETHER+ for more finegrained adaptation) fix (or upper bound) the distance of learned transformations from the identity matrix (the transformation neutral element), thereby minimizing the risk of finetuning divergence. Put together, ETHER methods operate more parameter-efficiently than other PEFT methods (e.g., around 10-100 times less than LoRA or OFT), have higher learning rate robustness and encourage fast convergence. Consequently, ETHER transformations require less expansive hyperparameter searches to achieve good performance, making them very attractive for practical deployment.

Limitations. Of course, there is no free lunch. While both ETHER and its relaxation ETHER+ show strong results with few parameters across a broad range of tasks, increasing the expressive power of the transformation is not as straightforward as in other methods, such as LoRA, where one can adjust the rank parameter to more closely approximate full finetuning. Moreover, multiplicative methods introduce a computational overhead during training compared to additive methods. Thanks to our block-parallel scheme, we make significant progress towards closing the gap between multiplicative and additive approaches; however, multiplicative methods still lag behind. This introduces a trade-off between parameter efficiency and computational overhead when achieving similar performance levels.

Impact Statement
----------------

This paper presents work that looks into better and more efficient finetuning of foundation models. By bringing down the need for compute-expensive hyperparameter grid searches and encouraging fast convergence, both the cost and environmental footprint of serving individually adapted models at scale can be brought down. Of course, with most advancement in the field of Machine Learning, there is potential for misuse and societal consequences, however, none of which we feel are specific to our proposed method and which need to be highlighted explicitly.

Acknowledgements
----------------

Massimo Bini was supported by Bosch Industry on Campus Lab at University of T√ºbingen. Karsten Roth thanks the European Laboratory for Learning and Intelligent Systems (ELLIS) PhD program and the International Max Planck Research School for Intelligent Systems (IMPRS-IS) for support. Zeynep Akata and Karsten Roth were supported by DFG project number 276693517, by BMBF FKZ: 01IS18039A, by the ERC (853489 - DEXIM), by EXC number 2064/1 ‚Äì project number 390727645.

References
----------

*   AI (2023) AI, L. Litgpt. [https://github.com/Lightning-AI/litgpt](https://github.com/Lightning-AI/litgpt), 2023. 
*   Bommasani et al. (2021) Bommasani, R., Hudson, D.A., Adeli, E., Altman, R., Arora, S., von Arx, S., Bernstein, M.S., Bohg, J., Bosselut, A., Brunskill, E., Brynjolfsson, E., Buch, S., Card, D., Castellon, R., Chatterji, N.S., Chen, A.S., Creel, K.A., Davis, J., Demszky, D., Donahue, C., Doumbouya, M., Durmus, E., Ermon, S., Etchemendy, J., Ethayarajh, K., Fei-Fei, L., Finn, C., Gale, T., Gillespie, L.E., Goel, K., Goodman, N.D., Grossman, S., Guha, N., Hashimoto, T., Henderson, P., Hewitt, J., Ho, D.E., Hong, J., Hsu, K., Huang, J., Icard, T.F., Jain, S., Jurafsky, D., Kalluri, P., Karamcheti, S., Keeling, G., Khani, F., Khattab, O., Koh, P.W., Krass, M.S., Krishna, R., Kuditipudi, R., Kumar, A., Ladhak, F., Lee, M., Lee, T., Leskovec, J., Levent, I., Li, X.L., Li, X., Ma, T., Malik, A., Manning, C.D., Mirchandani, S.P., Mitchell, E., Munyikwa, Z., Nair, S., Narayan, A., Narayanan, D., Newman, B., Nie, A., Niebles, J.C., Nilforoshan, H., Nyarko, J.F., Ogut, G., Orr, L., Papadimitriou, I., Park, J.S., Piech, C., Portelance, E., Potts, C., Raghunathan, A., Reich, R., Ren, H., Rong, F., Roohani, Y.H., Ruiz, C., Ryan, J., R‚Äôe, C., Sadigh, D., Sagawa, S., Santhanam, K., Shih, A., Srinivasan, K.P., Tamkin, A., Taori, R., Thomas, A.W., Tram√®r, F., Wang, R.E., Wang, W., Wu, B., Wu, J., Wu, Y., Xie, S.M., Yasunaga, M., You, J., Zaharia, M.A., Zhang, M., Zhang, T., Zhang, X., Zhang, Y., Zheng, L., Zhou, K., and Liang, P. On the opportunities and risks of foundation models. _ArXiv_, 2021. URL [https://crfm.stanford.edu/assets/report.pdf](https://crfm.stanford.edu/assets/report.pdf). 
*   Caron et al. (2021) Caron, M., Touvron, H., Misra, I., J√©gou, H., Mairal, J., Bojanowski, P., and Joulin, A. Emerging Properties in Self-Supervised Vision Transformers, May 2021. URL [http://arxiv.org/abs/2104.14294](http://arxiv.org/abs/2104.14294). arXiv:2104.14294 [cs]. 
*   Chen et al. (2023a) Chen, J., Zhang, A., Shi, X., Li, M., Smola, A., and Yang, D. Parameter-Efficient Fine-Tuning Design Spaces, January 2023a. URL [https://arxiv.org/abs/2301.01821v1](https://arxiv.org/abs/2301.01821v1). 
*   Chen et al. (2023b) Chen, Y., Qian, S., Tang, H., Lai, X., Liu, Z., Han, S., and Jia, J. LongLoRA: Efficient Fine-tuning of Long-Context Large Language Models, September 2023b. URL [http://arxiv.org/abs/2309.12307](http://arxiv.org/abs/2309.12307). arXiv:2309.12307 [cs]. 
*   Chiang et al. (2023) Chiang, W.-L., Li, Z., Lin, Z., Sheng, Y., Wu, Z., Zhang, H., Zheng, L., Zhuang, S., Zhuang, Y., Gonzalez, J.E., Stoica, I., and Xing, E.P. Vicuna: An open-source chatbot impressing gpt-4 with 90%* chatgpt quality, March 2023. URL [https://lmsys.org/blog/2023-03-30-vicuna/](https://lmsys.org/blog/2023-03-30-vicuna/). 
*   Clark et al. (2018) Clark, P., Cowhey, I., Etzioni, O., Khot, T., Sabharwal, A., Schoenick, C., and Tafjord, O. Think you have Solved Question Answering? Try ARC, the AI2 Reasoning Challenge, March 2018. URL [http://arxiv.org/abs/1803.05457](http://arxiv.org/abs/1803.05457). arXiv:1803.05457 [cs]. 
*   Dettmers et al. (2023) Dettmers, T., Pagnoni, A., Holtzman, A., and Zettlemoyer, L. QLoRA: Efficient Finetuning of Quantized LLMs, May 2023. URL [http://arxiv.org/abs/2305.14314](http://arxiv.org/abs/2305.14314). arXiv:2305.14314 [cs]. 
*   Devlin et al. (2019) Devlin, J., Chang, M.-W., Lee, K., and Toutanova, K. BERT: Pre-training of deep bidirectional transformers for language understanding. In Burstein, J., Doran, C., and Solorio, T. (eds.), _Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long and Short Papers)_, pp. 4171‚Äì4186, Minneapolis, Minnesota, June 2019. Association for Computational Linguistics. doi: 10.18653/v1/N19-1423. URL [https://aclanthology.org/N19-1423](https://aclanthology.org/N19-1423). 
*   Gal et al. (2022) Gal, R., Alaluf, Y., Atzmon, Y., Patashnik, O., Bermano, A.H., Chechik, G., and Cohen-Or, D. An image is worth one word: Personalizing text-to-image generation using textual inversion, 2022. 
*   Gao et al. (2023) Gao, L., Tow, J., Abbasi, B., Biderman, S., Black, S., DiPofi, A., Foster, C., Golding, L., Hsu, J., Le Noac‚Äôh, A., Li, H., McDonell, K., Muennighoff, N., Ociepa, C., Phang, J., Reynolds, L., Schoelkopf, H., Skowron, A., Sutawika, L., Tang, E., Thite, A., Wang, B., Wang, K., and Zou, A. A framework for few-shot language model evaluation, 12 2023. URL [https://zenodo.org/records/10256836](https://zenodo.org/records/10256836). 
*   Garg et al. (2024) Garg, S., Farajtabar, M., Pouransari, H., Vemulapalli, R., Mehta, S., Tuzel, O., Shankar, V., and Faghri, F. Tic-CLIP: Continual training of CLIP models. In _The Twelfth International Conference on Learning Representations_, 2024. URL [https://openreview.net/forum?id=TLADT8Wrhn](https://openreview.net/forum?id=TLADT8Wrhn). 
*   Gouk et al. (2021) Gouk, H., Hospedales, T.M., and Pontil, M. Distance-Based Regularisation of Deep Networks for Fine-Tuning, January 2021. URL [http://arxiv.org/abs/2002.08253](http://arxiv.org/abs/2002.08253). arXiv:2002.08253 [cs, stat]. 
*   Guo et al. (2021) Guo, D., Rush, A.M., and Kim, Y. Parameter-efficient transfer learning with diff pruning, 2021. 
*   He et al. (2023) He, P., Gao, J., and Chen, W. DeBERTav3: Improving deBERTa using ELECTRA-style pre-training with gradient-disentangled embedding sharing. In _The Eleventh International Conference on Learning Representations_, 2023. URL [https://openreview.net/forum?id=sE7-XhLxHA](https://openreview.net/forum?id=sE7-XhLxHA). 
*   Hendrycks et al. (2021) Hendrycks, D., Burns, C., Basart, S., Zou, A., Mazeika, M., Song, D., and Steinhardt, J. Measuring Massive Multitask Language Understanding, January 2021. URL [http://arxiv.org/abs/2009.03300](http://arxiv.org/abs/2009.03300). arXiv:2009.03300 [cs]. 
*   Heusel et al. (2018) Heusel, M., Ramsauer, H., Unterthiner, T., Nessler, B., and Hochreiter, S. Gans trained by a two time-scale update rule converge to a local nash equilibrium, 2018. 
*   Houlsby et al. (2019) Houlsby, N., Giurgiu, A., Jastrzebski, S., Morrone, B., de Laroussilhe, Q., Gesmundo, A., Attariyan, M., and Gelly, S. Parameter-Efficient Transfer Learning for NLP, February 2019. URL [https://arxiv.org/abs/1902.00751v2](https://arxiv.org/abs/1902.00751v2). 
*   Householder (1958) Householder, A.S. Unitary triangularization of a nonsymmetric matrix. _J. ACM_, 5(4):339‚Äì342, oct 1958. ISSN 0004-5411. doi: 10.1145/320941.320947. URL [https://doi.org/10.1145/320941.320947](https://doi.org/10.1145/320941.320947). 
*   Hu et al. (2022) Hu, E.J., Shen, Y., Wallis, P., Allen-Zhu, Z., Li, Y., Wang, S., Wang, L., and Chen, W. LoRA: Low-rank adaptation of large language models. In _ICLR_, 2022. URL [https://openreview.net/forum?id=nZeVKeeFYf9](https://openreview.net/forum?id=nZeVKeeFYf9). 
*   Ibrahim et al. (2024) Ibrahim, A., Th√©rien, B., Gupta, K., Richter, M.L., Anthony, Q., Lesort, T., Belilovsky, E., and Rish, I. Simple and scalable strategies to continually pre-train large language models, 2024. 
*   Kalamkar et al. (2019) Kalamkar, D., Mudigere, D., Mellempudi, N., Das, D., Banerjee, K., Avancha, S., Vooturi, D.T., Jammalamadaka, N., Huang, J., Yuen, H., Yang, J., Park, J., Heinecke, A., Georganas, E., Srinivasan, S., Kundu, A., Smelyanskiy, M., Kaul, B., and Dubey, P. A study of bfloat16 for deep learning training, 2019. 
*   Karras et al. (2018) Karras, T., Aila, T., Laine, S., and Lehtinen, J. Progressive growing of gans for improved quality, stability, and variation, 2018. 
*   Karthik et al. (2023) Karthik, S., Roth, K., Mancini, M., and Akata, Z. If at first you don‚Äôt succeed, try, try again: Faithful diffusion-based text-to-image generation by selection, 2023. 
*   Ke et al. (2023) Ke, Z., Shao, Y., Lin, H., Konishi, T., Kim, G., and Liu, B. Continual pre-training of language models. In _The Eleventh International Conference on Learning Representations_, 2023. 
*   Kirillov et al. (2023) Kirillov, A., Mintun, E., Ravi, N., Mao, H., Rolland, C., Gustafson, L., Xiao, T., Whitehead, S., Berg, A.C., Lo, W.-Y., Dollar, P., and Girshick, R. Segment anything. In _Proceedings of the IEEE/CVF International Conference on Computer Vision (ICCV)_, pp. 4015‚Äì4026, October 2023. 
*   Kirkpatrick et al. (2017) Kirkpatrick, J., Pascanu, R., Rabinowitz, N., Veness, J., Desjardins, G., Rusu, A.A., Milan, K., Quan, J., Ramalho, T., Grabska-Barwinska, A., Hassabis, D., Clopath, C., Kumaran, D., and Hadsell, R. Overcoming catastrophic forgetting in neural networks. _Proceedings of the National Academy of Sciences_, 114(13):3521‚Äì3526, 2017. doi: 10.1073/pnas.1611835114. URL [https://www.pnas.org/doi/abs/10.1073/pnas.1611835114](https://www.pnas.org/doi/abs/10.1073/pnas.1611835114). 
*   Kopiczko et al. (2023) Kopiczko, D.J., Blankevoort, T., and Asano, Y.M. VeRA: Vector-based Random Matrix Adaptation, October 2023. URL [http://arxiv.org/abs/2310.11454](http://arxiv.org/abs/2310.11454). arXiv:2310.11454 [cs]. 
*   Kornblith et al. (2019) Kornblith, S., Shlens, J., and Le, Q.V. Do Better ImageNet Models Transfer Better?, June 2019. URL [http://arxiv.org/abs/1805.08974](http://arxiv.org/abs/1805.08974). arXiv:1805.08974 [cs, stat]. 
*   K√∂pf et al. (2023) K√∂pf, A., Kilcher, Y., von R√ºtte, D., Anagnostidis, S., Tam, Z.-R., Stevens, K., Barhoum, A., Duc, N.M., Stanley, O., Nagyfi, R., ES, S., Suri, S., Glushkov, D., Dantuluri, A., Maguire, A., Schuhmann, C., Nguyen, H., and Mattick, A. Openassistant conversations ‚Äì democratizing large language model alignment, 2023. 
*   Lee et al. (2019) Lee, J., Cho, K., and Kiela, D. Countering language drift via visual grounding. In Inui, K., Jiang, J., Ng, V., and Wan, X. (eds.), _Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP)_, pp. 4385‚Äì4395, Hong Kong, China, November 2019. Association for Computational Linguistics. doi: 10.18653/v1/D19-1447. URL [https://aclanthology.org/D19-1447](https://aclanthology.org/D19-1447). 
*   Lester et al. (2021) Lester, B., Al-Rfou, R., and Constant, N. The Power of Scale for Parameter-Efficient Prompt Tuning. pp. 3045‚Äì3059, January 2021. doi: 10.18653/v1/2021.emnlp-main.243. 
*   Li et al. (2018) Li, X., Grandvalet, Y., and Davoine, F. Explicit Inductive Bias for Transfer Learning with Convolutional Networks, June 2018. URL [http://arxiv.org/abs/1802.01483](http://arxiv.org/abs/1802.01483). arXiv:1802.01483 [cs]. 
*   Li & Liang (2021) Li, X.L. and Liang, P. Prefix-tuning: Optimizing continuous prompts for generation, 2021. 
*   Lin et al. (2022) Lin, S., Hilton, J., and Evans, O. Truthfulqa: Measuring how models mimic human falsehoods, 2022. 
*   Lin et al. (2015) Lin, T.-Y., Maire, M., Belongie, S., Bourdev, L., Girshick, R., Hays, J., Perona, P., Ramanan, D., Zitnick, C.L., and Doll√°r, P. Microsoft coco: Common objects in context, 2015. 
*   Liu et al. (2023a) Liu, W., Qiu, Z., Feng, Y., Xiu, Y., Xue, Y., Yu, L., Feng, H., Liu, Z., Heo, J., Peng, S., Wen, Y., Black, M.J., Weller, A., and Sch√∂lkopf, B. Parameter-efficient orthogonal finetuning via butterfly factorization, 2023a. 
*   Liu et al. (2019) Liu, Y., Ott, M., Goyal, N., Du, J., Joshi, M., Chen, D., Levy, O., Lewis, M., Zettlemoyer, L., and Stoyanov, V. RoBERTa: A Robustly Optimized BERT Pretraining Approach, July 2019. URL [https://arxiv.org/abs/1907.11692v1](https://arxiv.org/abs/1907.11692v1). 
*   Liu et al. (2023b) Liu, Z., Feng, R., Zhu, K., Zhang, Y., Zheng, K., Liu, Y., Zhao, D., Zhou, J., and Cao, Y. (Cones) Cones: Concept Neurons in Diffusion Models for Customized Generation, March 2023b. URL [http://arxiv.org/abs/2303.05125](http://arxiv.org/abs/2303.05125). arXiv:2303.05125 [cs]. 
*   (40) Longpre, S., Hou, L., Vu, T., Webson, A., Chung, H.W., Tay, Y., Zhou, D., Le, Q.V., Zoph, B., Wei, J., and Roberts, A. The Flan Collection: Designing Data and Methods for Effective Instruction Tuning. 
*   Lu et al. (2020) Lu, Y., Singhal, S., Strub, F., Courville, A., and Pietquin, O. Countering language drift with seeded iterated learning. In III, H.D. and Singh, A. (eds.), _Proceedings of the 37th International Conference on Machine Learning_, volume 119 of _Proceedings of Machine Learning Research_, pp. 6437‚Äì6447. PMLR, 13‚Äì18 Jul 2020. URL [https://proceedings.mlr.press/v119/lu20c.html](https://proceedings.mlr.press/v119/lu20c.html). 
*   Mangrulkar et al. (2022) Mangrulkar, S., Gugger, S., Debut, L., Belkada, Y., Paul, S., and Bossan, B. Peft: State-of-the-art parameter-efficient fine-tuning methods. [https://github.com/huggingface/peft](https://github.com/huggingface/peft), 2022. 
*   Mehta et al. (2022) Mehta, S.V., Patil, D., Chandar, S., and Strubell, E. An empirical investigation of the role of pre-training in lifelong learning, 2022. URL [https://openreview.net/forum?id=D9E8MKsfhw](https://openreview.net/forum?id=D9E8MKsfhw). 
*   Mou et al. (2023) Mou, C., Wang, X., Xie, L., Zhang, J., Qi, Z., Shan, Y., and Qie, X. T2i-adapter: Learning adapters to dig out more controllable ability for text-to-image diffusion models. _arXiv preprint arXiv:2302.08453_, 2023. 
*   Mukhopadhyay et al. (2023) Mukhopadhyay, S., Gwilliam, M., Agarwal, V., Padmanabhan, N., Swaminathan, A., Hegde, S., Zhou, T., and Shrivastava, A. Diffusion Models Beat GANs on Image Classification, July 2023. URL [http://arxiv.org/abs/2307.08702](http://arxiv.org/abs/2307.08702). arXiv:2307.08702 [cs]. 
*   OpenAI (2023) OpenAI. Gpt-4 technical report. _ArXiv_, abs/2303.08774, 2023. 
*   Pfeiffer et al. (2020) Pfeiffer, J., Kamath, A., R√ºckl√©, A., Cho, K., and Gurevych, I. _AdapterFusion: Non-Destructive Task Composition for Transfer Learning_. May 2020. 
*   Podell et al. (2023a) Podell, D., English, Z., Lacey, K., Blattmann, A., Dockhorn, T., Muller, J., Penna, J., and Rombach, R. Sdxl: Improving latent diffusion models for high-resolution image synthesis. _ArXiv_, abs/2307.01952, 2023a. 
*   Podell et al. (2023b) Podell, D., English, Z., Lacey, K., Blattmann, A., Dockhorn, T., M√ºller, J., Penna, J., and Rombach, R. SDXL: Improving Latent Diffusion Models for High-Resolution Image Synthesis, July 2023b. URL [http://arxiv.org/abs/2307.01952](http://arxiv.org/abs/2307.01952). arXiv:2307.01952 [cs]. 
*   Qiu et al. (2023) Qiu, Z., Liu, W., Feng, H., Xue, Y., Feng, Y., Liu, Z., Zhang, D., Weller, A., and Sch√∂lkopf, B. Controlling text-to-image diffusion by orthogonal finetuning. _arXiv preprint arXiv:2306.07280_, 2023. 
*   Radford et al. (2021) Radford, A., Kim, J.W., Hallacy, C., Ramesh, A., Goh, G., Agarwal, S., Sastry, G., Askell, A., Mishkin, P., Clark, J., Krueger, G., and Sutskever, I. Learning Transferable Visual Models From Natural Language Supervision, February 2021. URL [http://arxiv.org/abs/2103.00020](http://arxiv.org/abs/2103.00020). arXiv:2103.00020 [cs]. 
*   Richardson et al. (2023) Richardson, E., Goldberg, K., Alaluf, Y., and Cohen-Or, D. ConceptLab: Creative Generation using Diffusion Prior Constraints, August 2023. URL [http://arxiv.org/abs/2308.02669](http://arxiv.org/abs/2308.02669). arXiv:2308.02669 [cs]. 
*   Rombach et al. (2022) Rombach, R., Blattmann, A., Lorenz, D., Esser, P., and Ommer, B. High-resolution image synthesis with latent diffusion models. In _CVPR_, 2022. 
*   Roth et al. (2024) Roth, K., Thede, L., Koepke, A.S., Vinyals, O., Henaff, O.J., and Akata, Z. Fantastic gains and where to find them: On the existence and prospect of general knowledge transfer between any pretrained model. In _The Twelfth International Conference on Learning Representations_, 2024. URL [https://openreview.net/forum?id=m50eKHCttz](https://openreview.net/forum?id=m50eKHCttz). 
*   Ruiz et al. (2022) Ruiz, N., Li, Y., Jampani, V., Pritch, Y., Rubinstein, M., and Aberman, K. Dreambooth: Fine tuning text-to-image diffusion models for subject-driven generation. _CVPR_, 2022. 
*   Ruiz et al. (2023) Ruiz, N., Li, Y., Jampani, V., Pritch, Y., Rubinstein, M., and Aberman, K. DreamBooth: Fine Tuning Text-to-Image Diffusion Models for Subject-Driven Generation, March 2023. URL [http://arxiv.org/abs/2208.12242](http://arxiv.org/abs/2208.12242). arXiv:2208.12242 [cs]. 
*   Saharia et al. (2022) Saharia, C., Chan, W., Saxena, S., Li, L., Whang, J., Denton, E., Ghasemipour, S. K.S., Ayan, B.K., Mahdavi, S.S., Lopes, R.G., Salimans, T., Ho, J., Fleet, D.J., and Norouzi, M. Photorealistic Text-to-Image Diffusion Models with Deep Language Understanding, May 2022. URL [http://arxiv.org/abs/2205.11487](http://arxiv.org/abs/2205.11487). arXiv:2205.11487 [cs]. 
*   Srivastava et al. (2014) Srivastava, N., Hinton, G., Krizhevsky, A., Sutskever, I., and Salakhutdinov, R. Dropout: A simple way to prevent neural networks from overfitting. _Journal of Machine Learning Research_, 15:1929‚Äì1958, 06 2014. 
*   Stojanovski et al. (2022) Stojanovski, Z., Roth, K., and Akata, Z. Momentum-based weight interpolation of strong zero-shot models for continual learning, 2022. 
*   Taori et al. (2023) Taori, R., Gulrajani, I., Zhang, T., Dubois, Y., Li, X., Guestrin, C., Liang, P., and Hashimoto, T.B. Stanford alpaca: An instruction-following llama model. [https://github.com/tatsu-lab/stanford_alpaca](https://github.com/tatsu-lab/stanford_alpaca), 2023. 
*   Touvron et al. (2023a) Touvron, H., Lavril, T., Izacard, G., Martinet, X., Lachaux, M.-A., Lacroix, T., Rozi√®re, B., Goyal, N., Hambro, E., Azhar, F., Rodriguez, A., Joulin, A., Grave, E., and Lample, G. Llama: Open and efficient foundation language models. _ArXiv_, abs/2302.13971, 2023a. 
*   Touvron et al. (2023b) Touvron, H., Martin, L., Stone, K.R., Albert, P., Almahairi, A., Babaei, Y., Bashlykov, N., Batra, S., Bhargava, P., Bhosale, S., Bikel, D.M., Blecher, L., Ferrer, C.C., Chen, M., Cucurull, G., Esiobu, D., Fernandes, J., Fu, J., Fu, W., Fuller, B., Gao, C., Goswami, V., Goyal, N., Hartshorn, A.S., Hosseini, S., Hou, R., Inan, H., Kardas, M., Kerkez, V., Khabsa, M., Kloumann, I.M., Korenev, A.V., Koura, P.S., Lachaux, M.-A., Lavril, T., Lee, J., Liskovich, D., Lu, Y., Mao, Y., Martinet, X., Mihaylov, T., Mishra, P., Molybog, I., Nie, Y., Poulton, A., Reizenstein, J., Rungta, R., Saladi, K., Schelten, A., Silva, R., Smith, E.M., Subramanian, R., Tan, X., Tang, B., Taylor, R., Williams, A., Kuan, J.X., Xu, P., Yan, Z., Zarov, I., Zhang, Y., Fan, A., Kambadur, M., Narang, S., Rodriguez, A., Stojnic, R., Edunov, S., and Scialom, T. Llama 2: Open foundation and fine-tuned chat models. _ArXiv_, abs/2307.09288, 2023b. 
*   Valipour et al. (2023) Valipour, M., Rezagholizadeh, M., Kobyzev, I., and Ghodsi, A. DyLoRA: Parameter Efficient Tuning of Pre-trained Models using Dynamic Search-Free Low-Rank Adaptation, April 2023. URL [http://arxiv.org/abs/2210.07558](http://arxiv.org/abs/2210.07558). arXiv:2210.07558 [cs]. 
*   Wang et al. (2018) Wang, A., Singh, A., Michael, J., Hill, F., Levy, O., and Bowman, S. GLUE: A multi-task benchmark and analysis platform for natural language understanding. In Linzen, T., Chrupa≈Ça, G., and Alishahi, A. (eds.), _Proceedings of the 2018 EMNLP Workshop BlackboxNLP: Analyzing and Interpreting Neural Networks for NLP_, pp. 353‚Äì355, Brussels, Belgium, November 2018. Association for Computational Linguistics. doi: 10.18653/v1/W18-5446. URL [https://aclanthology.org/W18-5446](https://aclanthology.org/W18-5446). 
*   Wang et al. (2023) Wang, Y., Kordi, Y., Mishra, S., Liu, A., Smith, N.A., Khashabi, D., and Hajishirzi, H. Self-Instruct: Aligning Language Models with Self-Generated Instructions, May 2023. URL [http://arxiv.org/abs/2212.10560](http://arxiv.org/abs/2212.10560). arXiv:2212.10560 [cs]. 
*   Xiao et al. (2018) Xiao, T., Liu, Y., Zhou, B., Jiang, Y., and Sun, J. Unified perceptual parsing for scene understanding, 2018. 
*   Xu et al. (2023) Xu, Y., Xie, L., Gu, X., Chen, X., Chang, H., Zhang, H., Chen, Z., Zhang, X., and Tian, Q. QA-LoRA: Quantization-Aware Low-Rank Adaptation of Large Language Models, September 2023. URL [http://arxiv.org/abs/2309.14717](http://arxiv.org/abs/2309.14717). arXiv:2309.14717 [cs]. 
*   Zhang et al. (2023a) Zhang, G., Wang, L., Kang, G., Chen, L., and Wei, Y. Slca: Slow learner with classifier alignment for continual learning on a pre-trained model. In _Proceedings of the IEEE/CVF International Conference on Computer Vision (ICCV)_, pp. 19148‚Äì19158, October 2023a. 
*   Zhang & Agrawala (2023) Zhang, L. and Agrawala, M. Adding conditional control to text-to-image diffusion models. In _ICCV_, 2023. 
*   Zhang et al. (2023b) Zhang, L., Rao, A., and Agrawala, M. Adding Conditional Control to Text-to-Image Diffusion Models, September 2023b. URL [http://arxiv.org/abs/2302.05543](http://arxiv.org/abs/2302.05543). arXiv:2302.05543 [cs]. 
*   Zhang et al. (2023c) Zhang, Q., Chen, M., Bukharin, A., He, P., Cheng, Y., Chen, W., and Zhao, T. Adaptive Budget Allocation for Parameter-Efficient Fine-Tuning, March 2023c. URL [http://arxiv.org/abs/2303.10512](http://arxiv.org/abs/2303.10512). arXiv:2303.10512 [cs]. 
*   Zhang et al. (2018) Zhang, R., Isola, P., Efros, A.A., Shechtman, E., and Wang, O. The Unreasonable Effectiveness of Deep Features as a Perceptual Metric, April 2018. URL [http://arxiv.org/abs/1801.03924](http://arxiv.org/abs/1801.03924). arXiv:1801.03924 [cs]. 
*   Zhang et al. (2023d) Zhang, S., Dong, L., Li, X., Zhang, S., Sun, X., Wang, S., Li, J., Hu, R., Zhang, T., Wu, F., and Wang, G. Instruction Tuning for Large Language Models: A Survey, October 2023d. URL [http://arxiv.org/abs/2308.10792](http://arxiv.org/abs/2308.10792). arXiv:2308.10792 [cs]. 
*   Zhang et al. (2023e) Zhang, Y., Huang, N., Tang, F., Huang, H., Ma, C., Dong, W., and Xu, C. Inversion-Based Style Transfer with Diffusion Models, March 2023e. URL [http://arxiv.org/abs/2211.13203](http://arxiv.org/abs/2211.13203). arXiv:2211.13203 [cs]. 
*   Zhao et al. (2024) Zhao, J., Zhang, Z., Chen, B., Wang, Z., Anandkumar, A., and Tian, Y. Galore: Memory-efficient llm training by gradient low-rank projection, 2024. 
*   Zhao et al. (2023) Zhao, W.X., Zhou, K., Li, J., Tang, T., Wang, X., Hou, Y., Min, Y., Zhang, B., Zhang, J., Dong, Z., Du, Y., Yang, C., Chen, Y., Chen, Z., Jiang, J., Ren, R., Li, Y., Tang, X., Liu, Z., Liu, P., Nie, J.-Y., and Wen, J.-R. A Survey of Large Language Models, September 2023. URL [http://arxiv.org/abs/2303.18223](http://arxiv.org/abs/2303.18223). arXiv:2303.18223 [cs]. 
*   Zhou et al. (2018) Zhou, B., Zhao, H., Puig, X., Xiao, T., Fidler, S., Barriuso, A., and Torralba, A. Semantic Understanding of Scenes through the ADE20K Dataset, October 2018. URL [http://arxiv.org/abs/1608.05442](http://arxiv.org/abs/1608.05442). arXiv:1608.05442 [cs]. 

Appendix

In this appendix, we augment the main paper with additional, qualitative evidence for the learning rate robustness of ETHER transformations in [Appendix A](https://arxiv.org/html/2405.20271v1#A1 "Appendix A Qualitative Evidence of Learning Rate Robustness ‚Ä£ ETHER: Efficient Finetuning of Large-Scale Models with Hyperplane Reflections"). In addition, we also provide benchmark-specific qualitative examples for subject-driven and controllable image generation in [Appendix B](https://arxiv.org/html/2405.20271v1#A2 "Appendix B Qualitative Examples for ETHER Finetuning ‚Ä£ ETHER: Efficient Finetuning of Large-Scale Models with Hyperplane Reflections"). For all experiments - both those in the main paper and supplementary results, we then list all relevant details in [Appendix C](https://arxiv.org/html/2405.20271v1#A3 "Appendix C Experimental Details ‚Ä£ ETHER: Efficient Finetuning of Large-Scale Models with Hyperplane Reflections") for our studies on finetuning in subject-driven image generation (¬ß[C.1](https://arxiv.org/html/2405.20271v1#A3.SS1 "C.1 Subject-driven Generation ‚Ä£ Appendix C Experimental Details ‚Ä£ ETHER: Efficient Finetuning of Large-Scale Models with Hyperplane Reflections")), controllable image synthesis (¬ß[C.2](https://arxiv.org/html/2405.20271v1#A3.SS2 "C.2 Controllable Generation ‚Ä£ Appendix C Experimental Details ‚Ä£ ETHER: Efficient Finetuning of Large-Scale Models with Hyperplane Reflections")), natural language understanding tasks (¬ß[C.3](https://arxiv.org/html/2405.20271v1#A3.SS3 "C.3 Natural Language Understanding ‚Ä£ Appendix C Experimental Details ‚Ä£ ETHER: Efficient Finetuning of Large-Scale Models with Hyperplane Reflections")) and instruction tuning (¬ß[C.4](https://arxiv.org/html/2405.20271v1#A3.SS4 "C.4 Instruction Tuning ‚Ä£ Appendix C Experimental Details ‚Ä£ ETHER: Efficient Finetuning of Large-Scale Models with Hyperplane Reflections")). We then provide two additional ETHER ablations in [Appendix D](https://arxiv.org/html/2405.20271v1#A4 "Appendix D ETHER Ablations ‚Ä£ ETHER: Efficient Finetuning of Large-Scale Models with Hyperplane Reflections") - for the number of block-diagonals and the specific double-sided application in ETHER+. Finally, we discuss LoRA shortcomings for adaptation in the controllable image synthesis task (¬ß[F](https://arxiv.org/html/2405.20271v1#A6 "Appendix F LoRA failures for S2I task ‚Ä£ ETHER: Efficient Finetuning of Large-Scale Models with Hyperplane Reflections")).

Appendix A Qualitative Evidence of Learning Rate Robustness
-----------------------------------------------------------

As introduced in Sec.[3](https://arxiv.org/html/2405.20271v1#S3 "3 Method ‚Ä£ ETHER: Efficient Finetuning of Large-Scale Models with Hyperplane Reflections"), when finetuning with ETHER transformation, by construction, the learning rate only controls the speed with which reflection angels change. As a consequence, ETHER methods are much more robust to learning rate choices, and less likely to diverge and cause model deterioration. This allows for user control over the convergence speed while minimizing the risk of model collapse during training. To demonstrate this, Sec.[4](https://arxiv.org/html/2405.20271v1#S4 "4 Intriguing Properties of ETHER ‚Ä£ ETHER: Efficient Finetuning of Large-Scale Models with Hyperplane Reflections") introduced both a qualitative example comparing the impact of minimal and maximal perturbation strength on the model output in Fig.[3](https://arxiv.org/html/2405.20271v1#S3.F3 "Figure 3 ‚Ä£ 3.4 Efficient ETHER through Block-Parallelism ‚Ä£ 3 Method ‚Ä£ ETHER: Efficient Finetuning of Large-Scale Models with Hyperplane Reflections"), and quantitative evaluations on the Semantic Map to Image task against learning rate choices in Figs.[5](https://arxiv.org/html/2405.20271v1#S4.F5 "Figure 5 ‚Ä£ 4 Intriguing Properties of ETHER ‚Ä£ ETHER: Efficient Finetuning of Large-Scale Models with Hyperplane Reflections") and [6](https://arxiv.org/html/2405.20271v1#S4.F6 "Figure 6 ‚Ä£ 4 Intriguing Properties of ETHER ‚Ä£ ETHER: Efficient Finetuning of Large-Scale Models with Hyperplane Reflections").

In this section, we augment Sec.[4](https://arxiv.org/html/2405.20271v1#S4 "4 Intriguing Properties of ETHER ‚Ä£ ETHER: Efficient Finetuning of Large-Scale Models with Hyperplane Reflections") and provide additional qualitative results and impressions to highlight the non-deteriorating nature of ETHER transformation. For this, we showcase subject-driven generation results using different finetuning methods in Fig.[8](https://arxiv.org/html/2405.20271v1#A1.F8 "Figure 8 ‚Ä£ Appendix A Qualitative Evidence of Learning Rate Robustness ‚Ä£ ETHER: Efficient Finetuning of Large-Scale Models with Hyperplane Reflections"), with default generations using the best learning rate. We then systematically increase the finetuning learning rate by 10 10 10 10 and by 100 100 100 100 times, and visualize the correspondingly generated output. As can be seen, for 10√ó10\times 10 √ó higher learning rates OFT and Naive fail to follow the text prompt, while LoRA finetuning quickly collapses. With 10√ó10\times 10 √ó lower learning rates instead, OFT, Naive and ETHER are not able to generate the subject correctly in the predefined number of iterations.

![Image 8: Refer to caption](https://arxiv.org/html/2405.20271v1/x8.png)

Figure 8: Qualitative visualization of learning rate robustness of ETHER and ETHER+ in subject-driven generation finetuning. We see how ETHER methods are able to consistently produce good results avoiding model deterioration. Specifically, ETHER+ shows impressive capabilities, being able to follow the subject-prompt instructions in the widest learning rate range.

Appendix B Qualitative Examples for ETHER Finetuning
----------------------------------------------------

We show some qualitative results by using the finetuning methods proposed in this paper.

### B.1 Subject-driven Generation.

In Figure [9](https://arxiv.org/html/2405.20271v1#A2.F9 "Figure 9 ‚Ä£ B.1 Subject-driven Generation. ‚Ä£ Appendix B Qualitative Examples for ETHER Finetuning ‚Ä£ ETHER: Efficient Finetuning of Large-Scale Models with Hyperplane Reflections") we report subject-driven generation examples. In particular, for a fair comparison, we report images which come from the same noise vector in the Stable Diffusion latent space. For the sunglasses images, we see how non-ETHER methods manage to reproduce the subject, but fail to follow the text prompt in most cases. Interestingly in the first row, we notice how ETHER+ is able to properly control the generation, by transforming the yellow area (associated to a beer in other models) in an enlightened Eiffel Tower. For the teapot images instead, we see how ETHER+ is able to better keep the appearances of the subject.

![Image 9: Refer to caption](https://arxiv.org/html/2405.20271v1/x9.png)

Figure 9: Subject-driven Generation results. Each row shares initial latent noise (notice row-wise similarities). We can see that ETHER+ method is better at adapting the model to the subjects. Notice how for the pink sunglasses, OFT and Naive fail in following the prompt.

### B.2 Controllable Generation.

In Figure [10](https://arxiv.org/html/2405.20271v1#A2.F10 "Figure 10 ‚Ä£ B.2 Controllable Generation. ‚Ä£ Appendix B Qualitative Examples for ETHER Finetuning ‚Ä£ ETHER: Efficient Finetuning of Large-Scale Models with Hyperplane Reflections") we show some examples from the Semantic Map to Image task. In particular, we notice how in the first row all models but ETHER+ fail to control the image correctly, not being able to separate the land from the water. Additionally, in the second row OFT fails to generate the sky, while Naive presents a halo effect. These examples showcase the abilities of ETHER+ finetuning over the other methods.

![Image 10: Refer to caption](https://arxiv.org/html/2405.20271v1/x10.png)

Figure 10: Semantic Map to Image Qualitative Results. We notice how in the first row all models but ETHER+ fail to control the image correctly. Overall ETHER+ controlled images show better control.

To show broader controllable capabilities, we also report few qualitative examples with ETHER methods trained with Landmarks and Canny Edge Maps control signals on CelebA-HQ (Karras et al., [2018](https://arxiv.org/html/2405.20271v1#bib.bib23)) and COCO 2017 (Lin et al., [2015](https://arxiv.org/html/2405.20271v1#bib.bib36)) datasets respectively.

![Image 11: Refer to caption](https://arxiv.org/html/2405.20271v1/x11.png)

Figure 11: Examples of Landmark to Face (left) and Canny Edge Map to Image (right) controlled generation with ETHER methods.

Appendix C Experimental Details
-------------------------------

This section provides additional experimental details for replication not listed in the main benchmark experimental section[5](https://arxiv.org/html/2405.20271v1#S5 "5 Benchmark Experiments ‚Ä£ ETHER: Efficient Finetuning of Large-Scale Models with Hyperplane Reflections"). It is worth noting that while in most of our experiments we do not employ regular dropout (Srivastava et al., [2014](https://arxiv.org/html/2405.20271v1#bib.bib58)), Liu et al. ([2023a](https://arxiv.org/html/2405.20271v1#bib.bib37)) proposes a multiplicative dropout form specifically designed for multiplicative finetuning methods, which we did not test in this study. We hypothesize that this specialized dropout technique could potentially work better than regular dropout for ETHER and ETHER+ as well. We also note that Qiu et al. ([2023](https://arxiv.org/html/2405.20271v1#bib.bib50)) report OFT‚Äôs number of parameters as half of the actual trainable parameters due to the redundancy in the skew symmetric matrices S B superscript ùëÜ ùêµ S^{B}italic_S start_POSTSUPERSCRIPT italic_B end_POSTSUPERSCRIPT in the Cayley parametrization of Q B superscript ùëÑ ùêµ Q^{B}italic_Q start_POSTSUPERSCRIPT italic_B end_POSTSUPERSCRIPT. Basically, we they report the storage parameters for Q B superscript ùëÑ ùêµ Q^{B}italic_Q start_POSTSUPERSCRIPT italic_B end_POSTSUPERSCRIPT rather than the training parameters. For consistency and fair comparisons, we follow the same convention for OFT throughout our paper.

### C.1 Subject-driven Generation

For subject-driven generation, we follow the same setting listed in DreamBooth (Ruiz et al., [2023](https://arxiv.org/html/2405.20271v1#bib.bib56)), using DreamBooth and OFT (Qiu et al., [2023](https://arxiv.org/html/2405.20271v1#bib.bib50)) baselines as implemented in official [OFT](https://github.com/Zeju1997/oft) GitHub repository. The additional trainable layers follow (Qiu et al., [2023](https://arxiv.org/html/2405.20271v1#bib.bib50)) and are added to the Q,K,V layers and the projection layer inside every attention module. The training is performed over 1400 iterations for each method, evaluating the generation results every 200 iterations at selecting the best one (typically around 1200 iterations). For DreamBooth and OFT, we follow the original implementations and use a learning rate of 5√ó10‚àí6 5 superscript 10 6 5\times 10^{-6}5 √ó 10 start_POSTSUPERSCRIPT - 6 end_POSTSUPERSCRIPT and 6√ó10‚àí5 6 superscript 10 5 6\times 10^{-5}6 √ó 10 start_POSTSUPERSCRIPT - 5 end_POSTSUPERSCRIPT respectively, with a batch size of 1 1 1 1. For Naive - the non-orthogonal OFT variant - we use the same setting of OFT for a fair comparison. For LoRA we select a learning rate of 6√ó10‚àí4 6 superscript 10 4 6\times 10^{-4}6 √ó 10 start_POSTSUPERSCRIPT - 4 end_POSTSUPERSCRIPT. For ETHER and ETHER+, we use a learning rate of 6√ó10‚àí3 6 superscript 10 3 6\times 10^{-3}6 √ó 10 start_POSTSUPERSCRIPT - 3 end_POSTSUPERSCRIPT. We perform the training on a Tesla V100-32GB GPU.

### C.2 Controllable Generation

For our experiments on controllable image generation we follow the setting of Qiu et al. ([2023](https://arxiv.org/html/2405.20271v1#bib.bib50)), using the signal encoder from ControlNet (Zhang & Agrawala, [2023](https://arxiv.org/html/2405.20271v1#bib.bib69)) (comprising 8 trainable convolutional layers, accounting for 3.1M additional learnable parameters). Finetuning parameters are added to the Q,K,V layers as well as the projection layer of the attention modules and the subsequent feedforward layers. As baselines, we use the official implementation of [OFT](https://github.com/Zeju1997/oft). Similarly to Qiu et al. ([2023](https://arxiv.org/html/2405.20271v1#bib.bib50)), for OFT and Naive we use a learning rate of 1√ó10‚àí5 1 superscript 10 5 1\times 10^{-5}1 √ó 10 start_POSTSUPERSCRIPT - 5 end_POSTSUPERSCRIPT. For ETHER and ETHER+ we use a larger learning rate of 1√ó10‚àí3 1 superscript 10 3 1\times 10^{-3}1 √ó 10 start_POSTSUPERSCRIPT - 3 end_POSTSUPERSCRIPT. For all experiments, we upper bound the learning rate of the signal encoder to 1√ó10‚àí4 1 superscript 10 4 1\times 10^{-4}1 √ó 10 start_POSTSUPERSCRIPT - 4 end_POSTSUPERSCRIPT. We perform all the training runs on a single Nvidia-A100-40GB with a batch size of 10. As listed in Sec.[5.1.2](https://arxiv.org/html/2405.20271v1#S5.SS1.SSS2 "5.1.2 Controllable Image Generation ‚Ä£ 5.1 ETHER for Image-generative Model Adaptation ‚Ä£ 5 Benchmark Experiments ‚Ä£ ETHER: Efficient Finetuning of Large-Scale Models with Hyperplane Reflections") and expanded in Sec.[F](https://arxiv.org/html/2405.20271v1#A6 "Appendix F LoRA failures for S2I task ‚Ä£ ETHER: Efficient Finetuning of Large-Scale Models with Hyperplane Reflections"), we tried to utilize LoRA for controllable generation as well but found no comparable results even after extensive trials with different hyperparameters.

### C.3 Natural Language Understanding

For our GLUE benchmark experiments finetuning DeBERTaV3-base (He et al., [2023](https://arxiv.org/html/2405.20271v1#bib.bib15)), we make use of the [peft](https://github.com/huggingface/peft) Hugging Face repository (Mangrulkar et al., [2022](https://arxiv.org/html/2405.20271v1#bib.bib42)) as the basis for our implementations. To compare our results with those of Liu et al. ([2023a](https://arxiv.org/html/2405.20271v1#bib.bib37)), we follow their implementation and apply ETHER and ETHER+ to all the linear layers in every transformer block. The relevant hyperparameters for each task are reported in Tab.[8](https://arxiv.org/html/2405.20271v1#A3.T8 "Table 8 ‚Ä£ C.4 Instruction Tuning ‚Ä£ Appendix C Experimental Details ‚Ä£ ETHER: Efficient Finetuning of Large-Scale Models with Hyperplane Reflections"). All training runs are conducted on a single Nvidia-A100-40GB GPU.

Table 7: GLUE benchmark hyperparameters.

Method Hyperparameters MNLI SST-2 CoLA QQP QNLI RTE MRPC STS-B
Learning Rate 8e-4 1e-3 1e-3 3e-4 1e-3 1e-3 3e-4 2e-3
Batch Size 32 32 32 8 8 32 32 8
ETHER Num. Epochs 9 14 10 20 7 13 14 8
Dropout 1e-3 1e-3 1e-1 1e-1 1e-3 1e-2 1e-1 1e-1
Max Seq. Len.256 128 64 320 512 320 320 128
Learning Rate 8e-4 1e-4 1e-3 3e-3 3e-3 3e-4 8e-4 8e-4
Batch Size 8 8 8 32 32 8 32 8
ETHER+Num. Epochs 8 10 6 16 5 35 17 11
Dropout 1e-3 1e-3 1e-1 1e-3 1e-3 1e-3 1e-2 1e-3
Max Seq. Len.256 128 64 320 512 320 320 128

### C.4 Instruction Tuning

For our Instruction Tuning experiments, we use the LoRA (Hu et al., [2022](https://arxiv.org/html/2405.20271v1#bib.bib20)) finetuning implementation in the [lit-gpt](https://github.com/Lightning-AI/lit-gpt) repository (AI, [2023](https://arxiv.org/html/2405.20271v1#bib.bib1)) as baseline. For evaluations, we make use of Gao et al. ([2023](https://arxiv.org/html/2405.20271v1#bib.bib11))‚Äôs benchmark implementations. For the recently proposed VeRA (Kopiczko et al., [2023](https://arxiv.org/html/2405.20271v1#bib.bib28)) baseline, we reproduce the model implementation following their best performing method as described in the paper: sampling random A ùê¥ A italic_A and B ùêµ B italic_B matrices with uniform kaiming initialization scaled by the matrix dimension, and a learnable, non-zero diagonalized vector initialized as a vector of all zeros apart for one element equal to 0.1 0.1 0.1 0.1. Same for OFT, for which we follow the implementation in the official repository [oft](https://github.com/Zeju1997/oft), selecting the number of block-diagonal matrices such that the overall number of parameters becomes comparable with ETHER+ and LoRA rank 8. For all experiments, we use a cosine annealing learning rate scheduler, no dropout, and 1000 warmup steps. For LoRA, VeRA, and OFT we use AdamW optimizer with a weight decay of 0.01, while for ETHER methods, given the normalization happening on the parameters, weight decay would have limited impact and thus we set it to 0. For LoRA and VeRA, we keep Œ± ùõº\alpha italic_Œ± fixed with respect to the learning rate by setting it equal to the rank. For all experiments, we conduct an extensive grid search over learning rates and batch sizes. For each combination, we perform the LLama-2-7B (Touvron et al., [2023b](https://arxiv.org/html/2405.20271v1#bib.bib62)) finetuning over Alpaca (Taori et al., [2023](https://arxiv.org/html/2405.20271v1#bib.bib60)) for one epoch. All training runs are conducted on a single Nvidia-A100-40GB GPU, but could also be run on a consumer NVIDIA GeForce-RTX-3090-24G GPU.

Table 8: Instruction Tuning hyperparameters.

VeRA r=64 VeRA r=256 LoRA r=1 LoRA r=8 OFT n=256 ETHER n=32 ETHER+n=32
Learning Rate 5e-3 1e-3 3e-3 5e-4 5e-4 2e-3 5e-3
Batch Size 32 32 8 8 16 8 16

Appendix D ETHER Ablations
--------------------------

This section details additional ablation experiments on the impact of the block-diagonality degree on the final performance, as well as experimental support to the theoretical motivation in Sec.[3.3](https://arxiv.org/html/2405.20271v1#S3.SS3 "3.3 Relaxing Orthogonality in ETHER ‚Ä£ 3 Method ‚Ä£ ETHER: Efficient Finetuning of Large-Scale Models with Hyperplane Reflections") to apply the relaxed Householder transformation on both the left and right side of the weight matrix.

### D.1 Block-diagonal ETHER Performances

In [Table 9](https://arxiv.org/html/2405.20271v1#A4.T9 "In D.1 Block-diagonal ETHER Performances ‚Ä£ Appendix D ETHER Ablations ‚Ä£ ETHER: Efficient Finetuning of Large-Scale Models with Hyperplane Reflections") and [Table 10](https://arxiv.org/html/2405.20271v1#A4.T10 "In D.1 Block-diagonal ETHER Performances ‚Ä£ Appendix D ETHER Ablations ‚Ä£ ETHER: Efficient Finetuning of Large-Scale Models with Hyperplane Reflections"), we compare the usage of multiple diagonal blocks for ETHER finetuning to allow for fast performance, especially in large models domain. Both tables augment our method description in Sec.[3.4](https://arxiv.org/html/2405.20271v1#S3.SS4 "3.4 Efficient ETHER through Block-Parallelism ‚Ä£ 3 Method ‚Ä£ ETHER: Efficient Finetuning of Large-Scale Models with Hyperplane Reflections") and the shortened results in Tab.[1](https://arxiv.org/html/2405.20271v1#S3.T1 "Table 1 ‚Ä£ 3.3 Relaxing Orthogonality in ETHER ‚Ä£ 3 Method ‚Ä£ ETHER: Efficient Finetuning of Large-Scale Models with Hyperplane Reflections"). In all cases, we notice that performance remains almost unaffected by the choice of block number, while on the other hand, the computational efficiency consistently increases (8.22 8.22 8.22 8.22 TFLOPs for n=32 ùëõ 32 n=32 italic_n = 32 versus 25.26 25.26 25.26 25.26 TFLOPs for n=1 ùëõ 1 n=1 italic_n = 1 for Llama-2-7B). It is worth noting that results for ETHER+ with n=32 ùëõ 32{n=32}italic_n = 32 show better performance with respect to less diagonalized counterparts.

Table 9: Semantic Map to Image (S2I) results for different number of diagonal blocks n ùëõ n italic_n on ETHER finetuning at epoch 10

ETHER#params mIoU ‚Üë‚Üë\uparrow‚ÜëAcc ‚Üë‚Üë\uparrow‚ÜëFID‚Üì‚Üì\downarrow‚Üì
n=1 ùëõ 1{n=1}italic_n = 1 0.1M 23.1 61.23 31.7
n=4 ùëõ 4{n=4}italic_n = 4 0.1M 22.9 60.92 30.5
n=16 ùëõ 16{n=16}italic_n = 16 0.1M 22.3 60.35 30.7

Table 10: Instruction Tuning results for different number of diagonal blocks n ùëõ n italic_n on ETHER finetuning

ETHER+#params TFLOPs MMLU ‚Üë‚Üë\uparrow‚ÜëARC ‚Üë‚Üë\uparrow‚ÜëTru-1‚Üë‚Üë\uparrow‚ÜëTru-2 ‚Üë‚Üë\uparrow‚Üë
n=1 ùëõ 1{n=1}italic_n = 1 1.04M 51.65 43.75 46.76 28.03 41.06
n=4 ùëõ 4{n=4}italic_n = 4 1.04M 18.66 43.91 45.73 27.54 40.46
n=32 ùëõ 32{n=32}italic_n = 32 1.04M 9.04 44.87 46.50 29.38 43.51

### D.2 Double-sided Application of ETHER+

Finally, we provide a brief ablation study in Tab.[11](https://arxiv.org/html/2405.20271v1#A4.T11 "Table 11 ‚Ä£ D.2 Double-sided Application of ETHER+ ‚Ä£ Appendix D ETHER Ablations ‚Ä£ ETHER: Efficient Finetuning of Large-Scale Models with Hyperplane Reflections"), comparing the ETHER+ performance when applying the relaxed Householder transformations H+superscript ùêª H^{+}italic_H start_POSTSUPERSCRIPT + end_POSTSUPERSCRIPT on only one side versus both sides. Although the parameter count doubles, we observe a significant increase in performance (e.g. 0.666 0.666 0.666 0.666 vs 0.618 0.618 0.618 0.618 in DINO score) as higher transformation distances can be achieved.

Table 11: Subject-driven Generation image quality results comparison (at iteration 1200) among standard ETHER+ and its version only applied on one side of the weight matrix.

#params DINO ‚Üë‚Üë\uparrow‚ÜëCLIP-I ‚Üë‚Üë\uparrow‚Üë
ETHER+ (one-sided)0.2M 0.618 0.777
ETHER+0.4M 0.666 0.800

Appendix E VTAB preliminary results
-----------------------------------

We also perform a small evaluation over a subset of the popular Visual Task Adaptation Benchmark (VTAB), using an ImageNet-21k pretrained ViT-B. As can be seen, ETHER and ETHER+ perform comparably to OFT with n=256 ùëõ 256 n=256 italic_n = 256 and LoRA rank 8 8 8 8, while using a fraction of the trainable parameters.

Table 12: VTAB results

#params Natural Specialized Structured
Caltech101 DTD Flowers102 SVHN EuroSAT sNORB-Elev
Full Finetuning 85.8M 96.26 73.03 98.71 73.71 96.16 63.36
Linear Probing 0 95.96 72.34 99.12 52.55 95.03 34.09
LoRA r=8 1.33M 97.69 77.50 99.10 97.40 98.92 74.89
OFT n=256 0.29M 96.95 75.80 98.60 96.58 98.83 74.37
ETHER 0.08M 97.64 75.85 98.83 95.81 98.80 74.17
ETHER+0.33M 98.27 76.92 98.88 96.84 99.15 78.41

Appendix F LoRA failures for S2I task
-------------------------------------

As described at the beginning of our benchmark experimental section (Sec.[5.1.2](https://arxiv.org/html/2405.20271v1#S5.SS1.SSS2 "5.1.2 Controllable Image Generation ‚Ä£ 5.1 ETHER for Image-generative Model Adaptation ‚Ä£ 5 Benchmark Experiments ‚Ä£ ETHER: Efficient Finetuning of Large-Scale Models with Hyperplane Reflections"), we excluded LoRA from our experimental results listed in Tab.[3](https://arxiv.org/html/2405.20271v1#S5.T3 "Table 3 ‚Ä£ 5.1.1 Subject-driven Generation ‚Ä£ 5.1 ETHER for Image-generative Model Adaptation ‚Ä£ 5 Benchmark Experiments ‚Ä£ ETHER: Efficient Finetuning of Large-Scale Models with Hyperplane Reflections"), as it did not convincingly converge to good results. In this section, we provide additional experimental details on our application of LoRA to controllable image synthesis, given which still no suitable results were able to be achieved. In particular, LoRA finetuning for the Semantic Map to Image task (S2I) task was tested on a large, randomly subsampled grid of learning rate {1e-5, 5e-5, 1e-4, 1e-3, 1e-2}, batch size {4, 8, 16, 32, 64}, rank {4, 8, 64}, scaling factor {0.1, 1}, dropout {0, 0.1}, for which we also visualize reflective samples in Fig.[12](https://arxiv.org/html/2405.20271v1#A6.F12 "Figure 12 ‚Ä£ Appendix F LoRA failures for S2I task ‚Ä£ ETHER: Efficient Finetuning of Large-Scale Models with Hyperplane Reflections").

![Image 12: Refer to caption](https://arxiv.org/html/2405.20271v1/x12.png)

Figure 12: LoRA randomly sampled generated samples from validation set control. Control signal reported on the left.
