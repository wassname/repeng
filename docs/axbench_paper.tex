\documentclass{article}


\begin{icmlauthorlist}
\icmlauthor{Zhengxuan Wu}{equal,stanford}
\icmlauthor{Aryaman Arora}{equal,stanford}
\icmlauthor{Atticus Geiger}{prair}
\icmlauthor{Zheng Wang}{stanford}
\icmlauthor{Jing Huang}{stanford}\\
\icmlauthor{Dan Jurafsky}{stanford}
\icmlauthor{Christopher D. Manning}{stanford}
\icmlauthor{Christopher Potts}{stanford}
\end{icmlauthorlist}

\icmlaffiliation{stanford}{Department of Computer Science, Stanford University}
\icmlaffiliation{prair}{Pr(AI)\textsuperscript{2}R Group}

\icmlcorrespondingauthor{Zhengxuan Wu}{\nolinkurl{wuzhengx@cs.stanford.edu}}
\icmlcorrespondingauthor{Aryaman Arora}{\nolinkurl{aryaman@cs.stanford.edu}}

% You may provide any keywords that you
% find helpful for describing your paper; these are used to populate
% the "keywords" metadata in the PDF but will not be shown in the document
\icmlkeywords{language models, interpretability}

\vskip 0.3in
]

% this must go after the closing bracket ] following \twocolumn[ ...

% This command actually creates the footnote in the first column
% listing the affiliations and the copyright notice.
% The command takes one argument, which is text to display at the start of the footnote.
% The \icmlEqualContribution command is standard text for equal contribution.
% Remove it (just {}) if you do not need this facility.

%\printAffiliationsAndNotice{}  % leave blank if no need to mention equal contribution
\printAffiliationsAndNotice{\icmlEqualContribution} % otherwise use the standard text.

\doparttoc 
\faketableofcontents
\begin{abstract}
Fine-grained steering of language model outputs is essential for safety and reliability. Prompting and finetuning are widely used to achieve these goals, but interpretability researchers have proposed a variety of representation-based techniques as well, including sparse autoencoders (SAEs), linear artificial tomography, supervised steering vectors, linear probes, and representation finetuning. At present, there is no benchmark for making direct comparisons between these proposals. Therefore, we introduce \shortbenchmark{}, a large-scale benchmark for steering and concept detection, and report experiments on \texttt{Gemma-2-2B} and \texttt{9B}. For steering,  we find that prompting outperforms all existing methods, followed by finetuning.
For concept detection, representation-based methods such as difference-in-means, perform the best. On both evaluations, SAEs are not competitive. 
We introduce a novel weakly-supervised representational method (Rank-1 Representation Finetuning; \mbox{ReFT-r1}), which is competitive on both tasks while providing the interpretability advantages that prompting lacks. 
Along with \shortbenchmark{}, we train and publicly release SAE-scale feature dictionaries for ReFT-r1 and DiffMean.
%%%%%%%%%%%%%% TODO: COMMENT BACK %%%%%%%%%%%%%% 
\begin{center}
\small
\raisebox{-0.2\height}{\includegraphics[width=1em,height=1em]{assets/github.png}}\hspace{0.5em}\href{https://github.com/stanfordnlp/axbench}{\texttt{https://github.com/stanfordnlp/axbench}}
\end{center}
\end{abstract}

\section{Introduction}

In order to be useful, language models (LMs) must follow user instructions and be aligned to human goals and values. While prompting and finetuning are now widely used to instill such behaviour in LMs, both methods have limitations: circumvention via jailbreaks and continued training, reliance on dataset quality, and uninterpretability \citep{anwar2024foundationalchallengesassuringalignment}.
Interpretability researchers have thus proposed a new class of representation-based interventions for \textbf{steering} LMs, which hope to address these issues. These methods include learning steering vectors from small labelled datasets, self-supervised sparse autoencoders (SAEs), among other techniques. Since steering may enable lightweight and interpretable control over model outputs, it has emerged as a potential alternative to finetuning and prompting (see \cref{sec:related}).

\begin{figure}[!t]
    \centering
    \includegraphics[width=0.9\linewidth]{figures/concept500_summary.pdf}
    \caption{Average results across eight tasks on \concepttag{} concept detection (0--2) vs.~\steeringtag{} model steering (0--2) for all methods on \shortbenchmark{}. *Only evaluated on \texttt{Gemma-2-2B}.}
    \label{fig:sdl}
\end{figure}

Unfortunately, \citet{pres2024reliableevaluationbehaviorsteering,braun2024} note that existing benchmarks for steering only evaluate a few methods at merely toy scales. To assess whether representation steering is a viable alternative to existing model control techniques, we need to evaluate it in a more realistic setting, e.g.~over open-vocabulary concepts and on long-form generation, and compare it to prompting and finetuning baselines.

% However, current steering techniques are difficult to scale up due to their reliance on task-specific data; these include intervention-based methods which search for causally-relevant circuits or features, which are trained in a supervised manner \cite{geiger2024causal}. Sparse autoencoders \cite[SAEs;][see \cref{sec:related}]{bricken2023monosemanticity,templeton2024scaling,gao2024scalingevaluatingsparseautoencoders} tackle the scalability problem by \textit{self-supervisedly} learning sparse dictionaries on LM activations collected over an unlabelled text corpus. Alternatively, LLM-aided synthetic data generation can scale up \textit{supervised} intervention-based methods \citep{shaham2024multimodal}.

% Both SAEs and supervised intervention methods have shown promise at steering LM behaviours \cite{huben2024sparse,rimsky-etal-2024-steering,wu2024reft}.
% However, comprehensive and large-scale evaluation benchmarks for these approaches are lacking. Moreover, these tools are still costly to scale, with a complex pipeline which inhibit iterative improvement over task objectives, effectively preventing any practical deployment.

In this work, we introduce \textbf{\shortbenchmark{}}, a benchmark for evaluating LM control methods at scale using synthetic data. \shortbenchmark{} takes in a list of natural language descriptions of concepts and samples relevant training and evaluation data from an LLM. We evaluate model-control methods, including prompting and finetuning baselines, along two utility \underline{\textbf{ax}}es: \textbf{concept detection} \concepttag{} and \textbf{model steering} \steeringtag{}. For the former, we use labelled synthetic data as ground truth; for the latter, we evaluate long-form generations using an LLM judge. The labelled training data enables comparison between supervised dictionary-learning methods (SDLs) and unsupervised methods like SAEs. The benchmark includes tasks generated from SAE concept lists for \texttt{GemmaScope} \cite{lieberum-etal-2024-gemma}, covering two layers each from \emph{instruction-tuned} \texttt{Gemma-2-2B} and \texttt{Gemma-2-9B} \cite{gemmateam2024gemma2improvingopen}. However, \shortbenchmark{} is by nature extensible to arbitrary concept descriptions: we intend to add new evaluation tasks as better feature-labelling techniques and new approaches to steering emerge.

We evaluate a variety of steering methods---including a novel weakly-supervised method we introduce, \textbf{ReFT-r1}---along with prompting, full finetuning, and two parameter-efficient finetuning methods (LoRA and LoReFT). On steering, only ReFT-r1 is competitive with finetuning and prompting baselines, while SAEs fall behind both ReFT-r1 and difference-in-means \citep{marks2024geometrytruthemergentlinear} on both axes. While representation steering methods largely lag behind incumbent model-control techniques, ReFT-r1 is evidence that steering can be pushed further with the availability of comprehensive evaluation benchmarks. Finally, along with \shortbenchmark{}, we train and publicly release SAE-scale feature dictionaries for ReFT-r1 and DiffMean.
% %%%%%%%%%%%%%% TODO: COMMENT BACK %%%%%%%%%%%%%% 
\footnote{We open-source all of our datasets and trained dictionaries at \url{https://huggingface.co/pyvene}.}; we call this approach \textit{supervised dictionary learning} (SDL; \cref{fig:axbench})

% In this work, we introduce \textbf{\shortbenchmark{}}, a framework for evaluating existing interpretability methods at scale using synthetic data. We evaluate along two utility \underline{\textbf{ax}}es: \textbf{concept detection} \concepttag{} and \textbf{model steering} \steeringtag{}. Surprisingly, we find that all existing steering methods fail to outperform a simple prompting-only baseline and generally lag behind gradient-based finetuning methods, disqualifying them from practical usage in \emph{the status quo}.

% In this work, we introduce \textbf{\shortbenchmark{}}, a framework for evaluating existing interpretability methods at scale using synthetic data. We formally descibe existing model interpretability methods (such as probing and SAEs) as \emph{model abstractions} which aim to simplify complex LM behaviours. We focus on two tasks: \textbf{concept detection} and \textbf{model steering}. Surprisingly, we find that all existing methods struggle to outperform a simple prompting-only baseline, disqualifying them from any practical usage. 

% Representations in language models (LMs) serve as the medium through which input prompts are encoded, processed and then decoded into structured next-token predictions. These representations inherently capture latent patterns of syntax and semantics within language~\cite{}, yet their high-dimensional nature makes them difficult to interpret and manipulate, limiting their practicality as an interface for interacting with LMs. 

% Model interpretability has made significant progress, yet its real-world impact remains limited. Unlike prompting LM with natural languages for model control and explanation~\cite{}, current interpretability insights are often task-specific, limited in vocabulary, and difficult to scale~\cite{}. Recent works such as sparse autoencoders (SAEs) tackle these challenges by providing scalable techniques in explaining and steering LMs~\cite{}. However, proper evaluations of these tools are lacking despite their anecdotic successes~\cite{}. Moreover, existing tools are costly to scale with a lengthy pipeline, which disallows them to iterate over task objectives effectively preventing any practical deployment~\cite{}. 

% In this work, we define existing model interpretability tools such as probing or SAEs as \emph{model abstractions} with a goal of simplifying complex LMs through different lenses for two purposes: latent semantic detection and model steering. We then introduce \textbf{\longbenchmark{}} (\textbf{\shortbenchmark{}}), a framework to evaluate existing interpretability tools grounded with these purposes at scale. Surprisingly, we show that existing tools struggle to outperform the simplest baselines, disqualifying them from any practical usage.

% In response, we propose \textbf{\longmethod{}} (\textbf{\shortmethod{}}), a novel and scalable approach for creating abstractions in an open-vocabulary setting. Building on recent works in representation finetuning (ReFT)~\cite{}, \shortmethod\ automates the pipeline of finding linear subspaces corresponding human interpretable concepts at scale with LM-in-the-loop. \TODO{ReAX result goes here.}

% Set up the header and footer
\pagestyle{fancy}
\fancyhf{}
\setlength{\headwidth}{\textwidth}
\fancyhfoffset[L,R]{0pt}
\fancyhead[C]{\shorttitle{}}
\setlength{\headheight}{15pt}

\begin{figure*}[!t]
    \centering
    \includegraphics[width=0.85\textwidth]{assets/AxBench_v2.png}
    \caption{Key components of \shortbenchmark{}: (a) an example of how we collect data for evaluating concept detection and model steering; (b) the synthetic data generation process for training and evaluation given \textit{Golden Gate Bridge} as a concept; and (c) the contrasting training pipelines of SAEs and SDLs; both use LLMs, but SAEs use them to label pretrained features while we instead direct them to generate training data.}
    \label{fig:axbench}
\end{figure*}

\section{Related work}
\label{sec:related}

\paragraph{Representation-based control.} Interventional/causal interpretability has emerged as the dominant paradigm for understanding neural networks in the LLM era, enabling the reverse-engineering of circuits underlying specific behaviours \cite{giulianelli-etal-2018-hood,genderbias,causalabstraction,pmlr-v162-geiger22a,meng2022,chan2022causal,wang2022interpretability,goldowsky2023localizing,geiger2024causal,guerner2024geometricnotioncausalprobing,geiger2024causal}. An important assumption in much of this work is the \textbf{linear representation hypothesis}, which claims that linear subspaces of representations in neural networks encode concepts \citep{mikolov-etal-2013-linguistic,pennington-etal-2014-glove,bolukbasi2016man,elhage2022superposition,park2023linear,nanda-etal-2023-emergent}. Intervening on representations has thus emerged as an alternative to finetuning and prompting for LM control.

\textit{Representation-based steering} by adding fixed vectors to activations, or clamping activations to a certain value along fixed directions, is one such intervention-based tool for model control \citep{zou2023representationengineeringtopdownapproach,liInferenceTimeInterventionEliciting2024,turner2024steeringlanguagemodelsactivation,marks2024geometrytruthemergentlinear,liu2024incontext,vanderweij2024extendingactivationsteeringbroad,rimsky-etal-2024-steering}. Finetuning-based approaches such as ReFT \cite{wu2024reft} enable optimisation of steering directions on a dataset. Steering vectors need not be computed from labelled data; SAEs enable scalable discovery of steering vectors from unlabelled data.
In the same class of approaches, latent adversarial training \cite{casper2024defendingunforeseenfailuremodes} and circuit breakers \cite{zou2024improvingalignmentrobustnesscircuit} are representation-based control methods that increase the adversarial robustness of LLMs.

% While steering has shown some promise for model control, it is still unclear whether it can reliably outperform alternatives. Existing evaluations for supervised steering are limited \citep{pres2024reliableevaluationbehaviorsteering}.

\paragraph{Sparse autoencoders.} Sparse autoencoders (SAEs) aim to enable \textit{self-supervised} and thus \textit{scalable} decomposition of the representation space into meaningful concepts \cite{templeton2024scaling,chalnev2024improvingsteeringvectorstargeting,makelov2024sparse,obrien2024steeringlanguagemodelrefusal,gao2024scalingevaluatingsparseautoencoders}. SAEs are trained to reconstruct LLM hidden representations in a higher-dimensional latent space with a sparsity penalty, based on the assumption that concepts must be represented sparsely in order to prevent interference. The latents are then labelled with natural-language descriptions using automatic interpretability pipelines \cite[e.g.][]{eleutherai}, which can then be used to identify useful latents to steer the LM.

Recent work reports mixed results when evaluating SAEs for steering; SAEs (but also several other steering methods) suffer from a tradeoff between model control and capabilities preservation \cite{mayne2024sparseautoencodersuseddecompose,chalnev2024improvingsteeringvectorstargeting,durmus2024steering,bhalla2025unifyinginterpretabilitycontrolevaluation}. However, \citet{karvonen2024sieve} report Pareto-optimal performance when using SAEs to prevent models from producing regular expressions in code. Overall, evaluating SAEs remains an open problem because there is no ground-truth set of features to compare against.

% \section{Model abstraction}

% Building on the theory of causal abstraction for neural networks~\cite{geiger2024causal}, we conceptualize existing interpretability tools as \emph{model abstractions}.\footnote{In contrast to causal abstractions, which offer causal guarantees through interventions, model abstractions may lack causal implications and remain purely observational.} Intuitively, an abstraction simplifies a model by emphasizing a specific aspect of its functionality. Formally, we define $_{\theta}\rmA_{\gC}^{\model}$ as the abstraction for a Transformer-based language model $\model$ with parameters $\theta$. This abstraction simplifies $\model$ by isolating parts relevant to a particular concept $\gC$, where $\gC$ represents concepts in an open-vocabulary setting, described in natural language (e.g., "terms related to neural networks").\footnote{For clarity in this section, we denote the abstraction as $\rmA_{\gC}$, assuming we are working with a fixed-parameter model.}

% \paragraph{Abstracting and de-abstracting.} Abstraction is a pseudo-invertible operator on model representations. Given any example $\mathbf{x}$ sampled from a dataset $\data$, a Transformer-based language model $\model$ processes the example and generates a set of hidden representations internally, denoted as $\mathbf{h}_{i}^{l} \in \R^{1\times d}$ in the residual stream at position $i$ in layer $l$.\footnote{For simplicity, we focus only on representations in residual streams. Abstractions can apply to any component in the model's computation graph, such as self-attention outputs. We omit all superscripts and subscripts for clarity.} 
% The \textbf{abstracting} process is described as $\rmA_{\gC}(\mathbf{h}) \rightarrow \tilde{\textbf{u}} \in \R^{1\times m}$, mapping representations to an $m$-dimensional space where $m \ll d$, ensuring abstraction utility. Note that $\rmA_{\gC}$ can represent any projection matrix or a complex neural model. The dimensionality constraint enforces model simplification.\footnote{We consider only abstractions that simplify models.}
% The \textbf{de-abstracting} process is the inverse operation, described as $\rmA^{-1}_{\gC}(\tilde{\textbf{u}}) \rightarrow \tilde{\mathbf{h}} \in \R^{1 \times d}$.\footnote{Note that $\rmA_{\gC}$ is not fully invertible, even if parameterized by a single projection matrix, since $m \ll d$ implies it is not full-rank.} Intuitively, given $\tilde{\textbf{u}}$, perfect de-abstraction is unattainable due to the many-to-many mapping between a representation and a concept.

% \paragraph{Utilities.} Abstracting can be viewed as a form of \textbf{latent semantic detection}. Consider a scenario where we binarize our concept $\gC$ (e.g., binarizing the concept of "terms related to neural networks" into two groups, with the positive group containing terms related to the concept and the negative group not containing them). A faithful abstraction can map representations into two low-dimensional clusters via $\rmA_{\gC}(\mathbf{h}^{+}) \Rightarrow \tilde{\textbf{u}}^{+}$ and $\rmA_{\gC}(\mathbf{h}^{-}) \Rightarrow \tilde{\textbf{u}}^{-}$, where $\tilde{\textbf{u}}^{+}$ and $\tilde{\textbf{u}}^{-}$ are linearly separable in the low-dimensional space. In the case of a 1-dimensional subspace, this implies that $\tilde{\textbf{u}}^{+}$ and $\tilde{\textbf{u}}^{-}$ can be distinguished by a threshold. 
% De-abstracting enables \textbf{model steering through intervention}. Given any $\mathbf{h}^{*}$ originally mapped to a concept group $\tilde{\textbf{u}}^{*}$, we can create a counterfactual representation by setting $\rmA^{-1}_{\gC}(\rmA_{\gC}(\mathbf{h}^{*}) \leftarrow \tilde{\textbf{u}}^{-}) = \tilde{\mathbf{h}}^{-}$, enforcing a negative representation of the concept. We then replace the original representation with $\tilde{\mathbf{h}}^{-}$ via intervention or activation patching~\cite{} to steer the model's behavior. \TODO{Note that $\rmA_{\gC}$ and $\rmA^{-1}_{\gC}$ can have tied or untied parameters. In case the case tied parameters, it could lead to fully invertible de-abstraction. We can describe DAS, ReFT here or in the appendix to show we can have a perfect reconstruction. And for intervention, we can thus only intervene on a isoluated concept subspace without touch others. Basically, we need to define something like $_{\perp}\rmA_{\gC}$}

% \paragraph{Abstraction creation.} There is no restriction in terms of how abstractions are obtained. Most of the existing interpretability tools for LMs can be conceptualized as abstractions. \textbf{Model probing} is a type of model abstractions which trains $\rmA_{\gC}$ for latent semantic detection with an objective of mapping representations to a low-dimensional subspace where concepts are clustered~\cite{}. \textbf{Activation steering vector} is a type of model abstractions which trains $\rmA^{-1}_{\gC}$ which maps a low-dimensional concept variable to a representation, which can then be used for model steering via interventions. \textbf{Sparse Autoencoders (SAEs)} is a type of model abstractions which learns $\rmA_{\gC}$ and $\rmA^{-1}_{\gC}$ in an unsupervised manner where the training objective is reconstruction representations while enforcing latent sparisty~\cite{}.

\section{\textbf{\shortbenchmark{}}}

\shortbenchmark{} is a benchmark which takes in a list of natural language descriptions of concepts and synthetically generates the appropriate training and evaluation data for each concept using an LLM (\cref{fig:axbench}). The training and evaluation data consists of labelled pairs of instructions and responses, where the responses are either \textit{positive} examples expressing the presence of the concept of interest, or \textit{negative} examples that represent the unsteered behaviour of the model (see \cref{sec:concept-dataset} for details).

We evaluate along two axes: \textbf{concept detection} \concepttag\ and \textbf{model steering} \steeringtag{}. For the former, we measure classification performance on a held-out set of labelled data.\footnote{We focus on binarised concept detection, as a multi-class classification task over $n$ classes can also be formulated into a binarised one over $n$ features.} For the latter, we use an LLM judge to rate steered outputs on three relevant axes (see \cref{sec:model-steering}).

In this work, we use natural language concept lists for \texttt{GemmaScope} SAEs as input, and generate training and evaluation data for the following representation sites: layers 10 and 20 of instruction-tuned \texttt{Gemma-2-2B}, and layers 20 and 31 of instruction-tuned \texttt{Gemma-2-9B}. We sample 500
% (out of $\approx$16K)
concepts for each task to generate data; we term this dataset \textsc{Concept500}. These eight tasks (4 sites $\times$ 2 axes) form the core training and evaluation testbeds for \shortbenchmark{}.
Below, we describe the data generation process and evaluation setup for both axes.

% \paragraph{SAEs vs.~supervised steering.} Representation-level interpretability methods ideally have two practical uses: they should help us detect the presence of certain concepts as the LM processes text, and they should let us control the concepts that the LM expresses during generation. Different methods provides different interfaces for detection and steering (e.g. SAEs expose detection via encoder activations and steering via decoder interventions), but methods generally share the goal of enabling both operations on LM representation space. \shortbenchmark{} thus evaluates both \concepttag{} \textbf{concept detection} and \steeringtag{} \textbf{model steering}.

% However, the methods we want to benchmark differ widely in how they are trained: SAEs are trained to self-supervisedly reconstruct activations, while probing, LAT, and many other steering methods are supervisedly trained on labelled data. We must first figure out how to train the supervised methods to make them comparable to SAEs.

% The key to comparable training and evaluation is natural-language descriptions of features. After pretraining, SAE features are labelled with natural-language explanations in order to e.g.~be searchable for end-users who want to steer models using them. The feature descriptions thus parameterise the SAE. Supervised steering methods, on the other hand, are parameterised by a dataset, which contains examples of how steering should affect model outputs. To evaluate all of these methods together, we first need equivalence between an SAE feature description and the supervised steering dataset.

% Since many large lists of SAE feature descriptions have already been released publicly,\footnote{In this work, we primarily use labels from \url{https://www.neuronpedia.org/}, but in principle any SAE concept list could be used.} we devise a pipeline to synthetically generate supervised steering datasets from SAE feature descriptions using an LLM. These datasets include both positive and negative examples of the behaviour described by a feature description, generated according to the pipeline in \cref{fig:axbench}.

% \paragraph{Evaluation.} Now that we can train equivalent supervised steering methods for every concept in an SAE feature description list, we can turn to evaluation. The pipeline for generating the supervised steering dataset actually directly enables evaluation of concept detection: it contains passages of text with ground-truth labels for the presence of a concept. We generate a held-out concept detection evaluation dataset along these lines which also includes LM-generated hard negatives to test robustness (see \cref{sec:concept-dataset}).

% For steering, we sample model responses to user instructions (sampled from existing instruction-tuning datasets) after applying the steering operation for each method and concept. We ask an LM judge to evaluate the response on three axes: whether it encodes the concept we desired, whether it directly answers the instruction, and whether it is fluent and coherent (see \cref{sec:model-steering}).

% This summarises the framework underlying \shortbenchmark{}. We describe more details about implementation below and provide prompts for the LM data generation and judging pipelines in \cref{sec:prompt_templates}.

% Unlike prompting or using pretrained SAEs off-the-shelf, some interpretability methods need supervision in training (e.g.~probing).  This synthetic data generation process also allows us to construct ground-truth evaluation data for

\subsection{Synthetic concept dataset generation}
\label{sec:concept-dataset}
We construct a small training dataset $\mathcal{D}_\text{train} 
  = \{(\mathbf{x}_{c,i}^{+}, y^{+}) \}_{i=1}^{n/2}
    \cup 
    \{(\mathbf{x}_{c,i}^{-}, y^{-}) \}_{i=1}^{n/2}.$
with $n$ examples and a concept detection evaluation dataset $\mathcal{D}_{\text{concept}}$ of the same structure and harder examples, where $y^{+}$ and $y^{-}$ are binary labels indicating whether the concept $\mathbf{c}$ is present. We set $n=144$ for our main experiments. 
\footnote{Using a small training dataset ensures our methods are practical and cost-effective alternatives to SAEs.}

% The $\mathcal{D}_{\text{train}}$ contains $n$ pairs of instructions and responses and $\mathcal{D}_{\text{concept}}$ contains $m$ such pairs, generated according to the pipeline in \cref{fig:axbench}. 
We query \texttt{gpt-4o-mini-2024-07-18} to generate the data; the prompts used in this pipeline are presented in \cref{sec:generation-prompts}. Generating the data requires the following steps (note that only the evaluation set includes hard negatives):
\begin{enumerate}
    \item \textbf{Genre labelling \& seed instructions}: We consider three genres: \textit{text}, \textit{code}, and \textit{math}. We prompt the LLM to pick the genre $\vg_{\vc}$ for each concept.\footnote{Genre labelling increases input diversity. For example, inputs related to concepts such as \emph{programming code contains syntactic errors} should contain code instead of descriptions of coding errors.} We then randomly select seed instructions from our instruction pool which belong to genre $\vg_{\vc}$; see \cref{sec:instruction_pool} for dataset details. We then prompt the LLM to generate responses to these instructions.\footnote{Each example costs less than \$$0.00006$.}

    \item \textbf{Positive examples}: For each randomly sampled instruction from the instruction pool, we prompt the LLM to generate a response that incorporates the concept $\vc$. We use the generated concept-conditioned responses concatenated with their instructions (using the LM's chat template) as our positive set.

    \item \textbf{Negative examples}: To evaluate the generalisation ability of each method, we independently sample seed instructions from all genres for negatives.\footnote{We sample instructions based on overall genre distribution: 70\% from \textit{text}, 15\% from \textit{code}, and 15\% from \textit{math}.} These instructions are shared across concepts in order to save generation costs (i.e., $(\mathbf{x}_{\mathbf{c}}^{-}, y^{-})_{0}^{n/2}$ is independent of the concept $\mathbf{c}$).
    % For an randomly sampled instruction, we prompt the LLM to generate a response by avoiding anything related to the target concept.
    We sample responses from the LM we plan to steer (not the LLM) without any additional instructions. We use the paired instructions and responses as our negative set.
    
    \item \textbf{Hard negative examples} \underline{\textit{(evaluation only)}}: For each concept, we find contrasting concepts that are semantically related to our concept of interest but which should not activate the concept. We find these by (a) generating a list of phrases that are semantically relevant to our concept, (b) filtering for those which are polysemous, and (c) finding alternative senses of those words which our concept should not activate on. This results in a set of contrast concepts $\vc_{\text{contrast}}$, each of which is a specific sense of a polysemous word $\vw_{\text{contrast}}$. We then ask the LLM to generate responses incorporating $\vw_{\text{contrast}}$ into the sentence where $\vw_{\text{contrast}}$ should express the sense related to $\vc_{\text{contrast}}$. We use the contrastive responses paired with their instructions as our hard negative set.
    % \item \textbf{Content generation}: We now generate our dataset, in the format of paragraph-sized chunks of input text along with a continuation. The input text, which is always conditioned on a genre, may be totally random or rewritten to introduce a particular concept; the same applies to the continuation. This dataset consists of a few types of text:
    % \begin{enumerate}
    %     \item \textbf{Null content}: arbitrary text generated only by conditioning on a genre $G_i$ chosen uniformly at random, with a continuation sampled from the \textbf{steering model}.
    %     \item \textbf{Hard negatives}: null content rewritten to represent one of the contrast concepts $C_i$ chosen uniformly at random, with a continuation sampled the same as for null content. We hold out some of the $C_i$ for evaluation.
    %     \item \textbf{Concept content}: null content rewritten to represent the concept of interest, with a continuation sampled from the \textbf{data-generation model} encoding the other concept it was paired with (which is not a hard negative).
    % \end{enumerate}
\end{enumerate}

The negative training set is not applicable to all methods (e.g.\ full finetuning only needs the positive training set for model steering).

% We then describe evaluating methods in details.

% \paragraph{Interpretability with 1-d linear subspace.} LMs encode information linearly with representations, making linear subspaces surprisingly powerful in terms of decoding latent information or model steering~\cite{}. In this paper, we focus on evaluating interpretability methods that target 1-d linear subspaces of representations among a fixed model component (e.g., the residual stream output of layer 20 of \texttt{Gemma-2-2b} model). Nevertheless, \shortbenchmark{} is a generic framework that is compatible with methods that target other types of subspaces.
% Building on the theory of causal abstraction for neural networks~\cite{geiger2024causal}, we conceptualize existing interpretability methods as model abstractions. Intuitively, these abstractions offer simplified proxies by emphasizing a specific aspect of model's functionality. Specifically, we focus on two types of abstractions: concept detection and model steering.

\subsection{\concepttag{} Concept detection}\label{sec:concept-detection}
A popular LM interpretability method is to train \textit{probes} \cite{conneau-etal-2018-cram, hewitt-manning-2019-structural, belinkov-etal-2017-neural} that measure to what extent LM representations encode properties of interest, e.g.~linguistic features. In recent years, the goal of concept detection has broadened to the open-vocabulary setting, with unsupervised methods becoming more common \cite{bills2023language,huben2024sparse,choi2024automatic}.

% A common automatic evaluation of feature interpretability \cite{bills2023language,huben2024sparse,choi2024automatic} is to use an LM to propose a label for the feature using dataset exemplars, and then separately ask the LM to predict feature activations conditioned on the label; the correlation between predicted and true presence of the concept ideally indicates how interpretable the feature is.

\paragraph{Task description.}
% Since the debut of LM, the examination of human-interpretable knowledge (e.g. syntax and semantics) from latent representations offers intuitive explanations of the internals of the model~\cite{conneau-etal-2018-cram, hewitt-manning-2019-structural}. The task is to classify hidden representations generated in the model given a set of known class labels (e.g., predicting part-of-speech from internal representations~\cite{belinkov-etal-2017-neural}).
% We first unify these preceding evaluations into the notion of \textbf{concept detection}. 
Formally, given a Transformer-based LM with a hidden dimension size of $d$, we define a concept classifier as a parameterized function $\Psi_{\text{Detect}}$ that maps a model representation $h \in \R^d$ into a \emph{binary} label $\hat{y}$ indicating the relative presence of a concept:
\begin{equation}
    \Psi_{\text{Detect}}(h) = \hat{y} \in \R^1 \label{eqn:concept-detection}
\end{equation}
where $\Psi$ is any function, e.g.~a neural network.

% Supervised probes are one of the most widely used tools for concept detection when labeled corpus is available. 

% Recent works in SAEs offer an alternative where concept subspaces are learned over unlabeled text corpus, and learned subspaces can be used as linear probes for detecting concepts~\cite{}. Given a Transformer-based LM with a hidden dimension size of $d$, these methods offer a parameterzied function $\Psi_{\text{Detect}}$ that maps model representations $\mathbf{h}$ from a $d$-dimensional space into a binarized subspace $\mathbf{c}$ to represent whether information about a concept can be decoded or not:
% \begin{equation}
%     \Psi_{\text{Detect}}(\mathbf{h}) = \mathbf{h} \times \textbf{u}^\top_{\text{Detect}} \label{eqn:concept-detection}
% \end{equation}
% where the output represents a linear subspace of the model's original representation and is the predicted binarzied variable represents indicating the presence of a concept. Note that additional activation function is needed to turn the output into a classification label.

\paragraph{Evaluation dataset.} To evaluate a concept classifier, we
%  first collect internal representations from LM when taking generated sentences as input, and then 
measure how accurately it can predict ground-truth labals on the labelled evaluation set from $\mathcal{D}_{\text{concept}}$ (see \cref{sec:concept-dataset}).

\paragraph{Evaluation metrics.} Since our labels are at the sequence-level, we need to aggregate token-label scores from $\Psi$ to evaluate it. Given a sequence of token representations \(\mathbf{h}^{l} = [\,h_1^l, h_2^l, \ldots, h_n^l\,]\) with $n$ tokens at layer $l \in [1, m]$, we max-pool the detection scores to get a sequence-level prediction:
\begin{equation}\label{eq:maxpool-detect}
    \hat{y}_{\text{Detect}} = \max\bigl(\Psi_{\text{Detect}}(\mathbf{h}^l)\bigr)
\end{equation}
We then normalize $\hat{y}_{\text{Detect}}$ between $[0, 1]$ by min-max normalisation over the evaluation dataset for each concept. The predicted score represents how strongly a concept is present in a sequence, which we can compare to the true label.

\subsection{\steeringtag{} Model steering}\label{sec:model-steering}
% Editing representations by placing interventions in the LM forward computation graph has been shown to be effective in steering the LM generation~\cite[][\textit{inter alia}]{subramani-etal-2022-extracting, zou2023representationengineeringtopdownapproach, wu2024reft}.
Representation-based steering has emerged as a potential alternative to existing model-control methods (e.g.~finetuning and prompting) and a practical application of various interpretability methods (see \cref{sec:related}). Unlike concept detection, model steering assesses \textit{causal} efficacy in controlling model behaviour.
Previous evaluation benchmarks for steering are not general-purpose; they either rely on a limited set of tasks~\cite{zou2023representationengineeringtopdownapproach, makelov2024sparse,bhalla2025unifyinginterpretabilitycontrolevaluation} or condition generation on a fixed prefix~\cite{chalnev2024improvingsteeringvectorstargeting}.
To the best of our knowledge, we are the first to evaluate model steering methods in the open-vocabulary setting at scale. 

% Interventions can be constructed in various ways such as supervised training to elicit targeted behaviors~\cite{}, training-free methods with vector-space arithmetic~\cite{} or SAEs where concept directions are learned over unlabeled text corpus~\cite{}. Given a Transformer-based LM, these methods provides a parameterized function $\Phi_{\text{Steer}}$ that modifies representations in-place as:
% \begin{equation}
%     \Phi_{\text{Steer}}(\mathbf{h}) = \mathbf{h} + \alpha \cdot \textbf{u}_{\text{Steer}}
% \end{equation}
% where we overwrite model's representations $\mathbf{h}$ in-place via an activation addition intervention by injecting $\textbf{u}_{\Phi}$ with a gating factor $\alpha$. Note that $\alpha$ is usually separately chosen as a hyperparameter at inference-time, and is kept as a constant for $\mathbf{h}$. Note that $\textbf{u}_{\text{Steer}}$ and $\textbf{u}_{\text{Detect}}$ can have tied weights or not. For SAEs, they have untied weights leaving unshared subspaces between concept detection and model steering for the same concept.

\paragraph{Task description.} Given a prompt $\mathbf{x}$, the model's original generation can be written as $\hat{\mathbf{y}} = \textsc{LM}(\mathbf{x})$. We produce the model's counterfactual generation conditioned on the concept-based intervention $\Phi_{\text{Steer}}(\mathbf{h})$:
\begin{equation}
    \hat{\mathbf{y}}_{\text{Steer}} = \textsc{LM}\bigl(\mathbf{x}, \mathbf{h} \leftarrow \Phi_{\text{Steer}}(\mathbf{h})\bigr)
\end{equation}
where $\mathbf{h} \leftarrow \Phi_{\text{Steer}}(\mathbf{h})$ is an in-place representation modification. We use the open-source intervention library \texttt{pyvene} to perform such interventions on PyTorch implementations of models~\cite{wu-etal-2024-pyvene}.

\paragraph{Evaluation dataset.} 
% Inspired by Golden Gate Claude \cite{templeton2024scaling}, 
We evaluate these steering methods
in the instruction-following setting, where
% we prompt LM with instructions while using these methods to steer responses conditioned on a target concept.
we sample instructions from \texttt{Alpaca-Eval} \cite{alpaca_eval} and prompt the LM to generate a response while intervening on its forward pass in-place using one of the steering methods.
% \footnote{Existing finetuning methods including full finetuning and LoRA can all be formulated as representation interventions~\cite{hetowards} For example, full fine-tuning learns a weight change for every parameter which is equivalent to applying an intervention for each representation that modifies the representations with the weight change.}

\paragraph{Evaluation metrics.} For the intervened model generation, we evaluate \(\hat{\mathbf{y}}_{\text{Steer}}\) based on the \emph{harmonic mean} of the following scores, each of which the LLM rates using a discrete score of 0, 1, or 2:
\begin{enumerate}
    \item \textbf{Concept score} represents how well the concept is incorporated into the response.
    \item \textbf{Instruct score} represents how well the response is related to the instruction.
    \item \textbf{Fluency score} represents how fluent the response is.
\end{enumerate}
Since we compute the harmonic mean, the overall score also ranges from 0 to 2, but heavily penalises poor performance on any of these three subscores. For each concept, we randomly sample 10 instructions from \texttt{Alpaca-Eval} and sample continuations for each steering factor (see discussion on steering factor in \cref{sec:result-model-steering}).
To ensure a fair comparison, we partition our instructions into two equally sized sets, selecting the best factor from one set and evaluating it on the holdout set.
Our judge prompts with further discussion can be found in \cref{sec:evaluation-prompts}.
% We also validate our LLM-generated overall score with human evaluation in \cref{sec:human_eval}.

\section{Methods}\label{sec:methods}
In this section, we describe the interpretability methods we evaluate along with our baseline prompting and finetuning methods. For each method, we label which axes it is evaluated on using \concepttag{} and \steeringtag{}. All of our interpretability methods except SAEs are SDLs that learn rank-1 subspaces for targeted concepts.

\paragraph{Notation.} Given a LM, the hidden representations of dimensionality $d$ for a token sequence of length $n$ in layer $l$ of the LM are represented as \(\mathbf{h}^{l} = [\,h_1^l, h_2^l, \ldots, h_n^l\,] \in \R^{n \times d}\). The set of representations concatenated from all of the training set inputs is denoted as $\mathbf{H} \in \R^{s \times d}$, where $s = \sum_{\mathbf{h}}\lvert\mathbf{h}\rvert$. We denote $\mathbf{H}^{+}$ as the subset of $\mathbf{H}$ including only positive training inputs and $\mathbf{H}^{-}$ for the negative inputs (see \cref{sec:concept-dataset} for training dataset details). Finally, per-method projection vectors $\mathbf{w}$ and representations $h_i$ are the same shape: $\R^{d \times 1}$.

\paragraph{\concepttag{}\steeringtag{} Difference-in-means (DiffMean).} DiffMean uses the difference between averaged representations from two classes of inputs as a steering vector~\cite{marks2024geometrytruthemergentlinear}. 
% First, let $\mathbf{c}^+ = \mathrm{concat}(\mathbf{H}^+)$ and $\mathbf{c}^- = \mathrm{concat}(\mathbf{H}^-)$. Then, 
The projection vector \(\mathbf{w}_{\text{DiffMean}}\) is defined as:
\begin{equation}
\mathbf{w}_{\text{DiffMean}} = \underbrace{\frac{1}{|\mathbf{H}^+|}\sum_{h^+_i \in \mathbf{H}^+}{h^+_i}}_{\text{mean of positives}} - \underbrace{\frac{1}{|\mathbf{H}^-|}\sum_{h^-_i \in \mathbf{H}^-}{h^-_i}}_{\text{mean of negatives}}
\end{equation}
We compute detection scores with the dot product, i.e.~$\Psi_{\text{Detect}}^{\text{DiffMean}}(h_i) = h_i\cdot\mathbf{w}_{\text{DiffMean}}$.\footnote{Following \citet{gao2024scalingevaluatingsparseautoencoders}, we normalize $\mathbf{w}_{\text{DiffMean}}$ to have unit norm. We apply the same normalization to the learned weights of PCA, LAT, Probe, and ReFT‑r1.} Our steering operation is simple activation addition: $\Phi_{\text{Steer}}^{\text{DiffMean}}(h_i) = h_i + \alpha \mathbf{w}_{\text{DiffMean}}$ where $\alpha$ is the steering magnitude, which depends on the steering factor and is optimized as a hyperparameter, as described in \cref{sec:result-model-steering}.
% Intuitively, this difference highlights how concept-present activations differ from concept-absent ones in the model’s hidden space. For an evaluation sentence \(\mathbf{x}\), the LM generates hidden representations \(\mathbf{h}\) at a layer. We then get the token-level concept score predictions as:
% \begin{equation} \label{diffmean-eq1}
% \Psi_{\text{Detect}}^{\text{DiffMean}}(\mathbf{h}) = \mathbf{h}\times\mathbf{w}_{\text{DiffMean}}
% \end{equation}

% We then use max-pooling as described in \cref{sec:concept-detection} to get sequence-level predictions. For model steering, our intervention function is formulated as:
% \[
% \Phi_{\text{Steer}}^{\text{DiffMean}}(\mathbf{h}) = \mathbf{h} + \alpha \cdot \mathbf{w}_{\text{DiffMean}}
% \]
% where $\alpha$ is a hyperparameter representing the steering factor. The intervention edits the representations in place during the forward pass as described in \cref{sec:model-steering}.

\paragraph{\concepttag{}\steeringtag{} Principle component analysis (PCA).} For PCA, we use the first principal component of the positive set of hidden representations as the projection vector.\footnote{We found no significant difference between using only the positive set vs.~the entire set of hidden representations for both PCA and LAT; see \cref{sec:abaltions} for ablations.} We first subtract the mean $\overline{\mathbf{H}^+}$ from each \(h^+\), gathering the centered vectors into a matrix 
\(\mathcal{H}\in \mathbb{R}^{\vert\mathbf{H}^{+}\rvert\times d}\). We then find the top principal component 
\(\mathbf{w}_{\mathrm{PCA}} \in \mathbb{R}^{d \times 1}\) of \(\mathcal{H}\), i.e.~the unit vector that captures the largest variance along its direction, using \texttt{sklearn.decomposition.PCA} \cite{sklearn}. We follow the same detection and steering setup as DiffMean.

\paragraph{\concepttag{}\steeringtag{} Linear artificial tomography (LAT).} 
LAT searches for a single latent direction that can separate positive examples by 
learning from their pairwise activation differences~\cite{zou2023representationengineeringtopdownapproach}. Concretely, we create pairwise activation differences \(\delta\) by randomly partitioning \(\mathbf{H}\)
into pairs \((h_i, h_j)\) (with \(i \neq j\)) and computing
$
\delta = \frac{h_i-h_j}{\lVert h_i - h_j \rVert}
$,
where the denominator ensures each difference is unit-normalized. We gather all these pairwise differences into a matrix \(\Delta\in\mathbb{R}^{\frac{|\mathbf{H}|}{2}\times d}\). We then perform PCA (using \texttt{sklearn}) on \(\Delta\); then \(\mathbf{w}_{\text{LAT}} \in \mathbb{R}^{d \times 1}\) is the top principal component of \(\Delta\).  
% This \(\mathbf{w}_{\text{LAT}}\) captures the dominant direction of variation among the activation \textit{differences}.
We follow the same detection and steering setup as DiffMean.

\paragraph{\concepttag{}\steeringtag{} Linear probe (Probe).} The linear probe learns to classify tokens as concept-relevant by projecting representations $h_i$ onto a learned direction $\mathbf{w}_{\text{Probe}} \in \mathbb{R}^{d \times 1}$ just as in DiffMean. To convert this into a probability, we apply the sigmoid activation, and then minimise binary cross-entropy loss with the true labels:
\begin{equation}
\min_{\mathbf{w}_{\text{Probe}}} \left\{ \frac{1}{\lvert \mathbf{h} \rvert}\sum_{h_i \in \mathbf{h}}\bigl(\mathcal{L}_{\text{BCE}}(y, \,\mathrm{Sigmoid}(h_i \cdot \mathbf{w}_{\text{Probe}}))) \right\}
\end{equation}
where $y$ is the token-level class label indicating whether this token belongs to a positive or negative example. The detection and steering setup is then identical to DiffMean.
% where we also experiment with L1 or L2 regularizations on the learned weights.
% Excluding regularization loss on TopK activations allow selected latents to be highly activated when concepts is presented in the inputs. To summary, the objective contains a binary cross-entropy loss with regularization terms to ensure effective and sparse activations.
% To evaluate, we then get the token-level concept score predictions as:
% \[
% \Psi_{\text{Detect}}^{\text{Linear}}(\mathbf{h}) = \text{ReLU}(\mathbf{h} \times \mathbf{w}_{\text{Probe}})
% \]
% We then use max-pooling as described in \cref{sec:concept-detection} to get sequence-level predictions. For evaluating model steering with learned $\mathbf{w}_{\mathrm{Probe}}$, we follow the same setup as DiffMean.


\paragraph{\concepttag{}\steeringtag{} Supervised steering vector (SSV).} The supervised steering vector method directly learns an intervention that maximises the language-modelling probability of the positive responses. For a sequence of token representations \(\mathbf{h}\), we apply an intervention to each token representation:
\begin{equation}
\Phi^{\text{SSV}}(h_i) = h_i + \mathbf{w}_{\text{SSV}}
\end{equation}
where \(\mathbf{w}_{\text{SSV}} \in \mathbb{R}^{d \times 1}\) is a learned vector. As described in \cref{sec:model-steering}, we backpropagate gradients by training with the language modeling loss, similar to supervised fine-tuning (SFT):
\begin{equation}
\min_{\mathbf{w}_{\text{SSV}}}\left\{\sum_{t=1}^n \log P_{\mathrm{LM}}\bigl(y_t \mid y_{<t}, \mathbf{x}; \mathbf{h} \leftarrow \Phi^{\text{SSV}}(\mathbf{h})\bigr)\right\}
\end{equation}
where \(y_i\) is the \(i\)-th output token, \(y_{<i}\) are the preceding tokens, and \(\mathbf{x}\) is the prompt. For evaluating concept detection and model steering SSV follows the same setup as DiffMean. We apply $\mathrm{ReLU}$ to get the detection scores.

\paragraph{\concepttag{}\steeringtag{} Rank-1 representation finetuning (ReFT-r1).} We introduce a novel method based on ReFT~\cite{wu2024reft} which jointly learns concept detection and steering on supervised data by combining the training objectives of linear probing and supervised steering.

We compute latents for concept detection as:
\begin{equation}
\Psi_{\text{Detect}}^{\text{ReFT-r1}}(h_i) = \mathrm{ReLU}(h_i \cdot \mathbf{w}_{\text{ReFT-r1}})
\end{equation}
During training we perform a representation-level intervention on each $h_i$ based on the latents of the sequence $\mathbf{h}$:
\begin{equation}
\Phi^{\text{ReFT-r1}}(h_i) = h_i + \left(\frac{1}{k}{\left\lVert \topk(\Psi_{\text{Detect}}^{\text{ReFT-r1}}(\mathbf{h})) \right\rVert_1}\right) \mathbf{w}_{\text{ReFT-r1}}
\end{equation}
% \begin{equation}
% \Phi^{\text{ReFT-r1}}(h_i) = h + \frac{1}{|\mathbf{h}|}\sum_{h_i \in \mathbf{h}}{\topk_{h_i \in \mathbf{h}}(\Psi_{\text{Detect}}^{\text{ReFT-r1}}(h_i))}
% \end{equation}
where \(\mathbf{w}_{\text{ReFT-r1}} \in \mathbb{R}^{d \times 1}\) is a learned vector. Finally, the training objective combines language modelling loss subject to this intervention, along with L1 regularisation on the non-top-$k$ latents:
\begin{equation}
\min_{\mathbf{w}_{\text{ReFT-r1}}}\left\{
-\sum_{t=1}^n{\log P_{\mathrm{LM}}^{\Phi^{\text{ReFT-r1}}}(y_t \mid y_{<t}, \mathbf{x})}
+ \lambda\sum_{\mathclap{a_i \notin \topk\left(\Psi(\mathbf{h})\right)}}{\lVert a_i \rVert_1}
\right\}
\end{equation}
Detection and steering is identical to DiffMean.

% For a sequence of token representations \(\mathbf{h}\), ReFT-r1 modifies the activation function of the linear probe by getting the mean of top-\(k\) activations over tokens in two steps:
% \begin{equation} \label{reftr1-eq1}
% \begin{split}
% % \Phi_{\text{Linear}}(\mathbf{h}_i)
% \textbf{a} & = \text{ReLU}(\mathbf{h} \times \mathbf{w}_{\text{ReFT-r1}}) \\
% \overline{a} & = \textsc{Mean}\bigl(\textsc{TopK}(\textbf{a})\bigr)
% \end{split}
% \end{equation}
% where $\textbf{a}$ is the raw latent activations and $\overline{a}$ is the mean of top-\(k\) activations.
% Then, ReFT-r1 uses $\mathbf{w}_{\text{ReFT-r1}}$ as the steering vector:
% \begin{equation} \label{reftr1-eq2}
% \begin{split}
% \Phi_{\text{ReFT-r1}}(\mathbf{h}) & = \mathbf{h} + \overline{a} \cdot \mathbf{w}_{\text{ReFT-r1}}
% \end{split}
% \end{equation}
% where $\overline{a}$ is used as the steering factor during training. This ensures that only the most salient features in the sequence contribute to the steering effect. The training objective combines language modeling and sparsity:

% \begin{align*}
% \mathcal{L} = & - \sum_{i=1}^n \log P_{\text{LM}}\bigl(
%     y_t \mid y_{<t}, \mathbf{x}; \mathbf{h} \leftarrow 
%     \Phi_{\text{ReFT-r1}}(\mathbf{h})\bigr) \\
%     & + \lambda_{\text{ReFT-r1}} 
%     \sum_{\mathclap{a_i \notin \text{TopK}(\mathbf{a})}} \|a_i\|_1
% \end{align*}
% Similar to SSV, ReFT-r1 push the activation valyes down for non-TopK activations. We also experiment with L1 or L2 regularizations on the learned weights. For evaluating concept detection and model steering with $\mathbf{w}_{\text{ReFT-r1}}$, we follow the same setup as for sparse linear probe.

\paragraph{\concepttag{}\steeringtag{} Sparse autoencoders (SAE).} Sparse autoencoders are a self-supervised dictionary learning method (see \cref{sec:related}). We use pretrained SAEs from \texttt{GemmaScope}, which are the best available SAEs for \texttt{Gemma}-family LLMs~\cite{lieberum-etal-2024-gemma}.\footnote{\texttt{GemmaScope} releases a set of SAEs for \texttt{Gemma-2-27B}, but the concept list is not publicly released, which makes the SAEs for \texttt{Gemma-2-9B} the largest ones available for evaluations.}
% It uses JumpReLU SAEs~\cite{rajamanoharan2024jumping} and often shows a lower reconstruction error than other SAEs such as TopK SAEs~\cite{gao2024scalingevaluatingsparseautoencoders}, Gated SAEs~\cite{rajamanoharan2024improving} or vanilla ReLU SAE~\cite{huben2024sparse}.
The SAEs we used are trained to learn two dictionary matrices, $\{\mathbf{W}_{\mathrm{enc}}, \mathbf{W}_{\mathrm{dec}}\} \in \R^{d\times z}$ where $z$ is the number of latents. For our evaluating concept $\mathbf{c}$, we use $\{\mathbf{w}_{\mathrm{enc}}, \mathbf{w}_{\mathrm{dec}}\} \in \R^{d \times 1}$ as the detection and steering representations, respectively:
\[
\Psi_{\text{Detect}}^{\text{SAE}}(h_i) = \sigma\left(h_i \cdot \mathbf{w}_{ 
 \textrm{enc}} + b_{\textrm{enc}} \right)
\]
where $\sigma$ is an activation function (in our case, JumpReLU) and $b_{\text{enc}}$ is a learned bias.\footnote{Note that this parameterisation cannot apply to TopK \cite{gao2024scalingevaluatingsparseautoencoders} and BatchTopK SAEs, which require loading in the entire encoder matrix to compute latents.}
For steering, we use activation addition as DiffMean.
% $$
% \Phi_{\text{Steer}}^{\text{SAE}}(h_i) = h_i + \alpha\mathbf{w}_{\textrm{dec}}
% $$
Note that \citet{templeton2024scaling} use activation clamping; we report ablations in \cref{sec:abaltions}.

\paragraph{\concepttag{}\steeringtag{} SAEs with AUROC selection (SAE-A).} Given that other methods have access to a training dataset, to enable fair comparison we attempt to use our training dataset for SAE feature selection. For each feature, we compute its max-pooled activations per \cref{eq:maxpool-detect} over each training example, compute AUROC over the dataset given true labels, and select the highest-scoring feature by this metric.

% Note that for concept detection, we rely on learned $\textbf{w}_{\text{enc}}$. For model steering, we adapt the existing approach as in the Golden Gate Bridge \texttt{Claude}~\cite{templeton2024scaling} example:

% \begin{align}\label{eqn:sae-steer}
% \mathbf{a}^{\text{clamp}} &= \sigma\bigl(
%     \mathbf{h} \times \textbf{W}_{\text{enc}} 
%     + \textbf{b}_{\text{enc}} \bigr)\,\odot\, 
%     (\mathbf{1} - \mathbf{e}_{\mathbf{c}}) \notag \\
% &\quad + \alpha \cdot \mathbf{e}_{\mathbf{c}} \\
% \Phi_{\text{Steer}}^{\text{SAE}}(\mathbf{h}) &= \underbrace{\mathbf{h} - \hat{\mathbf{h}}}_{\text{reconst.}} 
%     +\,\mathbf{a}^{\text{clamp}} \cdot \mathbf{W}_{\text{dec}}
% \end{align}

% where $\mathbf{e}_{\mathbf{c}}$ is an one-hot vector representing the corresponding concept, and $\mathbf{a}^{\text{clamp}}$ modifies the latent activation by clamping the activation corresponding to the concept to a higher value to amplify the contribution of the specific concept in the output representation. In this paper, we focus on a newer variant of SAEs with the JumpReLU activation function~\cite{rajamanoharan2024jumping}. Details about JumpReLU and how it is trained can be found in its original paper.

% \paragraph{\steeringtag\ Low-rank adaptation (LoRA).} LoRA~\cite{hu2022lora} is a parameter-efficient approach that injects rank-$r$ trainable matrices into select weight layers, keeping the original weights~\(\theta_{\text{LM}}\) frozen.  
% We then train these low-rank parameters on the supervised steering objective (similar to \(\Phi_{\text{SSV}}\)) to produce concept-steered outputs. As a result, the negative training set is discarded.
% We use LoRA as one of our baselines for model steering.


% \paragraph{\steeringtag\ Low-rank representation finetuning (LoReFT).} LoReFT~\cite{wu2024reft} is a variant of ReFT that targets orthogonal linear subspaces through learning interventions composed of rank$r$ trainable matrices. Unlike LoRA which learns a weight transformation, LoReFT learns representation edits to adapt the LM to the training task. Similarly to LoRA, we train LoReFT on the supervised steering objective. The negative training set is discarded.
% We use LoReFT as one of our baselines for model steering.

% \paragraph{\steeringtag\ Full finetuning (FFT).} Full finetuning (FFT) updates \emph{all} model parameters \(\theta_{\text{LM}}\) using the same supervised steering objective as LoRA or LoReFT. The negative training set is discarded.
% We use FFT to establish a strong baseline for model steering, as it trades compute resource for performance.

\paragraph{\concepttag{} Bag-of-Words (BoW).} For the BoW baseline, we first construct a featurizer that tokenizes text by whitespace and counts word frequencies. The vocabulary for this featurizer is derived from the training dataset. We then train a logistic regression classifier to predict class probabilities, framing the task as binary classification. To mitigate overfitting, we incorporate a regularization term. This BoW approach leverages statistical biases inherent in LLM-generated data.

\paragraph{\concepttag{} Gradient-based baselines.} We test two gradient-based attribution methods, which are applicable only to concept detection: Input $\times$ gradients (\textbf{I$\times$G}) and Integrated gradients (\textbf{IG}; \citealp{integratedgradient}). For both, we train a classification head on the hidden representations of some layer and apply the methods to produce token-level attribution scores $\Psi_{\text{Detect}}(h_i)$. Implementation details are in \cref{sec:gradient-baselines}.

\paragraph{\concepttag{}\steeringtag{} Prompting baseline.} For concept detection, we use the same LLM judge as described in \cref{sec:model-steering} to rate the presence of a concept on a scale of 0 to 2.
For model steering, we use an LLM to \emph{engineer} a prompt 
% with few-shot in-context demonstrations 
given a concept, which we use to steer our local model by prepending it to the actual instruction. We provide prompt templates and examples in \cref{sec:prompt_templates} and \cref{sec:model_generations}.
% In the current paper, prompting baseline for concept detection is not included.
% \TODO{\textbf{The prompt baseline is not evaluated for concept detection.} This is not true anymore, we will add in prompt baseline, but prompt baseline will be have AUC since it is a per-sequence class label (0, 1). We should consider to report the classic \texttt{Recall@N} at the end in the main text? }

\paragraph{\steeringtag{} Finetuning baselines.} We test full-parameter supervised finetuning (\textbf{SFT}) and two parameter-efficient finetuning methods: Low-rank adaptation (\textbf{LoRA}; \citealp{hu2022lora}) and low-rank representation finetuning (\textbf{LoReFT}; \citealp{wu2024reft}). In all cases, we finetune to minimise the language-modelling loss on the responses in the positive split of the dataset; the negative training split is discarded. We then use the finetuned models as baselines for steering.

For all of our SDLs except SSV, we constrain any learned subspace to have a unit norm, following the same setup as SAEs. With a unit-norm constraint, we find that SSV is hard to use for steering models. For prompting and finetuning baselines, we randomly score one generation on the testing instruction set (since the factor is not a parameter for those methods), resulting in the same number of observations for those methods.

% \paragraph{Supervised probes.} 

% \paragraph{Unsupervised interventions.} 

% \paragraph{Supervised interventions.} 

% \paragraph{Sparse autoencoders (SAEs).} 

% \paragraph{Rank-1 representation finetuning (ReFT).} 

% \subsection{Datasets}

% \paragraph{Random seed sentences.} For \textit{text}, we use the \texttt{ag-news} dataset. For \textit{code}, we use the \texttt{Rosetta} code dataset. For \textit{math}, we use the \texttt{Omni-Math} datasets. For each dataset, we subsample 1,000 examples to create our final sentence pool. 

\subsection{Evaluation}\label{sec:evaluation}

\paragraph{Datasets.} We synthetically generate training and validation datasets (see \cref{sec:concept-dataset}) for 500 concepts, which we release as \textsc{Concept500}. The concepts are sampled from the Neuronpedia SAE concept list for \texttt{GemmaScope} as described in \cref{sec:sae-concept-list}. For each concept, we include 144 examples for training and $\approx$72 samples for evaluating concept detection.\footnote{This varies based on valid hard negatives.} In this paper, we train and evaluate all methods, and report results on \textsc{Concept500}. For SFT, we only train and evaluate on the first 20 concepts due to limited resources.

For evaluating steering, we use the instructions from the \texttt{Alpaca-Eval} dataset~\cite{alpaca_eval}. For each concept, we sample 10 instructions. We generate up to 128 tokens for each instruction over 14 steering factors. We split the instructions into two equal sets -- one for selecting the best factor and the other for evaluation.

We additionally release training and evaluation datasets for all 16K concepts in \texttt{GemmaScope} as the \textsc{Concept16K} dataset suite. We train and release SAE-scale dictionaries on this dataset only for the best-performing methods found on \textsc{Concept500}. See \cref{sec:data_stats} for dataset statistics and \cref{sec:sdl-16k} for further experiments on \textsc{Concept16K}.

\paragraph{Models.} Our evaluations rely on access to and control over the LLM's representations. To reduce training cost, we prefer to use models for which pretrained SAEs are available. We thus evaluate our methods on two open models, \texttt{Gemma-2-2B-it} and \texttt{Gemma-2-9B-it} (henceforth referred to without the \texttt{-it} suffix), from the \texttt{Gemma}-family, with corresponding SAEs released as \texttt{GemmaScope}. We evaluate our methods with model representations from the residual streams of layers 10 and 20 for \texttt{Gemma-2-2B} and layers 20 and 31 for \texttt{Gemma-2-9B}. We use SAEs from \texttt{GemmaScope} that are trained for these layers.\footnote{For \texttt{Gemma-2-2B}, we follow the common practice to use SAEs for the base LM, as SAEs are not available for the instruction-tuned model at the time of publication~\cite{lieberum-etal-2024-gemma}.} To ensure a fair comparison, we perform separate hyperparameter-tuning for each method. Details can be found in \cref{sec:hyperparameters}.

 
% \paragraph{Hyperparameters.} To ensure a fair comparison, we perform separate hyperparameter-tuning for each method that requires training. To select the best hyperparameters, we use a held-out dataset for development, \textsc{Concept10}, which  covers 10 concepts each on our tasks. Details can be found in \cref{sec:hyperparameters}. We minimise the loss with \texttt{AdamW} with a linear scheduler for all methods that require training. 

% \paragraph{Steering factors.} For any method that learns a rank-1 subspace, the subspace can be treated as a \emph{concept} direction. A common practice is to steer a model in a direction of different magnitudes, varying the steering effect~\cite{subramani-etal-2022-extracting}. In total, we evaluate with 14 steering factors (listed in \cref{sec:hyperparameters}). For each steering factor, we calculate the final steering magnitude by multiplying the factor with maximal latent activation by projecting hidden representations to this rank-1 subspace for all training examples we created for sueprvised training. This is identical to getting the unnormalized token-level concept score, such as in \cref{diffmean-eq1}.
% For SAEs, we fetch the maximal latent activations from publicly released data.\footnote{We use \url{https://www.neuronpedia.org/} to fetch the maximum latent activations.} For SAEs, we also consider the reconstruction error term when adding the steering vector as in \cref{eqn:sae-steer}. 
% To ensure statistical equivalence, we treat the sampling process for non-rank-1 dictionary learning methods (e.g., LoRA or LoReFT) as if they had steering factors, giving them an equal chance of being favoured regardless of whether a steering factor is needed. For example, we would sample $14\times 5 = 70$ times to get the averaged scores. And for win-rate, we would sample $70$ times on a separate set for each concept to get the best generation. This is crucial as best-of-n is a common strategy to get the best generation~\cite{stiennon2020learning}. This ensures that we treat each method as fairly as possible.

\section{Results}

\subsection{\concepttag{} Concept detection}

% \begin{figure*}
%     \centering
%     \includegraphics[width=\linewidth]{figures/concept500_roc.pdf}
%     \caption{\concepttag{} ROC curves for each method on every concept in \textsc{Concept500} when evaluating layer 10 of Gemma-2-2B.}
%     \label{fig:roc-detailed}
% \end{figure*}

For concept detection, \textsc{Concept500} consists of passages of text with ground-truth labels for each concept. Each method provides us with token-level concept scores obtained from the representation of that token at a particular layer. To compute a passage-level score, we take the mean of the token-level concept scores. See \cref{sec:concept_detection_examples} for a visualization of token-level concept scores.

\begin{table}[!t]
    \small
    \centering
\begin{tabular}{lccccc}
\toprule
\multirow[t]{2}{*}{\textbf{Method}} & \multicolumn{2}{c}{\textbf{Gemma-2-2B}}  & \multicolumn{2}{c}{\textbf{Gemma-2-9B}} & \multirow[t]{2}{*}{\textbf{Avg.}} \\
& \multicolumn{1}{c}{L10} & \multicolumn{1}{c}{L20} & \multicolumn{1}{c}{L20} & \multicolumn{1}{c}{L31} \\
% identifier & 2b, l10, concept500 & 2b, l20, concept500 & 9b, l20, concept500 & 9b, l31, concept500 \\
% method &  &  &  &  \\
\midrule
DiffMean & \underline{0.948} & 0.946 & 0.955 & 0.921 & \textbf{0.942} \\
Probe & 0.940 & 0.946 & 0.933 & \textbf{0.942} & \underline{0.940} \\
ReFT-r1 & \textbf{0.952} & \textbf{0.965} & \textbf{0.966} & 0.869 & \underline{0.938} \\
\rowcolor{gray!20} Prompt & 0.910 & 0.921 & 0.940 & 0.943 & 0.929 \\
SAE-A & 0.924 & 0.911 & 0.924 & 0.907 & 0.917 \\
\rowcolor{gray!20} BoW & 0.909 & 0.931 & 0.904 & 0.912 & 0.914 \\
SSV & 0.934 & 0.950 & 0.910 & 0.854 & 0.912 \\
LAT & 0.742 & 0.809 & 0.572 & 0.725 & 0.712 \\
SAE & 0.735 & 0.755 & 0.631 & 0.659 & 0.695 \\
PCA & 0.714 & 0.712 & 0.559 & 0.622 & 0.652 \\
IG & 0.440 & 0.375 & 0.508 & 0.383 & 0.426 \\
IxG & 0.243 & 0.217 & 0.193 & 0.330 & 0.246 \\
\bottomrule
\end{tabular}
    \caption{\concepttag{} Mean AUROC for each method on concept detection. \textbf{Bold} indicates highest AUROC in that column; \underline{underline} indicates no significant difference vs.~the best performer. \textcolor{gray}{Gray} indicates non-representation steering methods.}
    \label{tab:roc-auc}
\end{table}

% \paragraph{ROC curves.} Using the passage-level scores and the ground truth labels on the evaluation dataset, we compute the ROC (rate of change) curve for each method on each concept across all model settings. In \cref{tab:roc-auc}, we report the average area under the ROC curve (ROC AUC) for each method over all concepts. We plot the average ROC curve for each method in \cref{fig:avg-roc}.

\begin{figure}[!t]
    \centering
    \includegraphics[width=1.0\linewidth]{figures/concept500_balance.pdf}
    \caption{\concepttag{} Mean F1 scores vs.~dataset balance.}
    \label{fig:avg-f1}
\end{figure}

\paragraph{AUROC.} In \cref{tab:roc-auc}, we report the average area under the ROC curve (AUROC) for each method over all concepts. Overall, we find that DiffMean, Probe, and ReFT-r1 are the best performers with no statistically significant difference ($p < 0.05$) between any of them under a paired $t$-test. Prompt, SAE-A, and SSV are not far behind and significantly outperform the remaining methods. LAT also performs better than random. Vanilla SAEs
% without supervised feature selection 
are thus significantly outperformed by five supervised methods, all of which are much cheaper to train using a limited amount of synthetic data. The remaining methods (PCA, IG, and IxG) perform poorly; PCA's better-than-random performance is nevertheless impressive given its unsupervised nature. Additional results are given in \cref{sec:detail-result-analysis}.

% We plot all of the ROC curves under one model and layer setting (layer 10 in Gemma-2-2B) in \cref{fig:roc-detailed}. We note that for PCA (and, to a lesser extent, LAT), ROC curves can be grouped into two classes by shape; near-optimal curves which ascend to the top left, and signal-less curves which are diagonal. This suggests that only sometimes is the direction with the most variation also the best classification direction.

% \begin{table}[]
%     \small
%     \centering
% \begin{tabular}{lccccc}
% \toprule
% \multirow[t]{2}{*}{\textbf{Method}} & \multicolumn{2}{c}{\textbf{Gemma-2-2B}}  & \multicolumn{2}{c}{\textbf{Gemma-2-9B}} & \multirow[t]{2}{*}{\textbf{Avg.}} \\
% & \multicolumn{1}{c}{L10} & \multicolumn{1}{c}{L20} & \multicolumn{1}{c}{L20} & \multicolumn{1}{c}{L31} \\
% % identifier & 2b, l10, concept500 & 2b, l20, concept500 & 9b, l20, concept500 & 9b, l31, concept500 \\
% % method &  &  &  &  \\
% \midrule
% ReFT-r1 & \textbf{0.935} & \textbf{0.949} & \textbf{0.942} & 0.870 & \textbf{0.924} \\
% DiffMean & 0.919 & 0.927 & 0.922 & \underline{0.904} & 0.918 \\
% Probe & 0.908 & 0.914 & 0.907 & \textbf{0.905} & 0.909 \\
% SSV & 0.900 & 0.922 & 0.871 & 0.809 & 0.876 \\
% LAT & 0.738 & 0.797 & 0.623 & 0.742 & 0.725 \\
% SAE & 0.746 & 0.761 & 0.652 & 0.677 & 0.709 \\
% PCA & 0.713 & 0.700 & 0.605 & 0.667 & 0.671 \\
% IG & 0.552 & 0.540 & 0.568 & 0.528 & 0.547 \\
% IxG & 0.536 & 0.533 & 0.535 & 0.546 & 0.538 \\
% \bottomrule
% \end{tabular}
%     \caption{\concepttag{} Mean classification accuracy for each method.}
%     \label{tab:classification}
% \end{table}

% \begin{figure}[]
%     \centering
%     % Subfigure
%     \includegraphics[width=0.9\linewidth]{figures/concept500_precision_vs_recall.pdf}
%     \caption{\concepttag{} Mean precision vs.~recall.}
%     \label{fig:avg-prec-rec}
% \end{figure}

\paragraph{F1 score under class imbalance.} In real-world text, positive instances of concepts are much rarer than negative instances.
% (as reflected by e.g.~SAE concept sparsity).
We thus report F1 on both the balanced setting (50\% positive instances) and an imbalanced setting with 3600 additional negative examples ($\approx$1\% positive).
We choose classification threshold by maximising F1, binarise the resulting predictions, and report statistics on this discrete classification. \Cref{fig:avg-f1} shows that the relative ordering of methods does not change substantially between the two settings; despite their sparsity, SAEs perform poorly, but LAT and PCA also degrade substantially.
% We report the mean classification accuracy for each method in \cref{tab:classification}, noting similar ordering to mean ROC AUC. We also report mean precision vs.~recall in \cref{fig:avg-prec-rec}, noting the high precision but low recall for SAEs. This is probably a consequence of the auto-interp pipeline, which uses only positive exemplars to produce concept labels.

\subsection{\steeringtag{} Model steering}\label{sec:result-model-steering}

\begin{table}[!t]
    \centering
    \small
    % Subtable
\adjustbox{max width=\textwidth}{
\begin{tabular}{lccccc}
\toprule
\multirow[t]{2}{*}{\textbf{Method}} & \multicolumn{2}{c}{\textbf{Gemma-2-2B}}  & \multicolumn{2}{c}{\textbf{Gemma-2-9B}} & \multirow[t]{2}{*}{\textbf{Avg.}} \\
& \multicolumn{1}{c}{L10} & \multicolumn{1}{c}{L20} & \multicolumn{1}{c}{L20} & \multicolumn{1}{c}{L31} \\
% identifier & 2b, l10, concept500 & 2b, l20, concept500 & 9b, l20, concept500 & 9b, l31, concept500 \\
% method &  &  &  &  \\
\midrule
\rowcolor{gray!20}Prompt & \underline{0.698} & \textbf{0.731} & \textbf{1.075} & \textbf{1.072} & \textbf{0.894} \\
\rowcolor{gray!20}LoReFT & \textbf{0.701} & \underline{0.722} & 0.777 & 0.764 & 0.741 \\
\rowcolor{gray!20}SFT & 0.637 & 0.714 & --- & --- & \textit{0.676} \\
\rowcolor{gray!20}LoRA & 0.637 & 0.641 & 0.602 & 0.580 & 0.615 \\
ReFT-r1 & 0.633 & 0.509 & 0.630 & 0.401 & 0.543 \\
DiffMean & 0.297 & 0.178 & 0.322 & 0.158 & 0.239 \\
SAE & 0.177 & 0.151 & 0.191 & 0.140 & 0.165 \\
SAE-A & 0.166 & 0.132 & 0.186 & 0.143 & 0.157 \\
LAT & 0.117 & 0.130 & 0.127 & 0.134 & 0.127 \\
PCA & 0.107 & 0.083 & 0.128 & 0.104 & 0.105 \\
Probe & 0.095 & 0.091 & 0.108 & 0.099 & 0.098 \\
SSV & 0.072 & 0.001 & 0.024 & 0.008 & 0.026 \\
\bottomrule
\end{tabular}
}
    \caption{\steeringtag{} Mean overall steering scores for each method, after steering factor selection. \textcolor{gray}{Gray} indicates non-representation steering methods.}
    \label{tab:steer-judge}
\end{table}

\begin{figure}[!t]
    \centering
    % Subfigure
    \includegraphics[width=1.0\linewidth]{figures/concept500_concept_vs_instruct.pdf}
    \caption{\steeringtag{} Mean concept score vs.~instruct score as the steering factor for each method is varied.}
    \label{fig:mean-concept}
\end{figure}

For model steering, we take concept labels from \textsc{Concept500} and apply the (pre)trained steering methods to the base model and sample generations. We score the generations using an LM judge as described in \cref{sec:model-steering}. We additionally benchmark prompting, full-finetuning (SFT), and two parameter-efficient finetuning methods (LoReFT and LoRA) as non-steering baselines.

For steering methods, we note that steering factor is an important hyperparameter. We select the optimal steering factor for each method independently for every concept based on which factor achieves the highest \textit{overall} steering score, as given by the LLM judge. Our actual steering magnitude (i.e., $\alpha$, as described in \cref{sec:methods}) is the product of the steering factor and the maximal activations aggregated over the evaluation dataset for concept detection.\footnote{For SAEs, we query \href{https://www.neuronpedia.org/}{Neuronpedia} to obtain the maximal activation per concept.}

\paragraph{Overall scores.} We report the mean overall score for each method (i.e.~the harmonic mean of three subscores: fluency, instruction-following, and concept presence) in \cref{tab:steer-judge}. Prompting, along with slightly worse finetuning baselines, outperforms all steering methods on average, except for ReFT-r1. ReFT-r1 is competitive with prompting in Gemma-2-2B but significantly behind on Gemma-2-9B; prompting scores improve by a large margin on the larger model. Additionally, DiffMean significantly outperforms SAEs, particularly in earlier layers. 

The remaining supervised steering methods fail to beat SAEs, and no steering methods besides ReFT-r1 approach prompting or finetuning performance. Importantly, we note that SAE-A slightly underperforms the unsupervised SAE; better classification does not directly lead to better steering.

\paragraph{Winrate.} We compute winrates against SAEs by comparing overall scores on each concept under each setting. We treat ties as 0.5 wins and 0.5 losses. We report the results in \cref{tab:winrate}. Again, ReFT-r1 (88.0\%) and DiffMean (61.6\%) achieve winrates of greater than 50\% against SAEs, and relative rankings are similar to those for overall score. We note that DiffMean and ReFT-r1 show higher winrates on earlier layers in both models.

\begin{table}[]
    \centering
    \small
    % Subtable
\adjustbox{max width=\textwidth}{
\begin{tabular}{lccccc}
\toprule
\multirow[t]{2}{*}{\textbf{Method}} & \multicolumn{2}{c}{\textbf{Gemma-2-2B}}  & \multicolumn{2}{c}{\textbf{Gemma-2-9B}} & \multirow[t]{2}{*}{\textbf{Avg.}} \\
& \multicolumn{1}{c}{L10} & \multicolumn{1}{c}{L20} & \multicolumn{1}{c}{L20} & \multicolumn{1}{c}{L31} \\
% identifier & 2b, l10, concept500 & 2b, l20, concept500 & 9b, l20, concept500 & 9b, l31, concept500 \\
% method &  &  &  &  \\
\midrule
% \rowcolor{gray!20} Prompt & 85.1\% & \textbf{88.6\%} & \textbf{96.6\%} & \textbf{98.6\%} & \textbf{92.2\%} \\
% ReFT-r1 & \textbf{90.0\%} & \underline{87.6\%} & 90.0\% & 84.4\% & 88.0\% \\
% \rowcolor{gray!20} SFT & \textbf{90.0\%} & 80.0\% & --- & --- & \textit{85.0\%} \\
% \rowcolor{gray!20} LoReFT & 81.4\% & 84.6\% & 85.5\% & 87.2\% & 84.7\% \\
% \rowcolor{gray!20} LoRA & 75.4\% & 79.6\% & 73.8\% & 75.4\% & 76.0\% \\
% DiffMean & 69.3\% & 56.7\% & 69.4\% & 51.1\% & 61.6\% \\
% \rowcolor{blue!20} SAE & 50.0\% & 50.0\% & 50.0\% & 50.0\% & 50.0\% \\
% SAE-A & 48.1\% & 47.1\% & --- & --- & \textit{47.6\%} \\
% Probe & 37.6\% & 40.8\% & 36.3\% & 41.6\% & 39.1\% \\
% LAT & 34.6\% & 40.8\% & 33.0\% & 44.0\% & 38.1\% \\
% PCA & 32.3\% & 32.2\% & 29.7\% & 38.9\% & 33.3\% \\
% SSV & 21.3\% & 15.5\% & 18.0\% & 19.8\% & 18.6\% \\
\rowcolor{gray!20}Prompt & \textbf{90.0}\% & \textbf{91.5}\% & \textbf{97.6}\% & \textbf{99.1}\% & \textbf{94.5}\% \\
\rowcolor{gray!20}LoReFT & \underline{88.9\%} & 88.2\% & 88.6\% & 90.3\% & 89.0\% \\
\rowcolor{gray!20}SFT & \textbf{90.0}\% & 87.5\% & --- & --- & \textit{88.8}\% \\
\rowcolor{gray!20}LoRA & 85.0\% & 83.4\% & 79.9\% & 81.5\% & 82.5\% \\
ReFT-r1 & 85.2\% & 82.3\% & 83.6\% & 76.0\% & 81.8\% \\
DiffMean & 63.2\% & 55.2\% & 64.3\% & 52.2\% & 58.7\% \\
SAE & 50.0\% & 50.0\% & 50.0\% & 50.0\% & 50.0\% \\
SAE-A & 49.3\% & 46.6\% & 48.5\% & 50.7\% & 48.8\% \\
LAT & 43.5\% & 48.2\% & 42.7\% & 48.6\% & 45.8\% \\
PCA & 42.1\% & 42.9\% & 42.2\% & 45.4\% & 43.1\% \\
Probe & 40.4\% & 44.0\% & 41.9\% & 45.6\% & 43.0\% \\
SSV & 38.8\% & 32.0\% & 32.5\% & 34.0\% & 34.3\% \\
\bottomrule
\end{tabular}
}
    \caption{\steeringtag{} Winrate against SAEs for each method, after steering factor selection.}
    \label{tab:winrate}
\end{table}

% \section{Analysis}

\paragraph{Steering factor.} We compare the effect of changing the steering factor on instruct vs.~concept scores in \cref{fig:mean-concept}. We notice that increasing the factor monotonically reduces instruct score in all methods, i.e.~larger steering vectors harm capabilities; this agrees with prior findings \cite{durmus2024steering,chalnev2024improvingsteeringvectorstargeting}. However, the effect varies by layer for concept score: concept score increases then decreases in earlier layers, while it roughly monotonically increases with steering factor in later layers. In all cases, ReFT-r1 traces a Pareto-optimal path, achieving the highest concept score for any chosen instruct score.

% \paragraph{\textsc{Concept16K}.} We scale up two supervised dictionary learning methods DiffMean and ReFT-r1 with \textsc{Concept16K}. They serve as drop-in replacements of existing SAEs on \texttt{Gemma} models with better performance for concept detection and steering.\footnote{We open-source our datasets and trained dictionaries at \url{https://huggingface.co/pyvene}.} 

% \begin{figure*}
%     \centering
%     \includegraphics[width=\linewidth]{figures/concept500_steering.pdf}
%     \caption{\steeringtag{} Mean LM Judge subscores for each method on \textsc{Concept500}.}
%     \label{fig:steering-detailed}
% \end{figure*}

\section{Discussion}

% Our results demonstrate the potential benefits of supervised dictionary-learning (SDL) methods, such as ReFT-r1 and DiffMean on synthetic data, over sparse autoencoders (SAEs). Additionally, we provide sobering evidence of the current limitations of steering techniques.

\paragraph{Simple yet powerful baselines.} While representation-level interventions have been shown to be useful in both enhancing model capabilities and for safety~(see \cref{sec:related}), they fail to outperform standard prompting and finetuning baselines on \shortbenchmark{}. This is sobering evidence of the current limitations of steering techniques. However, our results suggest that joint learning of concept detection and steering (as in ReFT-r1) may be the key to advancement.

\paragraph{SDL vs.~SAEs.} We have shown that SDL methods can achieve similar scalability and better performance at a lower cost compared to SAEs. Unlike SAEs, SDL methods require concepts to be known \emph{a priori}; however, SDLs can be easily augmented with new features without retraining. We also note that SDLs depend on high-quality data generators, whereas SAEs rely on high-quality concept discriminators. These methods are not mutually exclusive and can complement each other. 

% Throughout our experiments, SAEs were used without further adaptation. Future work could benchmark SAE performance under limited training or adaptation budgets, similar to ReFT-r1 and other parameter-efficient fine-tuning (PEFT) methods. In contrast, SDL methods could be augmented similarly, as they are fundamentally equivalent to SAEs in that they both produce subspace dictionaries.

% For example, SDLs could be trained with exemplars that emphasize regions of SAE latent space, enabling the identification of more fine-grained subspaces.

\paragraph{SAE concept label quality.} The concept lists used in this paper were adapted from Neuronpedia's auto-interpretability pipeline, which is often skewed towards token-level concepts and misses high-level abstractions. While we tried to do post-hoc SAE feature selection to mitigate this, the poor performance of SAEs is at least partially a reflection of the limitations of auto-interpretability. It would be interesting to explore whether the SAE performance on \shortbenchmark{} improves as better feature labelling methods are used and labels become less shallow (e.g.~\citealp{choi2024automatic}).

\section{Conclusion}
We introduced \shortbenchmark{}, a new benchmark for evaluating LM control methods at scale using synthetic data. To answer the question in the title of this work: our evaluation shows that even at SAE scale, representation steering is still \textit{far behind} simple prompting and finetuning baselines. Simultaneously, we showed that a novel steering method, ReFT-r1, is capable of \textit{closing the gap} to some extent; representation-based steering has not yet exhausted its potential. No matter the outcome, we believe that comprehensive evaluation benchmarks like \shortbenchmark{} are necessary for continued progress on this problem.

\section*{Impact Statements}

In this paper, we explore representation-based methods for steering language models and introduce \shortbenchmark{}, a large-scale benchmark for evaluating these techniques. We believe that the immediate ethical and societal implications of our research are minimal. However, we recognize that enhanced control over language model outputs could potentially be misused to reinforce biases or manipulate information. To address these concerns, we advocate for the responsible application of steering methods and ensure transparency by publicly releasing our datasets and feature dictionaries. We encourage ongoing collaboration and dialogue within the research community to monitor and mitigate any unintended consequences of these technologies.

\section*{Acknowledgements}
We thank Róbert Csordás, Qinan Yu, and Jiuding Sun for constant and extremely helpful feedback during our weekly interp meetings; Jake Mendel for enlightening discussion about the direction and framing of the work; Neel Nanda for helpful suggestions on SAE feature selection; 
% We thank all of our human annotators listed in \cref{sec:human_eval}; 
and Chenglei Si, Ken Ziyu Liu, Oam Patel, Luke Bailey, Harshit Joshi, Yanzhe `Sanju' Zhang, Nikil Roashan Selvam, Julie Kallini, Omar Shaikh, Thomas Chen, Tristan Thrush, and Yangjun Ruan for various helpful discussions. We thank Joseph Tey and Nick Jiang for pointing out equation typos in an earlier draft.

This research is supported in part by grants from
Open Philanthropy.

\bibliography{bibliography}
\bibliographystyle{plainnat}


\newpage
\appendix
\onecolumn
\renewcommand \thepart{}
\renewcommand \partname{}
\noptcrule
\part{Appendix} % Start the appendix part
\parttoc % Insert the appendix TOC
% \newpage

\section{Historical notes on steering}

\textit{Inspired by \citet{slp3} and noting the sociological observations about (mechanistic) interpretability as a field in \citet{saphra-wiegreffe-2024-mechanistic}, we offer some historical notes on the development of steering as a field in an effort to document and properly cite where these ideas came from.}

\textit{Steering} refers to applying interventions (usually adding a fixed vector) to the activation space of a neural model in order to control its generations. Early precursors to steering noted that linear subspaces of the representation space of pretrained word vectors seemed to encode meaningful concepts \cite{mikolov2013,pennington-etal-2014-glove,bolukbasi2016man}.

\citet{larsen2016auto} first used the \textit{difference-in-means} technique to extract visual \textit{attribute vectors} from GAN discriminators in order to steer generator outputs; this technique was widely adopted in computer vision \cite{white2016samplinggenerativenetworks,upchurch2017,goh,wang2019implicit}.

In NLP, initial work by \citet{subramani-etal-2022-extracting} proposed \textit{steering vectors}, learned to maximise the probability of some output, as an alternative to expensive fine-tuning and unreliable prompt optimisation for the task of controllable text generation. Soon after, steering was also use to localise behaviours in a maze-searching RL agent \citep{turner2023,turner2023b,mini2023understandingcontrollingmazesolvingpolicy}. Variations on this approach (sometimes using difference-in-means or other closed-form expressions to compute the vector) were adopted by researchers in \textit{mechanistic interpretability} from late 2023 for AI safety \cite{zou2023representationengineeringtopdownapproach,liInferenceTimeInterventionEliciting2024,turner2024steeringlanguagemodelsactivation,marks2024geometrytruthemergentlinear,rimsky-etal-2024-steering} and later as a general-purpose but localised and parameter-efficient alternative to finetuning \cite{wu2024reft,liu2024incontext,vanderweij2024extendingactivationsteeringbroad}.

\textit{Sparse autoencoders} (SAEs), a scalable technique for self-supervised rank-one linear feature discovery via dictionary learning, are also increasingly used to find or learn steering vectors \cite{templeton2024scaling,chalnev2024improvingsteeringvectorstargeting,makelov2024sparse,obrien2024steeringlanguagemodelrefusal}.

\section{SAE concept list}\label{sec:sae-concept-list}

We use SAE concept lists to enable a fair comparison with SAEs, which were annotated mostly by \texttt{gpt-3.5-turbo} or \texttt{gpt-4o-mini}. These concept lists are released by \href{https://www.neuronpedia.org/}{Neuronpedia} and were scraped by the authors of this paper in November 2024. We utilize the concept lists from four SAEs from \texttt{GemmaScope}: \texttt{10-gemmascope-res-16k} for the \texttt{Gemma-2-2B} base model and \texttt{20-gemmascope-res-131k} for the \texttt{Gemma-2-9B} instruction-tuned model, where we scraped a maximum of 16K concepts.

\clearpage

\section{Detailed analysis}\label{sec:detail-result-analysis}

\subsection{\concepttag{} Concept detection}

\begin{figure}[!h]
    \centering
    % Subfigure
    \includegraphics[width=0.5\linewidth]{figures/concept500_roc_mean.pdf}
    \caption{\concepttag{} Mean ROC curves over all concepts.}
    \label{fig:avg-roc}
\end{figure}

\begin{figure}[!h]
    \centering
    \includegraphics[width=\linewidth]{figures/concept500_roc_all.pdf}
    \caption{All ROC curves.}
    \label{fig:roc-all}
\end{figure}

\clearpage

\subsection{\steeringtag{} Model steering}

\begin{figure}[!h]
    \centering
    \includegraphics[width=\linewidth]{figures/concept500_steering.pdf}
\caption{Mean score breakdown for all methods on our unseen testing instruction set after selecting the optimal factor (based on the Overall Score) on our evaluation instruction set. For prompting and finetuning, we randomly score one generation on the testing instruction set (since the factor is not a parameter for those methods), resulting in the same number of observations for those methods.}
    \label{fig:steering-detailed}
\end{figure}

\begin{figure}[!h]
    \centering
    \includegraphics[width=\linewidth]{figures/concept500_optimal_factor.pdf}
    \caption{Distribution of optimal steering factors for each method across the 4 tasks.}
    \label{fig:steering-factor-optimal}
\end{figure}

\clearpage

\begin{figure}[t]
    \centering
    \includegraphics[width=\linewidth]{figures/concept500_steering_factor.pdf}
    \caption{Steering factor vs.~scores.}
    \label{fig:steering-factor-detailed}
\end{figure}

\clearpage

\section{Supervised dictionary learning method works with very limited amount of training data.}\label{sec:scaling-law} 

Based on the performance results, ReFT-r1 is the strongest SAE alternative. We further study the data scaling law of ReFT-r1 by varying the number of training examples. Specifically, we measure ReFT-r1 performance on both concept detection and steering with \textsc{Concept10} when the number of training example is set to \{6, 12, 24, 48, 72, 96, 120, 144\}. In the extreme setting, we provide only 3 positive and 3 negative examples. Since we have a limited pool of concepts, we average our results with three random seeds: \{42, 43, 44\}.
% The overall steering score is calculated solely using our evaluation set, which includes half of our testing instructions.

\cref{fig:scaling-law} shows how the performance of ReFT-r1 varies in \concepttag{} (concept detection) and \steeringtag{} (model steering) when trained with different numbers of training examples. For earlier layers, scores increase with more data, while for \texttt{Gemma-2-9B}, the trend is less clear for concept detection. Our results indicate that once a certain threshold is reached, performance saturates for both tasks, suggesting that the cost of training ReFT-r1 can be further reduced. The per-concept cost with 144 training examples is approximately \$0.008, and this cost decreases proportionally as the number of training examples is reduced.

\begin{figure}[h!]
    \centering
    % Subfigure
    \includegraphics[width=0.9\linewidth]{figures/concept10_scaling_factor.pdf}
    \caption{Scaling law for supervised dictionary learning (SDL) method ReFT-r1 with \textsc{Concept10} on both \concepttag{} concept detection and \steeringtag{} model steering.}
    \label{fig:scaling-law}
\end{figure}

\clearpage

\section{SDLs at scale: Analysing \textsc{Concept16K}}\label{sec:sdl-16k}

\subsection{ReFT-r1: \textsc{Concept16K} subspace for code error handling.}\label{sec:subspace-viz} 

We scale up two supervised dictionary learning methods DiffMean and ReFT-r1 with \textsc{Concept16K}. They serve as drop-in replacements of existing SAEs on \texttt{Gemma} models with better performance for concept detection and steering.
% \footnote{We open-source our datasets and trained dictionaries at \url{https://huggingface.co/pyvene}.} 

\cref{fig:umap-16k} shows the UMAP of ReFT-r1's \textsc{Concept16K} subspaces learned with \texttt{Gemma-2-2B} at layer 20's residual stream. Subspaces are meaningfully clustered together by genres. Within each genre cluster, related features are also clustered together. For instance, we identify a subspace cluster for concepts related to ``Code error handling and logging,'' which includes the following concepts:
\begin{itemize}
  \item \texttt{Subspace 16K/14404}: ``error messages related to system calls and file operations''
  \item \texttt{Subspace 16K/14801}: ``terms related to programming errors and error handling''
  \item \texttt{Subspace 16K/5656}: ``technical terms and parameters related to errors and status in programming contexts''
  \item \texttt{Subspace 16K/4884}: ``error messages and exceptions in code related to server or network operations''
  \item \texttt{Subspace 16K/2467}: ``references to errors and warnings, especially related to file or access issues''
\end{itemize}

\begin{figure}[h!]
    \centering
    % Subfigure
    \includegraphics[width=\linewidth]{figures/concept16k_umap.pdf}
    \caption{UMAP of ReFT-r1's \textsc{Concept16K} subspaces with \texttt{Gemma-2-2B} at layer 20's residual stream.}
    \label{fig:umap-16k}
\end{figure}

\clearpage

\subsection{Mapping natural language to subspaces.}\label{sec:subspace-gen} 

We explore whether we can find a direct mapping from natural-language concept descriptions to subspaces. We first train ReFT-r1 with \textsc{Concept16K} and create a supervised dataset $\mathcal{D}^{\text{Generator}} = \{(\vc, \mathbf{w}_{\text{ReFT-r1}}^{\vc})_{0}^{\text{16K}}\}$, where the input $\vc$ is the concept description in natural language and the output is the ReFT-r1 subspace vector corresponding to the concept. We divide $\mathcal{D}^{\text{Generator}}$ into training and testing sets, ensuring that the testing set contains only concepts from \textsc{Concept500}, which are excluded from the training set. To train the generator, we attach a supervised linear head $\Phi_{\text{Generator}}$ to the last input token representation at the $n$-th position of the last layer $m$, predicting the learned ReFT-r1 subspace:  
\begin{equation}
\gL = \gL_{\text{MSE+Cosine}}\bigl(\mathbf{w}_{\text{ReFT-r1}}^{\vc}, \Phi_{\text{Generator}}([\mathrm{LM}_{\theta}(\vc)]_{n}^{m})\bigr)
\end{equation}
where we fine-tune the generator head and the LM using equally weighted MSE and cosine distance losses. We do finetune the base LM \texttt{Gemma-2-2b} for our subspace generators. 
We partition the last 500 examples in our training dataset as our in-training development set to early-stop our training with a patience step set to 3.

We generate ReFT-r1 subspaces for \textsc{Concept500} and follow our evaluation paradigm in \shortbenchmark{} to evaluate concept detection and model steering. We show two cases below by unembedding generated subspaces with the output embedding matrix. We find that the subspace generator works better in English as opposed to other languages. 

As shown in \cref{tab:subspace-gen-roc-auc} and \cref{tab:subspace-gen-steer-judge}, subspaces for unseen concepts generated by our finetuned model exhibit only slight performance degradation in concept detection, while performance drops more significantly in model steering.

\input{tables/concept500_subspace_gen}

\input{examples/lang_to_subspace}

\clearpage

\subsection{Teleporting between subspaces across models through affine transformations.}\label{sec:subspace-transform} 

We explore whether structural equivalence in subspaces exists across models. Previous works have analyzed feature universality in SAEs but have been limited to a small set of features~\cite{lan2024sparse}. Given that our \textsc{Concept16K} dataset contains two sets of concepts for \texttt{Gemma-2-2B} and \texttt{Gemma-2-9B}, we first train ReFT-r1 on both models separately, obtaining $\mathbf{w}_{\text{ReFT-r1}}^{\text{2B}}$ and $\mathbf{w}_{\text{ReFT-r1}}^{\text{9B}}$. Next, we perform a cross-fitting experiment, training ReFT-r1 on \texttt{Gemma-2-2B} with concepts from \texttt{Gemma-2-9B}, resulting in $\mathbf{w}_{\text{ReFT-r1}}^{\text{9B}\;\mid\;\text{2B}}$, and vice versa for $\mathbf{w}_{\text{ReFT-r1}}^{\text{2B}\;\mid\;\text{9B}}$. Thus, $\mathbf{w}_{\text{ReFT-r1}}^{\text{9B}}$ and $\mathbf{w}_{\text{ReFT-r1}}^{\text{9B}\;\mid\;\text{2B}}$ represent two sets of subspaces from different models that correspond to the same set of concepts. 

We then study whether a transformation can map between these two sets of subspaces:  
\[
\mathbf{w}_{\text{ReFT-r1}}^{\text{9B}} = \Phi_{\text{Transformation}}^{\text{2B}\rightarrow\text{9B}}(\mathbf{w}_{\text{ReFT-r1}}^{\text{2B}\;\mid\;\text{9B}}),
\]
where $\Phi_{\text{Transformation}}$ is parameterized by a linear layer with a bias (i.e., an affine transformation). We learn the transformation using equally weighted MSE and cosine distance losses. Similarly, $\Phi_{\text{Transformation}}^{\text{9B}\rightarrow\text{2B}}$ is trained by reversing the direction. During training, we exclude concepts from \textsc{Concept500}, and evaluate the transformation on \textsc{Concept500} at test time by generating subspaces. We follow our evaluation paradigm in \shortbenchmark{} to assess concept detection and model steering.

Our evaluation results on \textsc{Concept500} are presented in \cref{tab:transform-roc-auc} and \cref{tab:transform-steer-judge}. Surprisingly, the \emph{affine} transformation performs well in both directions (from $\text{2B}\rightarrow\text{9B}$ and $\text{9B}\rightarrow\text{2B}$), with little to no change in concept detection performance. While performance drops for model steering, it still outperforms other methods, including fine-tuning. \cref{fig:transform-2b-to-9b} and \cref{fig:transform-9b-to-2b} visualize the transformations using the first two PCA dimensions. PCA is preferred over UMAP in this context because it is sensitive to rotation.

\input{tables/concept500_transform}

\begin{figure}[!h]
    \centering
    % Subfigure
    \includegraphics[width=0.9\linewidth]{figures/concept16k_transform_2B_to_9B.pdf}
    \caption{Visualizations of \textsc{Concept16K} subspaces of \texttt{Gemma-2-2B} and \texttt{Gemma-2-9B} at layer 20 with
    top 2 principal component analysis (PCA) dimensions. The last panel is the derived subspaces by transforming the subspaces from \texttt{Gemma-2-2B} to \texttt{Gemma-2-9B} through a learned affine transformation. The concept lists for \textsc{Concept16K} is taken from the source model.}
    \label{fig:transform-2b-to-9b}
\end{figure}

\begin{figure}[h!]
    \centering
    % Subfigure
    \includegraphics[width=0.9\linewidth]{figures/concept16k_transform_9B_to_2B.pdf}
    \caption{Visualizations of \textsc{Concept16K} subspaces of \texttt{Gemma-2-2B} and \texttt{Gemma-2-9B} at layer 20 with
    top 2 principal component analysis (PCA) dimensions. The last panel is the derived subspaces by transforming the subspaces from \texttt{Gemma-2-9B} to \texttt{Gemma-2-2B} through a learned affine transformation. The concept lists for \textsc{Concept16K} is taken from the source model.}
    \label{fig:transform-9b-to-2b}
\end{figure}

\clearpage

\section{Ablations}\label{sec:abaltions}

\subsection{SAE}\label{sec:ablations-add-clamp}

\paragraph{Addition vs.~clamping.}
In our main results, we steer using SAEs by adding their decoder features directly to the residual stream. While this is a common technique for steering with SAEs, most work by Anthropic \cite[e.g.][]{templeton2024scaling,durmus2024steering} uses an alternative formulation termed \textit{clamping}, where the latent $z_f$ for feature $f$ is directly clamped to a value $\alpha$ (multiplied by the maximum activation for that feature $m_f$) and the full intervened SAE output added to its unclamped reconstruction error $\mathrm{Err}(h_i)$:
% \begin{align}
% \Phi^{\text{SAE}}_{\text{Clamp}}(h_i) &= (\mathbf{W}_{\mathrm{enc}}^{\top}h_i + (\alpha \cdot m_f - (\mathbf{W}_{\mathrm{enc}}^{\top}h_i)_f) e_f^{\top})\mathbf{W}_{\mathrm{dec}} + \mathrm{Err}(h_i)\\
% \mathrm{Err}(h_i) &= h_i - (\mathbf{W}_{\mathrm{enc}}^{\top}h_i)\mathbf{W}_{\mathrm{dec}}
% \end{align}
\begin{align}
\Phi^{\text{SAE}}_{\text{Clamp}}(h_i) &= (\mathbf{W}_{\mathrm{enc}}^{\top}h_i + (\overbrace{\alpha \cdot m_f}^{\text{clamped}} - z_f)e_f^{\top})\mathbf{W}_{\mathrm{dec}} + \mathrm{Err}(h_i)\\
z_f &= (\mathbf{W}_{\mathrm{enc}}^{\top}h_i)_f\\
\mathrm{Err}(h_i) &= h_i - (\mathbf{W}_{\mathrm{enc}}^{\top}h_i)\mathbf{W}_{\mathrm{dec}}
\end{align}
where \( e_f^{\top} \) is a one-hot vector with a non-zero entry at the dimension corresponding to \( m_f \). We evaluate clamping on all steering tasks on \textsc{Concept500} for direct comparison with the addition-based \texttt{GemmaScope} SAE. We use the following values for $\alpha$ (the steering factor): $\{0.4, 0.8, 1.2, 1.6, 2.0, 3.0, 4.0, 6.0, 8.0, 10.0, 20.0, 40.0, 60.0, 100.0\}$. Overall, we find that clamping is on average \textit{worse} than addition for SAEs, although it exhibits marked improvement when scaling up from 2B to 9B.

\paragraph{Maximum activation and minimum clamping.}
In our main results, the maximum activation for our feature \( m_f \) is obtained from \texttt{Neuronpedia}. This approach differs from other methods, which determine the maximum activation by analyzing the activation distribution over the evaluation dataset for concept detection. For this experiment, we calculate \( m_f \) for SAEs in the same manner as other methods. As shown in \cref{tab:sae-ablation} and \cref{fig:sae-steering-factor}, changing the method of calculating maximum activations has minimal impact on the steering performance; most comparisons are statistically insignificant.

In addition, building on regular activation \textit{clamping} as described above, we try a novel minimal clamping where we only clamp the activation value if it is smaller than the target value:
\begin{align}
\Phi^{\text{SAE}}_{\text{Clamp}}(h_i) &= (\mathbf{W}_{\mathrm{enc}}^{\top}h_i + (\max(\overbrace{\alpha \cdot m_f}^{\text{clamped}}, z_f) - z_f)e_f^{\top}\mathbf{W}_{\mathrm{dec}} + \mathrm{Err}(h_i)
\end{align}
where $(\mathbf{W}_{\mathrm{enc}}^{\top}h_i)_{f}$ is the original activation value at the corresponding of feature $f$ and \( e_f^{\top} \) is a one-hot vector with a non-zero entry at the dimension corresponding to \( m_f \). As shown in \cref{tab:sae-ablation} and \cref{fig:sae-steering-factor}, using minimum clamping has no significant impact on SAE's steering performance.

\paragraph{Results.} We report results in \cref{tab:sae-ablation}. We also examine the effect of varying $\alpha$ in \cref{fig:sae-steering-factor}. Note that $\alpha$ is likely a concept-dependent parameter; the optimal $\alpha$ varies from concept to concept. We notice an odd trend for clamping: small values of $\alpha$ have a similar effect on model behaviour as large values of $\alpha$; both cause concept score to increase and instruct score to decrease.
\begin{table}[h!]
\begin{subtable}[b]{0.5\textwidth}
    \small
    \centering
\begin{tabular}{lccccc}
\toprule
\multirow[t]{2}{*}{\textbf{Method}} & \multicolumn{2}{c}{\textbf{Gemma-2-2B}}  & \multicolumn{2}{c}{\textbf{Gemma-2-9B}} & \multirow[t]{2}{*}{\textbf{Avg.}} \\
& \multicolumn{1}{c}{L10} & \multicolumn{1}{c}{L20} & \multicolumn{1}{c}{L20} & \multicolumn{1}{c}{L31} \\
% identifier & 2b, l10, concept500 & 2b, l20, concept500 & 9b, l20, concept500 & 9b, l31, concept500 \\
% method &  &  &  &  \\
\midrule
\rowcolor{blue!20}SAE & \textbf{0.177} & \textbf{0.151} & \textbf{0.191} & \textbf{0.140} & \textbf{0.165} \\
SAE (max act) & \underline{0.166} & \underline{0.150} & 0.163 & \underline{0.128} & 0.152 \\
SAE-c (min clamp) & 0.074 & 0.072 & 0.123 & 0.090 & 0.090 \\
SAE-c & 0.063 & 0.061 & 0.126 & 0.120 & 0.088 \\
\bottomrule
\end{tabular}
\caption{Overall score.}
\end{subtable}
\begin{subtable}[b]{0.5\textwidth}
    \small
    \centering
\begin{tabular}{lccccc}
\toprule
\multirow[t]{2}{*}{\textbf{Method}} & \multicolumn{2}{c}{\textbf{Gemma-2-2B}}  & \multicolumn{2}{c}{\textbf{Gemma-2-9B}} & \multirow[t]{2}{*}{\textbf{Avg.}} \\
& \multicolumn{1}{c}{L10} & \multicolumn{1}{c}{L20} & \multicolumn{1}{c}{L20} & \multicolumn{1}{c}{L31} \\
% identifier & 2b, l10, concept500 & 2b, l20, concept500 & 9b, l20, concept500 & 9b, l31, concept500 \\
% method &  &  &  &  \\
\midrule
\rowcolor{blue!20} SAE & \textbf{50.0}\% & \textbf{50.0}\% & \textbf{50.0}\% & \textbf{50.0}\% & \textbf{50.0}\% \\
SAE (max act) & \underline{49.1\%} & \underline{49.8\%} & 46.8\% & 47.5\% & 48.3\% \\
SAE-c & 36.3\% & 38.7\% & 42.1\% & \underline{49.2\%} & 41.6\% \\
SAE-c (min clamp) & 38.2\% & 40.1\% & 41.0\% & 42.8\% & 40.5\% \\
\bottomrule
\end{tabular}
\caption{Winrate.}
\end{subtable}
    \caption{\steeringtag{} Overall scores on model steering.}
    \label{tab:sae-ablation}
\end{table}

\begin{figure}[!h]
    \centering
    \includegraphics[width=0.8\linewidth]{figures/concept500_instruct_vs_concept_saec.pdf}
    \caption{\steeringtag{} Instruct score vs.~concept score for SAEs with addition (SAE) vs.~clamping (SAE-c) when varying the steering factor. Additionally, we include results when SAE is clamped with the maximum activation value calculated based on our evaluation dataset for concept detection, as well as results with minimum clamping of activation values.}
    \label{fig:sae-steering-factor}
\end{figure}

\newpage
\section{Large language model (LLM) usage}\label{sec:llm-usages}
We use LLMs for two purposes: to generate labelled concept data for training supervised steering methods and to evaluate the responses generated by the steered models. Specifically, we use OpenAI's \texttt{gpt-4o-mini-2024-07-18} (accessed via the alias \texttt{gpt-4o-mini} in the API) throughout our experiments. The date we access the LLM ranges from December 2024 to January 2025, and we use the default generation configuration with temperature set to 1.0 to fetching LLM responses. For 1M tokens, it costs $\$0.15$ for input tokens and $\$0.60$ for output tokens.

\section{Gradient-based baselines}
\label{sec:gradient-baselines}

\paragraph{\concepttag\ Input$\times$gradients (I$\times$G).} Gradient-based interpretability methods have been shown to be useful in computer vision and NLP~\cite{integratedgradient, wallace2019allennlp}. I$\times$G serves as the gradient-based baseline. We first train a linear classification head $\Phi_{\text{CLS}}$ on the token representation at the $n$-th position of the last layer $m$, to predict the ground-truth concept-presence class label $y$:
\begin{equation}
\gL = \gL_{\text{BCE}}\bigl(y, \Phi_{\text{CLS}}(h_{n}^{(m)})\bigr)
\end{equation}
where $\Phi_{\text{CLS}}$ is parameterised by an MLP with two linear layers. For an evaluation sentence \(\mathbf{x}\), the LM generates hidden representations \(\mathbf{h}\) with $n$ tokens at layer $l$. With \texttt{Autograd} provided in \texttt{PyTorch}, we calculate the gradient of the output classification head with respect to each hidden representations. To aggregate across dimensions, we compute the sum of the absolute gradients over all dimensions for each \(h_i\), which we use as the token-level importance. This gives a sequence of aggregated values:

\[
\Psi_{\text{Detect}}^{\text{I}\times\text{G}}(\mathbf{h}) = \mathbf{g} = [\,g_1, g_2, \ldots, g_n\,]
\]
which indicates the relevance of each token for the concept. For concept detection, we then use max-pooling as described in \cref{sec:concept-detection} to get sequence-level predictions. I$\times$G is not applicable for model steering.


\paragraph{\concepttag\ Integrated gradients (IG).} We adapt IG~\cite{integratedgradient} to trace the accumulated gradients with respect to intermediate representations. To use IG, we train a classification head as in I$\times$G. For each token representation \(h_i\), we compute IG along a straight-line path from a baseline \(h_i^{\text{baseline}}\) to \(h_i\). Here, we use the embedding of a single space token (i.e., \texttt{tokenizer(`` '')}), obtained via the tokenizer and model embeddings, as the baseline. The IG is computed as:

\begin{align*}
\text{IG}(h_i) = & \, (h_i - h_i^{\text{baseline}}) \cdot \int_{0}^{1} 
\nabla_h \Phi_{\text{CLS}} \bigl(h_i^{\text{baseline}} + \alpha (h_i - h_i^{\text{baseline}}) \bigr) \, d\alpha
\end{align*}

where the integral is approximated using a discrete summation with 50 interpolation steps.\footnote{We choose a small step number due to limited compute resource. For 9B models, we use a step size of 5.} See IG's original paper about implementation details on path intergral. To aggregate across dimensions, we compute the sum of the absolute gradients over all dimensions for each \(h_i\). We follow the setup for I$\times$G get sequence-level predictions for concept detection. IG is not applicable for model steering.

\section{Instruction pool}\label{sec:instruction_pool}
To create our instruction pool, we sample instructions from existing datasets covering three genres: \textit{text}, \textit{code}, and \textit{math}. For \textit{text}-based instructions, we sample from \texttt{Dolly-15K}\footnote{\url{https://huggingface.co/datasets/databricks/databricks-dolly-15k}.}. For \textit{code}-based instructions, we sample from a collection of \texttt{Python}-code puzzles formed in \texttt{Alpaca}-style (i.e., instructions with corresponding responses)\footnote{\url{https://huggingface.co/datasets/iamtarun/python_code_instructions_18k_alpaca}.} For \textit{math}-based instructions, we sample from \texttt{GSM8K}\footnote{\url{https://huggingface.co/datasets/openai/gsm8k}.} For each genre, we create two disjoint sets for training and testing separately with 1,000 examples each. The LLM generated response for each instruction is provided as well. To avoid any heterogeneity, we avoid using the golden labels provided by the orignal dataset and only use the LLM generated response.

\input{examples/instructions}

\section{Prompt templates}\label{sec:prompt_templates}
In this section, we present the templates that we use to call LLM to generate datasets or evaluate results. For placeholders in the template, they will be filled with proper information.

\subsection{Prompt-based steering}\label{sec:prompt-steering-template}
Our prompt-based model steering baseline is not a zero-shot prompting baseline.
Instead of directly prompting LLM to generate steered responses, we first use LLM to generate an enhanced prompt for model steering. Our template is included in the following. 

\input{examples/templates-prompt-steering}

\subsection{Synthetic data generation}\label{sec:generation-prompts}
Our data generation pipeline contains multiple steps, and we use different templates at each step. We present the template that we use for each step in the following.

\input{examples/templates-datagen}

\subsection{Automatic evaluation}\label{sec:evaluation-prompts}
We use LLM to evaluate the steering responses of the model. The responses are evaluated with three metrics: concept score, instruct score and fluency score. We prompt the model to always generate explanations before assigning scores, which significantly outperforms directly asking for scores in our offline experiments.

We find that this ternary scoring system is crucial to get faithful and stable scores from LLM. For instance, a unified prompt asking the LLM to rate a response given an instruction and a concept is not effective through our experiments, as the LLM usually ignores the lack of the concept as long as the response follows the instruction. The fluency score is needed as the model can cheat by generating fragmented tokens that relate to the concept and the instruction, while being incoherent to humans.

Templates for all scores are provided below:

\input{examples/templates-eval}

\section{Hyperparameters}\label{sec:hyperparameters}
To ensure a fair comparison, we perform separate hyperparameter-tuning for each method that requires training.
% To select the best hyperparameters, we use a held-out dataset for development, \textsc{Concept10}, which  covers 10 concepts each on our tasks. 
For each method, we conduct separate hyperparameter-tuning on a small \textsc{Concept10} Dataset containing training and testing datasets only for 10 concepts. These concepts overlap with \textsc{Concept500}. \cref{tab:hyperparameters_settings_2B} and \cref{tab:hyperparameters_settings_9B} show hyperparameter settings for methods that require training. Due to limited compute resource, we select the best setting of hyperparameters based on performance on the \textbf{\concepttag{} concept detection} task using AUC for all dictionary learning methods (i.e., can be evaluated on \textbf{\concepttag{} concept detection}).  We minimise the loss with \texttt{AdamW} with a linear scheduler for all methods that require training. Following \citet{gao2024scalingevaluatingsparseautoencoders}, we remove gradients that are parallel to the learned weights when training Probe and ReFT-r1, to account for interaction between Adam and our weight normalization step.

For methods that only for steering, we select the best setting based on \textbf{\steeringtag{} model steering} performance. We follow a setting where we only have a single constant steering factor for hyperparameter-tuning. We acknowledge that this might lead to an overall underestimation of the performance of \textbf{\steeringtag{} model steering} performance. For steering factors, we enumerate factors from $\{0.2, 0.4, 0.6, 0.8, 1.0, 1.2, 1.4, 1.6, 1.8, 2.0, 2.5, 3.0, 4.0,
5.0\}$.

\paragraph{Comments about decoding strategies.} Through our offline experiments, we observed that the choice of decoding strategies can positively or negatively impact overall steering scores for each method (e.g., perplexity scores increase more drastically with repetition penalties). We use the default decoding strategy (i.e., setting the decoding temperature to 1.0) without applying additional penalties for repeating tokens. We believe that this setup reflects the typical user interaction with language models. \textbf{However, this is not a common practice in representation-based model steering}. Existing works often apply repeatition or frequency penalties, which we argue is not the fairest setting, as it often does not accurately resemble normal user behaviour. 

% Below, we enumerate the existing setups used by others: 
% \begin{itemize} 
% \item Neuronpedia\footnote{Steering interface at \url{https://www.neuronpedia.org/gemma-2-9b-it/steer}.}: it uses frequency penalty of 2.0 with a temperature of 0.5 by default. 
% \item CAA: 
% \item RepEng: 
% \end{itemize}

\input{tables/hypersearch}

\clearpage

\section{Dataset Statistics}\label{sec:data_stats}
We show a set of concepts sampled from our \textsc{Concept10} datasets in \cref{tab:concepts-genres}. \cref{tab:datasets-stats} shows dataset statistics including the number of concepts, the number of training and testing examples, the percentage distribution of genre types, and the averaged length of input and output sequence. The output sequence length of \textsc{Concept16K} is expected to be shorter since we restrict the maximum sequence length to 64 during data creation.

\input{tables/concept-examples}

\input{tables/data-stats}

% \section{Dictionary Learning Cost Analysis}\label{sec:learning-cost-analysis}

% Take Gemma Scope as an example. The cost of training one Gemma Scope SAE can be divided into 3 stages: training, inference, and feature labelling (assuming LLMs automatically generate these interpretations).

% \subsection{Training}

% Gemma Scope has used over 20\% of GPT-3 175B's training compute \citep{lieberum-etal-2024-gemma}, which took about $3.14 \cdot 10^{23}$ FLOPs of compute \citep{patterson2021carbonemissionslargeneural}. Assuming that all these compute are spent on TPU v3 and their accelerators achieved 50\% utilization (which would mean 110.7PFLOPs/\$, \citet{heim2022estimatepalmtrainingcost}), then the compute of training Gemma Scope would take about \$567,300. Gemma Scope also stored about 20PB of activations. Assuming that these activations are stored for a month, according to the Google Cloud Storage Pricing Sheet as of Jan 8, 2025 \citep{googlecloud}, Gemma Scope would take \$200,000 for storing activations. Since Gemma Scope trained 30 million features, the training cost of Gemma Scope is about \$0.0256 per feature.

% \subsection{Inference}

% Assuming that one typical SAE is 1k wide, and it will take 1 million tokens forward pass to acquire the SAE's activation distribution across the whole corpus, and Gemma Scope costs \$1/1 million tokens in this state, the inference cost of Gemma Scope per feature is approximately \$0.0010.

% \subsection{Feature Labelling}

% Both the Gemma Scope paper and the JumpReLUSAE paper \citep{rajamanoharan2024jumping} did not disclose the numeric details of their automatic feature labelling. But we do know that Gemma Scope's Automatic Interpretability pipeline uses Gemini Flash to perform feature explanation and simulation. Using Eleuther AI's \citep{eleutherai} SAE automatic interpretability cost estimate (\$4,860 for both explanation and simulation for GPT-4o-mini per 1 million features), and assume that Gemini Flash costs similar to GPT-4o-mini, we can estimate that the labelling cost of Gemma Scope is about \$0.0049 per feature.

% \subsection{Summary}

% In short, the total cost of one Gemma Scope feature is about \$0.0315. This number is similar in magnitude to the cost of labelling neurons directly (about \$0.046 per neuron according to \citet{choi2024automatic}).

% \begin{table}[h]
% \centering
% \caption{Cost Breakdown of a Gemma Scope Feature}
% \begin{tabular}{p{3cm}p{4cm}p{3cm}p{3cm}}
% \toprule
% \textbf{Stage} & \textbf{Description} & \textbf{Cost per Stage} & \textbf{Cost per Feature} \\
% \midrule
% Training & Training Gemma Scope SAE, including compute cost (based on 20\% of GPT-3 175B's training compute and TPU v3 with 50\% utilization) and activation storage cost (20PB stored for a month). & \$767,300 for 30 million features (\$567,300 compute + \$200,000 storage) & \$0.0256 \\
% \midrule
% Inference & Performing inference with a 1k-wide SAE on 1 million tokens to acquire activation distribution. & \$1 for 1,000 features (based on \$1/1 million tokens) & \$0.0010 \\
% \midrule
% Feature Labelling & Automatic feature explanation and simulation using Gemini Flash (estimated based on Eleuther AI's cost for GPT-4o-mini). & \$4,860 for 1 million features & \$0.0049 \\
% \midrule
% \textbf{Total} & & & \textbf{\$0.0315} \\
% \bottomrule
% \end{tabular}
% \label{tab:gemma_scope_cost}
% \end{table}

% \input{tables/human_eval}

% \clearpage

% \section{Human Evaluation}\label{sec:human_eval}
% To validate scores generated by our LLM judges, we conduct human evaluation by asking participants to rate three scores given a steered response with the original prompt and the steering concept: concept score, instruct score, and fluency score. This setup directly mimics our LLM scoring system as described in \cref{sec:model-steering}. As shown in \cref{fig:human-eval-UI}, our human evaluation interface starts with instructions describing how the scores should be given.\footnote{We use \texttt{gpt-4o-mini-2024-07-18} to translate our prompts into 1-2 sentence-long natural language.} One example question is included in \cref{fig:human-eval-UI}, where the participant sees the original prompt together with the steering concept and the model generation. Each survey has 30 questions. The survey is single-blind as the participant is unaware of the underlying models. To evaluate the robustness of our judges, we sample the best generations among six methods, including Prompting, ReFT-r1, LoReFT, LoRA, DiffMean, and SAE trained on \texttt{Gemma-2-9B} at layer 20. We follow the same procedure as in \cref{sec:result-model-steering} to select the best factors for each method on each concept. We then generate our human evaluation dataset by considering three cases for each method except SAE: (1) won over SAE (i.e., receiving a higher score); (2) lost to SAE (i.e., receiving a lower score); and (3) tied with SAE (i.e., receiving the same score). For each case per method, we random sample a single example. As a result, it produces 15 pairs which we then split up into 30 individual questions.

% \paragraph{Participants.} We gratefully acknowledge the contribution of each of our volunteers. The raters were Stanford students from undergraduate through Ph.D., or Stanford faculty or recent Stanford graduates, listed in random order starting with the authors:

% \begin{table}[h]
% \centering
% \begin{tabular}{llll}
% 1. Zhengxuan Wu (\emph{author}) & 2. Chenglei Si & 3. Aryaman Arora (\emph{author}) & 4. Jing Huang (\emph{author}) \\
% 5. Ken Ziyu Liu & 6. Atticus Geiger (\emph{author}) & 7. Jason Boxi Zhang & 8. Aahil Awatramani \\
% 9. Christopher Potts (\emph{author}) & 10. Lanruo Lora Xie & 11. Houjun Liu & 12. Jiuding Sun \\
% 13. Zheng Wang (\emph{author}) & 14. Qinan Yu & 15. Arpandeep Khatua & 16. Dilara Soylu \\
% 17. Julie Kallini & 18. Kushal Thaman & &  \\
% \end{tabular}
% \end{table}


% \begin{figure*}[!h]
%     \centering
%     \fbox{\includegraphics[width=0.9\textwidth]{figures/human-eval-UI.png}}
%     \caption{Layout of our human evaluation survey. Each participant is surveyed with 30 questions.}
%     \label{fig:human-eval-UI}
% \end{figure*}

\clearpage

\section{Concept detection examples}\label{sec:concept_detection_examples}

\begin{figure*}[!h]
    \centering
    \fbox{\includegraphics[width=1.0\textwidth]{figures/heatmap-UI.png}}
    \caption{Visualization of token-level activations by using ReFT-r1 trained on \texttt{Gemma-2-2B} instruction-tuned model on layer 10.}
    \label{fig:heatmap-UI}
\end{figure*}

\clearpage

\section{Model generations}\label{sec:model_generations}
We provide sampled model generations from different methods to illustrate success cases and common failure cases. We use models trained with \texttt{Gemme-2-9B-it} at layer 20 for these examples.

\input{examples/model_generations}

\end{document}
