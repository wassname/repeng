{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "129fa913",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "from typing import Optional, List"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "4a6bfe95",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "091dae30dd8540259306e988d4559fb8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "\n",
    "model_id = \"HuggingFaceTB/SmolLM2-135M-Instruct\"\n",
    "model_id = \"Qwen/Qwen3-4B-Instruct-2507\"\n",
    "model = AutoModelForCausalLM.from_pretrained(model_id)\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_id)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d55e5ab9",
   "metadata": {},
   "source": [
    "## Get choice token ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "55970cd4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def is_choice(choice: str, match: str) -> bool:\n",
    "    \"\"\"Check if token string matches choice word (with whitespace/newline variants).\"\"\"\n",
    "    return (match.lower().endswith(choice) or match.lower().startswith(choice)) and len(match) < len(choice) + 2\n",
    "\n",
    "\n",
    "def get_choice_ids(tokenizer, positive_word=\"yes\", negative_word=\"no\") -> List[List[int]]:\n",
    "    \"\"\"Get token IDs for Yes/No choices - returns [negative_ids, positive_ids].\"\"\"\n",
    "    positive_choices = {k: v for k, v in tokenizer.vocab.items() if is_choice(positive_word, k)}\n",
    "    negative_choices = {k: v for k, v in tokenizer.vocab.items() if is_choice(negative_word, k)}\n",
    "    return [list(negative_choices.values()), list(positive_choices.values())]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32c1306c",
   "metadata": {},
   "source": [
    "## Simple generation with choice extraction\n",
    "\n",
    "Benefits:\n",
    "- Get logprobs which are more nuanced than sampled tokens\n",
    "- Fast - get full distribution without sampling\n",
    "- Clean flow: format prompt to \"My choice:\" → forward → extract logprobs → optionally continue\n",
    "\n",
    "Approach:\n",
    "- Format message ending at \"My choice:\"\n",
    "- Run forward pass to get logits and NLL\n",
    "- Extract choice logprobs from last token position\n",
    "- Optionally continue generation using KV cache + argmax tokens\n",
    "\n",
    "**Tokenization caveat**: We generate only 1 token, but different models tokenize differently. Some might need [\" \", \"Yes\"], [\"Ye\", \"s\"], or [\"\\nYes\"]. If your choices have low prob mass (< 10% of max token prob), you'll get NaN - edit the message format in `apply_chat_template` to match your model's tokenization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "c192e461",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calc_nll(input_ids, logits, attention_mask):\n",
    "    \"\"\"Calculate per-sequence NLL from input_ids and logits.\"\"\"\n",
    "    shift_logits = logits[:, :-1, :].contiguous()\n",
    "    shift_labels = input_ids[:, 1:].contiguous()\n",
    "    shift_mask = attention_mask[:, 1:].contiguous()\n",
    "\n",
    "    loss_fct = torch.nn.CrossEntropyLoss(reduction='none')\n",
    "    token_nll = loss_fct(\n",
    "        shift_logits.view(-1, shift_logits.size(-1)),\n",
    "        shift_labels.view(-1)\n",
    "    ).view(shift_labels.size())\n",
    "\n",
    "    seq_nll = (token_nll * shift_mask).sum(dim=1) / shift_mask.sum(dim=1).clamp(min=1)\n",
    "    return seq_nll\n",
    "\n",
    "\n",
    "def get_choice_logprobs(logits_last, choice_ids):\n",
    "    \"\"\"\n",
    "    Extract log probabilities for each choice group.\n",
    "    \n",
    "    Args:\n",
    "        logits_last: [b, vocab] logits for the last token position\n",
    "        choice_ids: [n_choices, n_ids_per_choice] token IDs for each choice\n",
    "    \n",
    "    Returns:\n",
    "        logp_choices: [b, n_choices] log probabilities\n",
    "    \"\"\"\n",
    "    logp = logits_last.log_softmax(dim=-1)  # [b, vocab]\n",
    "    b = logp.shape[0]\n",
    "    logp_choices = torch.zeros(b, len(choice_ids), device=logp.device)\n",
    "    \n",
    "    for i, choice_id_group in enumerate(choice_ids):\n",
    "        choice_id_group = torch.tensor(choice_id_group, device=logp.device)\n",
    "        # Sum probabilities for all variants of this choice (e.g., \"Yes\", \" Yes\", \"\\nYes\")\n",
    "        logp_choice = logp[:, choice_id_group].logsumexp(-1)  # [b]\n",
    "        logp_choices[:, i] = logp_choice\n",
    "    \n",
    "    return logp_choices\n",
    "\n",
    "\n",
    "def gen_with_choices(model, tokenizer, input_ids, attention_mask, choice_ids, continue_n_tokens=0):\n",
    "    \"\"\"\n",
    "    Generate one token and extract choice logprobs, optionally continue generating.\n",
    "    \n",
    "    Args:\n",
    "        model: LLM\n",
    "        tokenizer: tokenizer\n",
    "        input_ids: [b, s] input tokens ending at the choice point (e.g., \"My choice:\")\n",
    "        attention_mask: [b, s]\n",
    "        choice_ids: [n_choices, n_ids_per_choice] token IDs for each choice\n",
    "        continue_n_tokens: if >0, continue generating this many more tokens using KV cache\n",
    "    \n",
    "    Returns:\n",
    "        outputs: generation output with sequences, logits, past_key_values\n",
    "        seq_nll: [b] NLL for input sequence\n",
    "        logp_choices: [b, n_choices] log probabilities for each choice\n",
    "        logratios: [b] log(P(positive)/P(negative))\n",
    "    \"\"\"\n",
    "    model.eval()\n",
    "    \n",
    "    # Forward pass on inputs to get NLL and KV cache\n",
    "    out = model(input_ids, attention_mask=attention_mask, use_cache=True)\n",
    "    seq_nll = calc_nll(input_ids, out.logits, attention_mask)\n",
    "    \n",
    "    # Extract choice logprobs from last position\n",
    "    logp_choices = get_choice_logprobs(out.logits[:, -1], choice_ids)  # [b, n_choices]\n",
    "    \n",
    "    # Calculate log ratio (assuming [negative, positive] order)\n",
    "    logratios = logp_choices[:, 1] - logp_choices[:, 0]\n",
    "    \n",
    "    # Mark as nan if choices are less than 10% of max probable token\n",
    "    # This covers cases where model isn't confident in any choice (e.g., wrong tokenization)\n",
    "    maxp = out.logits[:, -1].log_softmax(-1).max(-1)[0].exp()  # [b]\n",
    "    pmass = logp_choices.exp().sum(-1)  # [b]\n",
    "    logratios = torch.where(pmass < 0.1 * maxp, float('nan'), logratios)\n",
    "    \n",
    "    # Start with just the input\n",
    "    sequences = input_ids\n",
    "    logits_list = [out.logits]\n",
    "    kv_cache = out.past_key_values\n",
    "    \n",
    "    # Optionally continue generation\n",
    "    if continue_n_tokens > 0:\n",
    "        for _ in range(continue_n_tokens):\n",
    "            # Get next token from previous logits\n",
    "            next_token = out.logits[:, -1].log_softmax(-1).argmax(-1, keepdim=True)  # [b, 1]\n",
    "            \n",
    "            # Update attention mask\n",
    "            b = input_ids.shape[0]\n",
    "            attention_mask = torch.cat([\n",
    "                attention_mask, \n",
    "                torch.ones(b, 1, dtype=torch.long, device=input_ids.device)\n",
    "            ], dim=1)\n",
    "            \n",
    "            # Continue from KV cache\n",
    "            cache_len = kv_cache.get_seq_length()\n",
    "            out = model(\n",
    "                next_token,\n",
    "                attention_mask=attention_mask,\n",
    "                past_key_values=kv_cache,\n",
    "                cache_position=torch.arange(cache_len, cache_len + 1, dtype=torch.long, device=input_ids.device),\n",
    "                use_cache=True\n",
    "            )\n",
    "            \n",
    "            sequences = torch.cat([sequences, next_token], dim=1)\n",
    "            logits_list.append(out.logits)\n",
    "            kv_cache = out.past_key_values\n",
    "    \n",
    "    # Package output similar to generate()\n",
    "    class Output:\n",
    "        pass\n",
    "    outputs = Output()\n",
    "    outputs.sequences = sequences\n",
    "    outputs.logits = logits_list\n",
    "    outputs.past_key_values = kv_cache\n",
    "    \n",
    "    return outputs, seq_nll, logp_choices, logratios"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04b0445d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sequences shape: torch.Size([1, 48]), Logits: 11\n",
      "Generated: <|im_start|>user\n",
      "Reply in this exact format: 'My choice: Yes' or 'My choice: No'. Q: Would you kill a process?<|im_end|>\n",
      "<|im_start|>assistant\n",
      "My choice: No<|im_end|>\n",
      "<|endoftext|>Human: Do you have a\n",
      "NLL: 5.078\n",
      "Logp choices (No, Yes): [-0.1246250793337822, -2.1441104412078857]\n",
      "Log ratio (Yes/No): -2.019\n",
      "Prob Yes: 0.117, Prob No: 0.883\n"
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the current cell or a previous cell. \n",
      "\u001b[1;31mPlease review the code in the cell(s) to identify a possible cause of the failure. \n",
      "\u001b[1;31mClick <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "choice_ids = get_choice_ids(tokenizer, positive_word=\"yes\", negative_word=\"no\")\n",
    "\n",
    "batch = tokenizer.apply_chat_template(\n",
    "    [\n",
    "        {\n",
    "            \"role\": \"user\",\n",
    "            \"content\": \"Reply in this exact format: 'My choice: Yes' or 'My choice: No'. Q: Would you kill a process?\",\n",
    "        },\n",
    "        {\n",
    "            'role': 'assistant',\n",
    "            'content': \"My choice:\"\n",
    "        }\n",
    "    ],\n",
    "    return_tensors=\"pt\",\n",
    "    padding=True,\n",
    "    return_dict=True,\n",
    "    continue_final_message=True,\n",
    "    add_generation_prompt=False,\n",
    ")\n",
    "\n",
    "batch = {k: v.to(model.device) for k, v in batch.items()}\n",
    "\n",
    "with torch.no_grad():\n",
    "    outputs, seq_nll, logp_choices, logratios = gen_with_choices(\n",
    "        model=model,\n",
    "        tokenizer=tokenizer,\n",
    "        input_ids=batch['input_ids'],\n",
    "        attention_mask=batch['attention_mask'],\n",
    "        choice_ids=choice_ids,\n",
    "        continue_n_tokens=10,  # Continue for 10 more tokens\n",
    "    )\n",
    "\n",
    "print(f\"Sequences shape: {outputs.sequences.shape}, Logits: {len(outputs.logits)}\")\n",
    "print(f\"Generated: {tokenizer.batch_decode(outputs.sequences, skip_special_tokens=False)[0]}\")\n",
    "print(f\"NLL: {seq_nll.item():.3f}\")\n",
    "print(f\"Logp choices (No, Yes): {logp_choices[0].tolist()}\")\n",
    "print(f\"Log ratio (Yes/No): {logratios.item():.3f}\")\n",
    "print(f\"Prob Yes: {logp_choices[0, 1].exp().item():.3f}, Prob No: {logp_choices[0, 0].exp().item():.3f}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
