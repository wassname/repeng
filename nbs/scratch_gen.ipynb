{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "129fa913",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from transformers.cache_utils import DynamicCache\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "from tqdm.auto import tqdm\n",
    "from collections import defaultdict\n",
    "from typing import Optional"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a6bfe95",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(\"gpt2\")\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"gpt2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f555052f",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "batch2 = tokenizer.apply_chat_template([{'role': 'user', 'content': 'The eifel tower is in'}], return_tensors='pt', padding=True)\n",
    "batch2 = {k: v.to(model.device)[None] for k, v in batch2.items()}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "187ae64d",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "forward_out = model(**batch2, use_cache=True)\n",
    "logits = forward_out.logits  # [b, s, vocab]\n",
    "past_key_values = forward_out.past_key_values\n",
    "next_input_ids = forward_out.logits[:, -1].log_softmax(-1).argmax(-1)[None]\n",
    "new_attn_mask = torch.cat(\n",
    "    [batch2['attention_mask'], torch.ones_like(next_input_ids)],\n",
    "    dim=1\n",
    ")\n",
    "\n",
    "# Shift logits and labels for NLL: predict token t from tokens 0..t-1\n",
    "shift_logits = logits[:, :-1, :].contiguous()\n",
    "shift_labels = batch2['input_ids'][:, 1:].contiguous()\n",
    "\n",
    "# Compute NLL per token, masking padding\n",
    "shift_mask = (shift_labels != tokenizer.pad_token_id).float()\n",
    "loss_fct = torch.nn.CrossEntropyLoss(reduction='none')\n",
    "token_nll = loss_fct(\n",
    "    shift_logits.view(-1, shift_logits.size(-1)),\n",
    "    shift_labels.view(-1)\n",
    ").view(shift_labels.size())\n",
    "\n",
    "# Average NLL per sequence (excluding padding)\n",
    "seq_nll = (token_nll * shift_mask).sum(dim=1) / shift_mask.sum(dim=1).clamp(min=1)\n",
    "\n",
    "# Continue generation from the cached KV states\n",
    "# Cache must be seq_len-1 since we're passing the last input token as new input\n",
    "input_ids = batch2['input_ids']\n",
    "n = past_key_values.get_seq_length()\n",
    "outputs = model.generate(\n",
    "    input_ids=next_input_ids,  # Last token as new input\n",
    "    attention_mask=new_attn_mask,  # Keep full mask\n",
    "    past_key_values=past_key_values,\n",
    "    cache_position=torch.arange(n, n+1, dtype=torch.long, device=input_ids.device),\n",
    "    output_logits=True,\n",
    "    output_scores=True,\n",
    "    return_dict_in_generate=True,\n",
    "    **kwargs\n",
    ")\n",
    "\n",
    "# now we need to modify this as generate does return the full sequences, including inputs ids\n",
    "outputs.sequences = torch.concat([input_ids[:, :-1], outputs.sequences], 1)\n",
    "# outputs.scores = #TODO\n",
    "# outputs.logits = torch.concat([forward_out.logits, torch.stack(outputs.logits, 1)], 1)\n",
    "\n",
    "# # FIXME len(outputs.logits)==outputs.sequences.shape[1]\n",
    "# kwargs['max_new_tokens']=32\n",
    "# outg2 = model.generate(\n",
    "#     input_ids=batch2['input_ids'],  # Last token as new input\n",
    "#     attention_mask=batch2['attention_mask'],  # Keep full mask\n",
    "#     output_logits=True,\n",
    "#     output_scores=True,\n",
    "#     return_dict_in_generate=True,\n",
    "#     **kwargs\n",
    "# )\n",
    "# print(outputs.sequences.shape[1]+input_ids.shape[1], outg2.sequences.shape, len(outputs.logits), len(outg2.logits))\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
